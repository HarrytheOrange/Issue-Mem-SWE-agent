{
    "search_index": {
        "description_for_embedding": "Refactored the Optuna PyTorch MNIST example to define the model via a `define_model(trial)` factory that returns an `nn.Sequential` network instead of a custom `nn.Module` class. The refactor simplifies layer construction, makes the hyperparameter search space explicit (including per-layer dropout ratios), and moves the input flattening (`view(-1, 28*28)`) out of the model and into the training loop.",
        "keywords": [
            "optuna",
            "pytorch_simple.py",
            "PyTorch",
            "nn.Sequential",
            "model definition refactor",
            "hyperparameter optimization",
            "dropout per layer",
            "MNIST example",
            "flatten input outside model",
            "search space consistency"
        ]
    },
    "agent_memory": {
        "episodic_memory": "In this PR, the PyTorch MNIST example in Optuna (`examples/pytorch_simple.py`) was refactored to simplify how the model is defined and how hyperparameters are sampled.\n\nOriginally, the example defined a custom `Net(nn.Module)` class that: (1) took an Optuna `trial` in its constructor, (2) dynamically constructed a variable number of fully-connected layers and a single dropout layer probability sampled once and reused for all layers, (3) manually stored these layers in Python lists and then re-attached them as attributes (`fc0`, `fc1`, ..., `drop0`, `drop1`, ...) to satisfy PyTorch's module registration, and (4) included the input flattening (`data.view(-1, 28 * 28)`) inside the `forward` method. The forward pass used `F.relu` and `F.log_softmax` directly on the outputs.\n\nThe refactor removed the `Net` class and replaced it with a `define_model(trial)` function that returns an `nn.Sequential` model. Inside `define_model`, the code now:\n- Samples `n_layers` using `trial.suggest_int('n_layers', 1, 3)`.\n- Iteratively creates linear layers and activation layers:\n  - For each layer index `i`, it samples `out_features` with `trial.suggest_int('n_units_l{i}', 4, 128)` (note: previously `loguniform`, now simple `int` in this diff) and appends `nn.Linear(in_features, out_features)` and `nn.ReLU()`.\n  - It now samples a separate dropout probability per layer: `trial.suggest_uniform('dropout_l{i}', 0.2, 0.5)` and appends `nn.Dropout(p)`.\n- After the loop, it appends the final classification layer `nn.Linear(in_features, CLASSES)` and `nn.LogSoftmax(dim=1)`.\n\nThe training loop was updated accordingly:\n- Instead of instantiating `Net(trial)`, it calls `model = define_model(trial).to(DEVICE)`.\n- Because the model is now pure `nn.Sequential` and expects flat vectors, the input flattening (`data.view(-1, 28*28)`) is moved out of the model and into the training and test loops: `data, target = data.view(-1, 28 * 28).to(DEVICE), target.to(DEVICE)`.\n\nA clarifying comment was added above `define_model` explaining: `# We optimize the number of layers, hidden untis and dropout ratio in each layer.`\n\nDuring review, it was noted that this PR changes the behavior of dropout hyperparameters: instead of using a single sampled dropout probability for all layers, it now optimizes a separate dropout ratio per layer. Reviewers mentioned that other framework examples (Chainer, TensorFlow Eager) do not optimize dropout at all, and it may be desirable to align the search space across frameworks. However, they agreed to handle that alignment in separate PRs and approved this refactor as-is.\n\nAll modified lines were covered by tests, and overall project coverage remained unchanged, indicating that the refactor preserved behavior sufficiently for existing tests.",
        "semantic_memory": "This PR illustrates several general best practices when defining models in PyTorch for hyperparameter optimization:\n\n1. **Use model factories with `nn.Sequential` when appropriate**:\n   - When a model is relatively simple and feed-forward (no complex control flow or multiple outputs), defining it via a function that returns an `nn.Sequential` instance (`define_model(trial)`) can greatly simplify the code compared to defining a full `nn.Module` subclass.\n   - This avoids manual attribute registration and reduces boilerplate (no need for custom `__init__`/`forward` if the computation is just a straight chain of layers).\n\n2. **Keep hyperparameter sampling close to model construction**:\n   - Hyperparameter optimization frameworks (like Optuna) benefit from a clean separation where the trial object is used inside a dedicated model-construction function. This keeps the search space explicit and concentrated in one place.\n   - Explicitly naming hyperparameters per layer (e.g., `n_units_l0`, `dropout_l0`, ...) makes the search space more interpretable and debuggable.\n\n3. **Per-layer hyperparameters vs. shared hyperparameters**:\n   - Sharing a single hyperparameter (e.g., one dropout value) across all layers is simpler but less expressive than optimizing distinct values per layer.\n   - Choosing between them is a modeling decision: per-layer hyperparameters enlarge the search space and may yield better models at the cost of search complexity.\n\n4. **Separate data reshaping from model definition**:\n   - Putting input reshaping (e.g., flattening 2D image tensors into 1D feature vectors) inside the model's `forward` method is not inherently wrong, but it embeds data preprocessing into the model.\n   - For simple `nn.Sequential` models, it's often cleaner to keep the model purely as a sequence of trainable layers and activation functions and perform shape/format transformations in the training loop or a distinct preprocessing step.\n\n5. **Leverage built-in functional layers instead of functional calls when possible**:\n   - Using `nn.ReLU` and `nn.LogSoftmax` as explicit layers makes the architecture easier to inspect (e.g., `print(model)` shows the full structure) and consistent with the rest of `nn.Sequential`.\n   - It can simplify exporting/scripting and improves clarity for readers.\n\n6. **Align example search spaces across frameworks where possible**:\n   - When maintaining multiple examples across different frameworks (PyTorch, Chainer, TensorFlow), using a similar hyperparameter search space (same number and type of hyperparameters) reduces user confusion and aids in cross-framework comparison.\n\nOverall, the refactor showcases how to simplify PyTorch code in a hyperparameter-tuned setting by using `nn.Sequential`, explicit layer-wise hyperparameters, and moving preprocessing out of the model.",
        "procedural_memory": [
            "When refactoring or designing PyTorch models for hyperparameter optimization (e.g., with Optuna), use the following process:",
            "Step 1: Identify whether the model can be expressed as a simple feed-forward chain of layers. If the forward logic is linear (no branching or complex control flow), consider replacing a custom `nn.Module` class with an `nn.Sequential`-based model.",
            "Step 2: Create a model factory function, e.g., `def define_model(trial):`, that takes a hyperparameter optimization trial and returns the model instance. Move all `trial.suggest_*` calls into this function to keep the search space localized and explicit.",
            "Step 3: Inside the factory, iteratively build a `layers` list rather than managing layers and dropouts in manual lists plus `setattr`. For each layer index `i`, append the necessary modules in order: `nn.Linear`, activation (e.g., `nn.ReLU`), and optional `nn.Dropout`. Sample per-layer hyperparameters with descriptive names like `n_units_l{i}` and `dropout_l{i}` if you want distinct values per layer.",
            "Step 4: Add the final output layer and any terminal activation or log-probability layer (e.g., `nn.Linear` followed by `nn.LogSoftmax(dim=1)`) as the last elements in `layers`. Then construct `model = nn.Sequential(*layers)` and return it.",
            "Step 5: Adjust the training and evaluation loops to accommodate the new model interface. If you previously reshaped the input inside the model (e.g., `data.view(-1, 28 * 28)` in `forward`), move that reshaping into the training loop right before calling `model(data)` so that the model can be a pure `nn.Sequential` over already-preprocessed inputs.",
            "Step 6: Update the call site where the model is created to use the factory: replace `model = Net(trial).to(DEVICE)` with `model = define_model(trial).to(DEVICE)` or similar. Ensure the optimizer and loss computation remain compatible with the new model output (e.g., log probabilities vs. logits).",
            "Step 7: Add or update comments near the model factory to clearly document what is being optimized (e.g., number of layers, units per layer, dropout ratios) so future maintainers understand the search space.",
            "Step 8: Run existing tests and example scripts to confirm behavior is preserved or changes are intentional. If multiple framework examples exist, compare hyperparameter spaces and decide whether to align them for consistency, potentially in separate PRs.",
            "Step 9: Inspect the printed model (`print(model)`) to verify that the constructed `nn.Sequential` matches the intended architecture (layer order, sizes, dropout placement, and activations).",
            "Step 10: If coverage tools are in use, verify that all modified lines are covered by tests, and that refactoring has not reduced coverage. This helps ensure that behavior changes are controlled and observable."
        ]
    }
}