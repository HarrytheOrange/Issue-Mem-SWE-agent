{
    "search_index": {
        "description_for_embedding": "Home Assistant TekSavvy internet usage sensor: add support for unlimited (uncapped) plans, prevent division-by-zero when total_bandwidth is 0, adjust entity names/labels, and add tests plus error handling for bad HTTP status and invalid JSON responses.",
        "keywords": [
            "Home Assistant",
            "TekSavvy",
            "sensor.teksavvy",
            "unlimited plan",
            "uncapped bandwidth",
            "division by zero",
            "bandwidth_cap 0",
            "percentage usage",
            "usage ratio",
            "infinite limit",
            "aiohttp",
            "async_update",
            "JSON decode error",
            "HTTP status handling",
            "unit tests"
        ]
    },
    "agent_memory": {
        "episodic_memory": "The TekSavvy sensor in Home Assistant originally assumed that every ISP plan had a non-zero data cap. The configuration allowed users to set total_bandwidth, but the documentation encouraged using 0 to represent an unlimited plan. When total_bandwidth was 0, the sensor attempted to compute the percentage usage as `100 * on_peak_download / bandwidth_cap`, causing a division-by-zero error. Additionally, the 'usage' label was ambiguous between GB usage and percentage usage, and one entity name had a trailing space in its display name.\n\nTo fix this, the TekSavvyData initializer was updated so that if `bandwidth_cap > 0`, `self.data[\"limit\"]` is set to the numeric cap, but if `bandwidth_cap == 0`, it is set to `float('inf')`, representing an unlimited plan. In `async_update`, the percentage usage calculation is now guarded: when `bandwidth_cap > 0`, `usage` is computed as `100 * on_peak_download / bandwidth_cap`; when `bandwidth_cap == 0`, `usage` is set to 0 to avoid division by zero. The GB-based usage (`usage_gb`) is still set to the raw `on_peak_download`, and `onpeak_remaining` is calculated as `limit - on_peak_download`, which yields `inf` for unlimited plans.\n\nError handling was also improved: the JSON parsing of the TekSavvy API response is now wrapped in a try/except. If `req.json()` raises `ValueError` (invalid JSON), the code logs 'JSON Decode Failed' and returns False instead of raising. Non-200 HTTP responses were already handled by checking `req.status` and returning False.\n\nThe sensor metadata was adjusted: the 'usage' sensor's friendly name is now 'Usage Ratio' with unit '%', and the GB-based metric remains under 'Usage' with unit 'GB'. A trailing space in the 'On Peak Upload ' label was removed. The entity IDs for the percentage usage and for peak upload were effectively clarified/standardized in line with this change (e.g., percentage usage exposed as `sensor.teksavvy_usage_ratio`).\n\nComprehensive unit tests were added for the TekSavvy sensor, which previously had no test coverage. Two configuration-based tests verify behavior for both capped and unlimited plans using `aioclient_mock` to simulate the TekSavvy API. For a capped plan (400 GB), the tests assert that values like limit, on/off-peak usage, totals, remaining, and percentage usage (56.69%) are correct, and that units of measurement are as expected. For an unlimited plan (`total_bandwidth: 0`), the tests confirm that the limit and remaining values are 'inf', the GB usage values match the API, and the usage ratio is '0' without any division error. Additional tests exercise HTTP error handling (404 status returns False from `async_update`) and JSON decode failures (non-JSON text results in `async_update` returning False). The TekSavvy sensor was removed from the `.coveragerc` omit list so this new test coverage is counted.",
        "semantic_memory": "This change illustrates several generalizable patterns:\n\n1. **Guarding percentage/ratio calculations against zero or unlimited denominators**: When computing percentages or ratios based on a configured limit (e.g., data cap, quota, maximum capacity), the limit may legitimately be 0 to represent 'unlimited' or 'no limit'. Directly dividing by that value can cause runtime exceptions or undefined behavior. A robust pattern is:\n   - Treat a zero or missing limit as a special case (e.g., unlimited), not as a numeric denominator.\n   - Conditionally compute the ratio only when the limit is positive and finite; otherwise either skip the computation, provide a sentinel (e.g., 0 or NaN), or expose the metric in a different way.\n   - Optionally model unlimited as an infinite value (`float('inf')`) for downstream calculations like `remaining = limit - used`, which then naturally yields `inf`.\n\n2. **Disambiguating similar metrics with clear names and units**: Overloading a single label or entity name for multiple interpretations (e.g., 'usage' meaning both GB and %) leads to confusion and brittle integrations. Better practice is to:\n   - Use distinct identifiers and friendly names for conceptually different metrics (e.g., `usage_gb` vs `usage_ratio`).\n   - Always associate a unit of measurement (GB, %, etc.) and keep it consistent.\n   - Keep display names clean (no trailing spaces) for predictability and easier testing.\n\n3. **Defensive handling of remote API responses**: Integrations that depend on remote HTTP APIs should treat the network and data as untrusted:\n   - Check HTTP status codes and treat non-200 statuses as non-fatal errors, returning a failure indicator instead of raising unhandled exceptions.\n   - Wrap JSON decoding in a try/except block; log decoding failures and degrade gracefully instead of crashing the component.\n   - Return a boolean success flag from update routines so the framework can decide whether to keep the last known state or mark the entity unavailable.\n\n4. **Testing both normal and edge configurations**: When an integration has configuration-dependent behavior (e.g., capped vs unlimited), it is important to have explicit tests for each branch:\n   - Use HTTP mocking (`aioclient_mock` or equivalents) to reliably simulate API responses.\n   - Assert on both the numerical values and the units of measurement of exposed entities.\n   - Include tests for error cases (bad status code, invalid JSON) to verify graceful handling.\n\n5. **Aligning documentation and implementation**: Documentation might suggest special configuration values (like `0` meaning unlimited), but unless the code explicitly supports that convention, it can lead to runtime errors. Code should be updated to match documented behavior, or documentation should be corrected. The combination of clear docs, explicit semantics in code, and unit tests prevents regressions when behavior is refined.",
        "procedural_memory": [
            "Step-by-step instructions on how to diagnose and fix similar issues.",
            "Step 1: Identify configuration-driven edge cases\n- Review the component's configuration schema and documentation to see if special values (e.g., 0, -1, null) are used to signal special modes like 'unlimited', 'auto', or 'disabled'.\n- Search for all uses of those configuration fields in the code and note where they are used in arithmetic operations, especially as denominators.",
            "Step 2: Reproduce the failure with edge configuration\n- Configure the component to use the documented edge case (e.g., total_bandwidth: 0 for unlimited) in a local dev or test environment.\n- Trigger the code path that performs the calculation (e.g., forcing an update of the sensor).\n- Observe exceptions such as ZeroDivisionError or other unexpected behavior in logs or stack traces.",
            "Step 3: Add explicit semantics for the edge case in the data model\n- In the component's data or model class initializer, convert special configuration values into explicit, internally consistent representations.\n  - For an 'unlimited' numeric limit, consider setting an internal `limit` field to `float('inf')` and the original config value to `0` for tracking.\n- Ensure that downstream calculations that use the limit field can handle infinite values appropriately (e.g., remaining = inf - used).",
            "Step 4: Guard percentage and ratio calculations\n- Locate any percentage or ratio calculations of the form `value / limit`.\n- Modify them to only execute when `limit` (or the original configuration value) is greater than zero and finite:\n  - Example: `if bandwidth_cap > 0: usage_ratio = 100 * used / bandwidth_cap else: usage_ratio = 0` or another suitable sentinel.\n- Decide what the ratio should indicate in the unlimited case (0, None, or 'N/A') and align with UX expectations.",
            "Step 5: Improve error handling for remote API calls\n- Ensure that HTTP requests check `status` for 200 (or appropriate success codes) and return a safe failure indicator on errors instead of throwing exceptions.\n- Wrap JSON parsing (`await resp.json()` or equivalent) in a try/except for `ValueError` or library-specific JSON exceptions.\n- On error, log a clear message (e.g., 'JSON Decode Failed') and return False or an error object so the caller can handle it gracefully.",
            "Step 6: Write unit tests for both standard and edge scenarios\n- Use your framework's testing tools and HTTP mocking utilities to simulate the external API responses.\n- For the standard configuration (e.g., capped plan), assert:\n  - Correct unit_of_measurement on each entity.\n  - Correct computed values (totals, remaining, percentages) based on the mocked API data and configured limit.\n- For the edge configuration (e.g., unlimited with limit 0), assert:\n  - The internal limit/remaining representation (e.g., 'inf').\n  - The absence of division errors and a defined behavior for the ratio metric (e.g., 0%).\n- Add tests to verify behavior on non-200 HTTP responses (e.g., 404) and on invalid JSON payloads, asserting that the update function returns False and does not raise.",
            "Step 7: Clean up entity naming and metadata\n- Review sensor/entity metadata mappings to ensure that each metric has a clear friendly name and unique semantics.\n- Avoid ambiguous labels like 'Usage' for both GB and %; instead use something like 'Usage' and 'Usage Ratio'.\n- Fix minor issues such as trailing spaces in names, and ensure units of measurement are appropriate and consistent.",
            "Step 8: Update coverage configuration and run tests\n- If the affected module was excluded from code coverage, remove it from coverage omit lists (e.g., `.coveragerc`).\n- Run the full test suite (e.g., `tox`) to ensure tests pass in all supported environments.\n- Verify that coverage for the module increases and that both happy-path and error conditions are exercised.",
            "Step 9: Align documentation with behavior\n- Confirm that documentation describing special configuration values (like 0 for unlimited) matches the implemented behavior.\n- If necessary, update docs to reflect the new semantics and any changes in entity IDs or names (e.g., renaming a percentage entity to `usage_ratio`).\n- Cross-link the code and docs updates in the PR description for reviewers."
        ]
    }
}