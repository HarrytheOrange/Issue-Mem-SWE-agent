{
    "search_index": {
        "description_for_embedding": "StackStorm scheduler was prematurely re-marking long-running execution queue items as orphaned due to a hard-coded 60s timeout, causing some actions to be scheduled and executed more than once. The fix replaces the hard-coded constant with a configurable scheduler.execution_scheduling_timeout_threshold_min option, used by the scheduler GC to determine how far back to search for orphaned scheduled actions before resetting their handling flag.",
        "keywords": [
            "StackStorm",
            "scheduler",
            "execution_scheduling_timeout_threshold_min",
            "EXECUTION_SCHEDUELING_TIMEOUT_THRESHOLD_MS",
            "execution scheduling timeout",
            "orphaned scheduled actions",
            "duplicate action execution",
            "scheduler GC",
            "handling flag reset",
            "configurable timeout",
            "long running requests"
        ]
    },
    "agent_memory": {
        "episodic_memory": "In issue #4887, a user observed that certain long-running action executions in StackStorm were being requested and scheduled more than once. The root symptom was that the same execution queue item appeared to be 're-deployed' after it had already started, particularly when dealing with large JSON payloads that took a long time to persist to the database.\n\nInvestigation revealed that the scheduler uses a garbage-collection-style mechanism to recover from crashed schedulers: it periodically finds ActionExecutionSchedulingQueueItemDB records whose scheduled_start_timestamp is older than a fixed threshold and whose handling flag is still True, and it resets handling to False so another scheduler process can pick them up. This threshold was controlled by a hard-coded constant in st2actions/scheduler/handler.py:\n\n    EXECUTION_SCHEDUELING_TIMEOUT_THRESHOLD_MS = 60 * 1000  # 1 minute\n\nFor environments where DB writes or processing take longer than a minute, this logic misclassified in-progress executions as 'stuck' or 'orphaned' and reset their handling flag, causing them to be picked up again and effectively re-scheduled.\n\nThe fix was to replace this hard-coded timeout with a configuration-driven value. A new scheduler configuration option was introduced:\n\n- Name: scheduler.execution_scheduling_timeout_threshold_min\n- Type: Float\n- Default: 1 (minute), preserving prior behavior by default\n- Description: \"How long GC to search back in minutes for orphaned scheduled actions\"\n\nChanges made:\n- In st2actions/st2actions/scheduler/config.py, the new cfg.FloatOpt execution_scheduling_timeout_threshold_min was registered in the scheduler options.\n- In st2actions/st2actions/scheduler/handler.py:\n  - The global EXECUTION_SCHEDUELING_TIMEOUT_THRESHOLD_MS constant was removed.\n  - The timeout is now computed per handler instance as:\n\n        self._execution_scheduling_timeout_threshold_ms = (\n            cfg.CONF.scheduler.execution_scheduling_timeout_threshold_min * 60 * 1000\n        )\n\n  - The _reset_handling_flag() logic now uses self._execution_scheduling_timeout_threshold_ms instead of the constant when computing how far back to look for items:\n\n        'scheduled_start_timestamp__lte': date.append_milliseconds_to_time(\n            date.get_datetime_utc_now(),\n            -self._execution_scheduling_timeout_threshold_ms\n        ),\n\n  - The explanatory comments about the purpose of this timeout (recovering items if the scheduler process crashed) were moved next to the computed attribute and linted for style.\n\n- In st2tests/st2tests/config.py, the same cfg.FloatOpt was added to the test configuration registration so that test code can access the option without NoSuchOptError.\n- In conf/st2.conf.sample, the new option was added under [scheduler], along with a descriptive comment, and the file was re-generated using make configgen. Multiple follow-up commits corrected ordering and spacing so that CI checks comparing st2.conf.sample against the generator output would pass.\n- CHANGELOG.rst was updated to document the new configuration option: \"Add config option scheduler.execution_scheduling_timeout_threshold_min to better control the cleanup of scheduled actions that were orphaned. #4886\".\n\nSome discussion in the PR also mentioned that a more robust long-term solution would be to incorporate service discovery/health checks for scheduler processes before releasing locks, but that would require more work. For now, making the timeout configurable allows operators to tune the threshold for their environment and avoid accidental re-scheduling of long-running actions while retaining crash-recovery behavior.",
        "semantic_memory": "Generalizable lessons and patterns from this fix:\n\n1. **Avoid hard-coded timeouts for distributed or asynchronous processing**\n   - A static, hard-coded timeout for determining 'orphaned' work items (e.g., jobs, tasks, executions) can easily be too aggressive or too conservative depending on workload, infrastructure performance, and data size.\n   - For systems that schedule and track work across processes/nodes, timeouts should be configurable, with defaults that preserve historical behavior but allow tuning for different environments.\n\n2. **GC / recovery logic can produce duplicate work if misconfigured**\n   - Mechanisms that attempt to recover from crashed workers (e.g., resetting a `handling` or `locked` flag after a timeout) can unintentionally cause tasks to be re-processed if they misclassify slow-but-healthy work as stuck.\n   - This is especially important when the underlying operations are not idempotent; duplicates can have real side effects.\n   - When adding such recovery logic, always consider edge cases where legitimate processing time may exceed the timeout and offer configuration or safeguards.\n\n3. **Encapsulate critical constants via configuration and per-instance state**\n   - Moving a module-level constant into a configuration option, and then into an instance attribute (`self._execution_scheduling_timeout_threshold_ms`), makes behavior more transparent and easier to test.\n   - It also enables runtime behavior changes (via config) without modifying code, and aligns with patterns used by configuration frameworks (e.g., oslo.config).\n\n4. **Config additions must be consistently registered in all environments (runtime and tests)**\n   - When adding a new configuration option, remember to register it both in the main service config module and in any test config registration modules to avoid `NoSuchOptError` in tests.\n   - Sample configuration files that are auto-generated or validated in CI (like st2.conf.sample) may require running a generator (e.g., `make configgen`) and paying attention to ordering and formatting.\n\n5. **Default behavior should be backward compatible**\n   - The new option uses a default (1 minute) that matches the previous constant, ensuring that existing deployments get the same behavior unless they explicitly adjust the setting.\n   - This pattern is important for minimizing surprises when upgrading.\n\n6. **More robust recovery strategies may involve service discovery/health checks**\n   - A simple timeout-based GC is a heuristic; more reliable solutions might check whether the worker that 'owns' a task is still healthy before releasing its lock.\n   - However, such strategies are more complex to implement and may require additional infrastructure (service registries, heartbeats, etc.).\n\nOverall, the fix illustrates a common pattern: when behavior depends heavily on environment performance or workload characteristics, make that behavior configurable and integrate it cleanly into existing configuration and testing frameworks.",
        "procedural_memory": [
            "Step-by-step instructions on how to diagnose and fix similar issues.",
            "Step 1: Identify symptoms of premature timeout or duplicate work.\n- Look for tasks/actions/jobs that appear to be executed more than once, especially for long-running or heavy operations (large payloads, slow DB writes).\n- Check logs for GC or scheduler messages that indicate resetting a 'handling' or 'locked' flag, e.g., messages about re-scheduling or re-queuing tasks thought to be stuck.",
            "Step 2: Locate the timeout / GC logic.\n- Search the codebase for logic that decides when a task is considered orphaned or stuck (e.g., queue item selection with a timestamp comparison and a flag like `handling=True`).\n- Look for hard-coded constants controlling how far back in time the logic searches (e.g., `*_TIMEOUT_THRESHOLD_MS = 60 * 1000`).",
            "Step 3: Analyze whether the timeout is appropriate for your workloads.\n- Compare the hard-coded timeout value to the maximum or typical processing time for your long-running actions.\n- If legitimate tasks can exceed this timeout, the logic will misclassify them as stuck and cause them to be reprocessed.",
            "Step 4: Make the timeout configurable.\n- Introduce a new configuration option for this threshold in the appropriate config module (e.g., using oslo.config or your configuration framework):\n  - Choose a clear name and unit (e.g., `execution_scheduling_timeout_threshold_min` in minutes).\n  - Document it with a helpful description.\n  - Set the default to match the existing hard-coded behavior to preserve backwards compatibility.\n- Example (oslo.config):\n\n    cfg.FloatOpt(\n        'execution_scheduling_timeout_threshold_min',\n        default=1,\n        help='How long GC to search back in minutes for orphaned scheduled actions'\n    )",
            "Step 5: Use the config value where the constant was used.\n- Remove or deprecate the module-level hard-coded constant.\n- In the relevant class (e.g., scheduler handler), compute an instance attribute based on the config.\n- Replace references to the old constant with the new attribute.\n- Example:\n\n    self._execution_scheduling_timeout_threshold_ms = (\n        cfg.CONF.scheduler.execution_scheduling_timeout_threshold_min * 60 * 1000\n    )\n\n    # Later in query logic:\n    date.append_milliseconds_to_time(\n        date.get_datetime_utc_now(),\n        -self._execution_scheduling_timeout_threshold_ms\n    )",
            "Step 6: Register the option in test configuration and regenerate sample config.\n- Update any test config registration modules to include the new option so tests can access it without errors.\n- If the project uses a generated sample config file (like st2.conf.sample), run the appropriate generator (e.g., `make configgen`) to add the new option and ensure the sample reflects the code.\n- Fix any ordering or spacing issues indicated by CI, since some linters or tests compare the generated file exactly.",
            "Step 7: Add documentation and changelog entries.\n- Update the changelog to briefly describe the new configuration option and what problem it addresses.\n- Optionally update user/admin documentation to explain how to tune this value for slower or faster environments.",
            "Step 8: Test under realistic conditions.\n- Write or run tests that simulate long-running tasks to ensure they are not incorrectly reclassified as stuck before they complete.\n- Validate that increasing the timeout in configuration eliminates duplicate executions, while still allowing the GC or scheduler to recover truly orphaned tasks after the configured period.",
            "Step 9: Consider long-term robustness improvements.\n- Evaluate whether relying solely on timeouts is sufficient, or whether you should also implement health checks or service discovery to verify that the worker responsible for a task is actually down before releasing its lock.\n- Plan such enhancements separately, as they may require more substantial architectural changes."
        ]
    }
}