{
    "search_index": {
        "description_for_embedding": "Home Assistant androidtv integration was updated to androidtv==0.0.26 and its media_player tests were migrated from unittest.TestCase to standalone pytest tests. The new tests mock ADB connections via androidtv.adb_manager (both Python ADB and ADB server modes), verify reconnection and logging behavior using caplog, and assert entity availability and state transitions when ADB commands fail or return None.",
        "keywords": [
            "androidtv",
            "Home Assistant",
            "media_player",
            "pytest migration",
            "unittest removal",
            "ADB connection mocking",
            "androidtv.adb_manager",
            "ClientFake",
            "AdbCommandsFake",
            "STATE_UNAVAILABLE",
            "caplog",
            "logging assertions",
            "androidtv==0.0.26"
        ]
    },
    "agent_memory": {
        "episodic_memory": "In this change set, the Home Assistant androidtv integration was updated to depend on androidtv==0.0.26 instead of 0.0.25, and its tests were refactored to use pytest rather than unittest.TestCase.\n\nThe existing tests in tests/components/androidtv/test_media_player.py used unittest classes and direct patching of low-level ADB functions (adb.adb_commands.AdbCommands.ConnectDevice/Shell and adb_messenger.client.Client.device) to simulate connection success and failure. They also used assertLogs from unittest to validate log output. Core maintainers requested that new tests be written as standalone pytest functions and that failures not be caused by exceptions in the test code itself (e.g., accessing state.state on a None object), but rather by assertions about behavior.\n\nThe author first extracted the various mock helpers into a shared module tests/components/androidtv/patchers.py. Initial versions used simple function-level patches for connect_device_success/fail and shell callbacks. As androidtv 0.0.26 introduced or emphasized the androidtv.adb_manager abstraction, the patching strategy was revised to mock the higher-level classes used by the integration: AdbCommands and Client.\n\nThe final patchers implementation introduces:\n- AdbCommandsFake (base), AdbCommandsFakeSuccess, and AdbCommandsFakeFail to simulate the Python ADB implementation's connection attempts.\n- ClientFakeSuccess and ClientFakeFail to simulate the adb_messenger.client.Client used for ADB server connections, with a DeviceFake to represent attached devices.\n- helper functions patch_connect(success) to patch androidtv.adb_manager.AdbCommands and androidtv.adb_manager.Client based on desired success/failure, and patch_shell(response=None, error=False) to patch the Shell/shell methods for both Python ADB and server-based devices, either returning a given response or raising appropriate errors (AttributeError for Python ADB, ConnectionResetError for server ADB).\n\nThe actual tests were rewritten as asynchronous pytest functions using async_setup_component to configure media_player entities with various androidtv/Fire TV configurations:\n- CONFIG_ANDROIDTV_PYTHON_ADB: Android TV via Python ADB.\n- CONFIG_ANDROIDTV_ADB_SERVER: Android TV via ADB server.\n- CONFIG_FIRETV_PYTHON_ADB: Fire TV via Python ADB.\n- CONFIG_FIRETV_ADB_SERVER: Fire TV via ADB server.\n\nTwo core helper coroutines encapsulate the logic:\n1. _test_reconnect(hass, caplog, config):\n   - Uses patch_connect(True) and patch_shell(\"\") to set up the component and confirm the initial state is STATE_OFF.\n   - Then with patch_connect(False) and patch_shell(error=True), it triggers multiple updates, asserting the entity state becomes STATE_UNAVAILABLE, and uses caplog to confirm exactly two log records: an ERROR and then a WARNING about unavailability.\n   - Finally, with patch_connect(True) and patch_shell(\"1\"), it verifies reconnection. For ADB server, the first reconnection update already refreshes to STATE_IDLE; for Python ADB, the first update restores availability with the last-known state (STATE_OFF), and the second update moves the state to STATE_IDLE. The test asserts the presence of the appropriate success log message, differing for direct ADB vs ADB server.\n\n2. _test_adb_shell_returns_none(hass, config):\n   - Sets up the component with patch_connect(True) and patch_shell(\"\") and confirms that state is not STATE_UNAVAILABLE.\n   - Then calls update with patch_shell(None) combined with patch_shell(error=True) to simulate shell returning None and raising errors, and asserts the entity transitions to STATE_UNAVAILABLE. In all places, the code first checks that hass.states.get(entity_id) returns a non-None state object before accessing state.state.\n\nThe specific tests now are:\n- test_reconnect_androidtv_python_adb\n- test_adb_shell_returns_none_androidtv_python_adb\n- test_reconnect_firetv_python_adb\n- test_adb_shell_returns_none_firetv_python_adb\n- test_reconnect_androidtv_adb_server\n- test_adb_shell_returns_none_androidtv_adb_server\n- test_reconnect_firetv_adb_server\n- test_adb_shell_returns_none_firetv_adb_server\n\nEach test calls one of the helpers with appropriate configs. The tests verify correct logging semantics (one ERROR, one WARNING, then success message), correct handling of ADB shell errors and None responses, and correct use of STATE_UNAVAILABLE for devices that cannot be reached. They also ensure state objects from hass.states.get(...) are never assumed non-null without explicit checks.\n\nOverall, this PR modernized the test suite for the androidtv integration (pytest, async testing, caplog, shared patchers) while adapting mocks to the updated androidtv library's adb_manager API and retaining coverage for reconnection and error-handling behavior.",
        "semantic_memory": "This change illustrates several generalizable testing and integration practices:\n\n1. **Migrate from unittest to pytest with async-friendly patterns**:\n   - In async-heavy frameworks (like Home Assistant), prefer pytest over unittest.TestCase. Use async test functions (async def) and framework helpers (e.g., async_setup_component) to exercise integration logic in a realistic event loop context.\n   - Replace unittest.TestCase.assert* methods and assertLogs with plain assert statements and pytest fixtures such as caplog for capturing logs.\n\n2. **Mock external dependencies at the correct abstraction level**:\n   - When an integration relies on a library (here, androidtv) that itself wraps lower-level APIs (adb, adb_messenger), tests should patch the public or integration-facing abstractions (androidtv.adb_manager.AdbCommands, androidtv.adb_manager.Client) rather than patching low-level modules directly. This makes tests more robust to internal changes in the third-party library.\n   - Build small fake classes (e.g., AdbCommandsFakeSuccess/Fail, ClientFakeSuccess/Fail, DeviceFake) that implement only the methods the integration actually uses. Then provide helper functions like patch_connect(success) and patch_shell(response, error) that centralize mocking behavior.\n\n3. **Model multiple backend variants with shared test logic**:\n   - The same integration may support multiple backends or modes (Python ADB vs ADB server; Android TV vs Fire TV). Shared helper functions (_test_reconnect, _test_adb_shell_returns_none) can encapsulate common behavior, while configs select the target variant. This reduces duplicated code and ensures consistent behavioral expectations across modes.\n\n4. **Test logging behavior explicitly with caplog**:\n   - When an integration's quality criteria include logging behavior (e.g., \"log ERROR once on first failure, WARNING once when unavailable, log once when reconnected\"), tests should assert on the number, level, and content of log records captured via caplog.\n   - This guards against regressions in error handling and log noise.\n\n5. **Distinguish entity unavailability from absence of a state object**:\n   - In Home Assistant, an entity being unavailable is represented as a state object with state == STATE_UNAVAILABLE, not by hass.states.get(entity_id) returning None. Tests should:\n     - First confirm that hass.states.get(entity_id) is not None,\n     - Then assert on state.state.\n   - Avoid letting tests pass via AttributeError or other exceptions caused by the test code itself rather than by the integration.\n\n6. **Simulate transient failures and reconnection**:\n   - To test robustness, simulate sequences of failures (e.g., repeated ADB shell errors or connection failures) followed by successful connections. Validate that:\n     - The entity transitions to STATE_UNAVAILABLE during failures.\n     - Upon successful reconnection, the entity recovers state appropriately (possibly reusing the last known state on first reconnect, then refreshing on subsequent updates).\n     - Corresponding logs are emitted once per transition to avoid log spam.\n\nThese patterns are broadly applicable when testing integrations that depend on external services/devices, especially when they support multiple connection modes or have explicit availability and logging semantics.",
        "procedural_memory": [
            "When updating an integration to a new library version and modernizing tests, follow these steps:",
            "Step 1: Identify API changes and update imports/mocking targets",
            " - Inspect the new version of the external library (here androidtv==0.0.26). Determine which classes and functions your integration actually uses (e.g., androidtv.adb_manager.AdbCommands and androidtv.adb_manager.Client instead of direct adb.adb_commands / adb_messenger.client usage).",
            " - Update integration code imports if necessary, but ideally patch tests at the integration-facing abstraction (androidtv.adb_manager) rather than low-level internal modules.",
            "Step 2: Factor common mocking helpers into a shared module",
            " - Create a tests/components/<integration>/patchers.py module that defines fake classes and patch helpers used across tests (e.g., AdbCommandsFake, ClientFake, DeviceFake, patch_connect, patch_shell).",
            " - Implement success and failure variants of fakes, modeling only the behavior the integration actually cares about (e.g., ConnectDevice success/fail, shell command return values or errors).",
            "Step 3: Design robust fakes for external protocols",
            " - For connection-level behavior, make separate classes for success and failure conditions (AdbCommandsFakeSuccess vs AdbCommandsFakeFail; ClientFakeSuccess vs ClientFakeFail).",
            " - For command-level behavior, use helper functions such as patch_shell(response=None, error=False) that patch the Shell/shell methods to either return a specific response or raise appropriate errors (AttributeError for Python ADB, ConnectionResetError for server ADB).",
            " - Ensure you patch the correct fully-qualified paths as used inside the integration (e.g., f\"{module_name}.AdbCommandsFake.Shell\").",
            "Step 4: Migrate tests from unittest to pytest",
            " - Replace unittest.TestCase classes with standalone async test functions using pytest.",
            " - Use Home Assistant's async_setup_component to configure the integration under test and the hass fixture to interact with entity states and services.",
            " - Replace TestCase assertions (self.assertEqual, assertLogs, etc.) with plain assert statements and pytest fixtures such as caplog for log inspection.",
            "Step 5: Build shared test helpers for repeated behaviors",
            " - If the same behavior (e.g., reconnection logic, handling of shell returning None) needs to be tested for multiple configs (Android TV vs Fire TV, Python ADB vs ADB server), implement helper coroutines (like _test_reconnect and _test_adb_shell_returns_none) that take config and patching mode as arguments.",
            " - From each concrete test, call these helpers with the appropriate configuration to avoid duplicated logic and ensure consistency.",
            "Step 6: Assert on entity states carefully",
            " - When validating Home Assistant entity behavior, always get the state via hass.states.get(entity_id) and first assert that the result is not None.",
            " - Then assert on state.state (e.g., STATE_OFF, STATE_IDLE, STATE_UNAVAILABLE) to distinguish between a truly missing entity and an unavailable one.",
            " - Use multiple updates to simulate retries and reconnection, asserting that the state transitions follow the intended sequence.",
            "Step 7: Use caplog to validate logging semantics",
            " - Set caplog.set_level(logging.WARNING) or logging.DEBUG as needed to capture warning/error or debug-level messages.",
            " - After exercising the failure path, assert on len(caplog.record_tuples) and the log levels (e.g., first is ERROR, second is WARNING), and verify that specific success or failure messages appear exactly once.",
            "Step 8: Test both success and error paths for shell commands",
            " - First simulate a successful connection and shell command; ensure the entity is not STATE_UNAVAILABLE and has a reasonable default state (e.g., STATE_OFF).",
            " - Then simulate shell returning None and/or raising errors using patch_shell(None, error=True) and verify that the entity becomes STATE_UNAVAILABLE and logs the appropriate errors.",
            "Step 9: Run test suite and linters",
            " - Execute the full test suite (e.g., tox) to ensure that all tests pass under the new library version and pytest-based structure.",
            " - Address any failing tests by adjusting mocks or expectations to match the integration's actual behavior under the new dependency."
        ]
    }
}