{
    "search_index": {
        "description_for_embedding": "Optimized PokemonGo-Bot by introducing a time-based cache for gym detail API calls. Previously, the bot fetched details for up to 16 gyms every tick, but the API was throttled to two requests per second, causing ~8 seconds of delay each tick and hurting XP/hour. The new GymCache wraps get_gym_details, caches responses by gym_id with a 10-minute TTL, and reuses cached data for the web/map UI instead of repeatedly calling the remote API.",
        "keywords": [
            "performance optimization",
            "API throttling",
            "rate limiting",
            "TTL cache",
            "gym_cache.py",
            "get_gym_details",
            "PokemonGo-Bot",
            "repeated remote calls",
            "request caching",
            "XP/hr impact"
        ]
    },
    "agent_memory": {
        "episodic_memory": "In this incident, the PokemonGo-Bot was making excessive API calls to fetch gym details. During each update tick, the bot iterated over map cells and, for every non-Pokestop fort (i.e., gyms), called self.api.get_gym_details. With up to 16 gyms per tick and an API throttle of two requests per second, each tick incurred roughly 8 seconds of blocked time just to gather gym data that was only used for a UI map JSON file, not for core bot behavior. This directly reduced XP/hour and overall responsiveness. There was even discussion about whether gym data should be fetched at all, but the chosen solution for this PR was to keep the feature while optimizing it.\n\nTo fix this, a new GymCache class was introduced in pokemongo_bot/gym_cache.py. The bot now instantiates GymCache (self.gym_cache = GymCache(self)) and, in update_web_location, replaces direct calls to self.api.get_gym_details with self.gym_cache.get(...). The GymCache maintains an in-memory dictionary mapping gym_id to the API response. When get(...) is called, it checks if the gym_id is already in the cache. If not, it makes a single get_gym_details API call, stores the response in the cache, and then returns it. Each access updates a 'last_accessed' timestamp on the cached record. A configurable TTL (default 10 minutes) is enforced by _remove_stale_gyms, which purges entries whose last_accessed time is older than cache_length_seconds.\n\nAs a result, repeated requests for the same gym details within the TTL window no longer hit the external API, effectively eliminating the 8-second-per-tick delay caused by gym detail fetching while preserving the existing UI functionality that relies on gym information.",
        "semantic_memory": "This fix illustrates a classic pattern in performance optimization for systems that depend on external, rate-limited APIs: when the same data is requested frequently and does not change rapidly, introduce a cache with an appropriate time-to-live (TTL).\n\nKey generalizable concepts:\n- **Avoid unnecessary repeated remote calls**: When a feature repeatedly fetches the same external data (e.g., gym details for a map UI) and the data is relatively stable over short periods, it is inefficient to call the API every time. Instead, cache responses and reuse them.\n- **Rate-limited APIs magnify inefficiencies**: If an API is throttled (two requests per second in this case), naive repeated calls compound into long delays (16 calls â†’ ~8 seconds). Reducing the number of calls can have disproportionate performance benefits (e.g., XP/hour in a game bot, throughput in a service).\n- **Layered responsibilities**: The core bot logic did not need gym data; it was only used for visualization. When UI-only data is fetched by core logic, it is especially important to minimize its impact (via caching, batching, or even removing it) so that nonessential features do not degrade primary behavior.\n- **TTL cache design**: A simple in-memory cache keyed by a stable identifier (gym_id) plus a 'last_accessed' timestamp and a TTL is often sufficient. The cache should (a) transparently fetch on miss, (b) refresh timestamps on access, and (c) periodically prune stale entries to avoid unbounded growth.\n- **Encapsulation of external calls**: Wrapping the remote call (get_gym_details) behind a dedicated component (GymCache) makes it easier to swap implementations later (e.g., different caching strategies, disabling gym fetching entirely, or mocking for tests) without touching call sites.\n\nThese ideas apply generally to any system that repeatedly fetches relatively static data from a rate-limited or high-latency external service.",
        "procedural_memory": [
            "Step-by-step approach to diagnosing and fixing performance issues caused by repeated external API calls:",
            "Step 1: Profile or instrument the system to identify slow operations.\n- Measure how long each iteration of your main loop or request processing takes.\n- Log or count external API calls per time interval.\n- Confirm whether specific endpoints (e.g., get_gym_details) are being called excessively.",
            "Step 2: Analyze the necessity and frequency of those calls.\n- Determine whether the data returned by the API is needed that often.\n- Check whether it is used for core functionality or only for secondary/UI-related features.\n- Assess how frequently the data realistically changes (seconds, minutes, hours).",
            "Step 3: Decide on a suitable caching strategy.\n- If data is relatively stable and keyed by a clear identifier (e.g., gym_id), choose a key-based cache.\n- Select a TTL based on how fresh the data must be (e.g., 10 minutes for gym details).\n- Decide whether you need per-request invalidation or if time-based expiration is enough.",
            "Step 4: Implement a cache wrapper for the external API call.\n- Create a dedicated class/module (e.g., GymCache) that:\n  - Accepts the necessary context (the API client, configuration) in its constructor.\n  - Exposes a get(...) method mirroring the original API signature.\n  - On cache miss: calls the real API, stores the result in a dictionary keyed by an identifier, and returns it.\n  - On cache hit: returns the stored result.\n- Add a 'last_accessed' or 'updated_at' timestamp to each cached entry.",
            "Step 5: Add expiration and cleanup logic.\n- Store a configurable cache_length_seconds (TTL).\n- Each time you access the cache, update last_accessed.\n- Periodically (e.g., on each get or via a background job) iterate over cached entries and remove any whose last_accessed is older than now - TTL.\n- When implementing cleanup, avoid mutating the dictionary while iterating over it; instead, collect keys to delete in a list, then delete them in a separate loop.",
            "Step 6: Integrate the cache into call sites.\n- Replace direct calls to the external API (self.api.get_gym_details(...)) with calls to the cache wrapper (self.gym_cache.get(...)).\n- Ensure the function signatures match and all required parameters are passed.\n- Keep the calling code otherwise unchanged so behavior remains the same aside from performance.",
            "Step 7: Test and validate improvements.\n- Confirm that functionality is preserved (UI still receives expected data).\n- Re-profile: measure the number of external calls and the time per loop/request before and after the change.\n- Ensure the rate limit is no longer hit unnecessarily and that overall throughput (e.g., XP/hour for a bot) has improved.",
            "Step 8: Reassess feature necessity if performance is still poor.\n- If even cached behavior imposes unacceptable overhead or complexity, reconsider whether the feature is needed at all.\n- For purely cosmetic/UI features in performance-sensitive systems, disabling them may be the simplest and most robust solution."
        ]
    }
}