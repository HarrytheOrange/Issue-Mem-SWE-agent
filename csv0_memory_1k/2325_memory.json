{
    "search_index": {
        "description_for_embedding": "Fix slow import times in napari by avoiding importing heavy array libraries (like xarray, dask, zarr) inside a generic equality helper. Instead, determine the correct equality operator via class MRO and module namespace strings, and add an import-time test to ensure pkg_resources is not pulled in during napari import.",
        "keywords": [
            "napari",
            "import time",
            "slow imports",
            "pkg_resources",
            "xarray",
            "dask",
            "zarr",
            "numpy",
            "pick_equality_operator",
            "equality operator",
            "namespace-based type detection",
            "MRO inspection",
            "pytest",
            "CI",
            "MIN_REQ",
            "performance regression"
        ]
    },
    "agent_memory": {
        "episodic_memory": "In this incident, the napari project noticed that importing the library was slower than expected. Investigation (including a report from an external contributor) showed that importing xarray indirectly pulled in pkg_resources, which is known to be slow. The root cause was a utility function, pick_equality_operator, in napari/utils/misc.py that imported dask.array, zarr, and xarray purely to perform isinstance/issubclass checks when deciding which equality operator to use (e.g., numpy.array_equal vs operator.is_ vs operator.eq).\n\nOriginally, pick_equality_operator used direct imports and issubclass checks like:\n- import dask.array; if issubclass(type_, dask.array.Array): ...\n- import zarr.core; if issubclass(type_, zarr.core.Array): ...\n- import xarray.core.dataarray; if issubclass(type_, xarray.core.dataarray.DataArray): ...\nThis meant that importing napari always imported these heavy libraries, even if the user never used them, and xarray in particular dragged in pkg_resources, significantly increasing import time.\n\nThe fix refactored pick_equality_operator to avoid importing those libraries at all. Instead, it now inspects the object's type and its MRO (method resolution order) and matches types by their top-level module name and class name. A mapping _known_arrays is defined:\n- 'numpy.ndarray' -> numpy.array_equal\n- 'dask.Array' -> operator.is_\n- 'zarr.Array' -> operator.is_\n- 'xarray.DataArray' -> numpy.array_equal\nFor each base class in type_.mro(), the function computes key = f\"{base.__module__.split('.', maxsplit=1)[0]}.{base.__name__}\" and looks it up in _known_arrays. If a match is found, it returns the corresponding equality function; otherwise it falls back to operator.eq. This preserves behavior for numpy arrays, dask arrays, zarr arrays, xarray DataArray, and subclasses (verified via a custom MyNPArray subclass in tests) without importing those libraries inside napari.utils.misc.\n\nTests were added in napari/utils/_tests/test_misc.py to validate that:\n- numpy.ndarray and its subclass use numpy.array_equal,\n- dask.array.Array and zarr.core.Array use operator.is_,\n- xarray.DataArray uses numpy.array_equal.\n\nAdditionally, a new import-time regression test was created in napari/_tests/test_import_time.py. It runs a separate Python process with `-X importtime` to import napari, captures the stderr log, and prints the measured import time. Due to instability of timing in CI environments, the explicit time threshold check was commented out. The test instead focuses on ensuring that 'pkg_resources' does not appear in the import log, since that module is a common source of slow imports.\n\nDuring CI integration, the team observed that the import-time test failed in the Python 3.7 minimum requirements (min_req) environment because older versions of dependencies there still imported pkg_resources. To avoid spurious failures, they added a pytest skip condition based on an environment variable MIN_REQ, and updated GitHub Actions workflows and tox.ini to propagate MIN_REQ into the test environment. Thus, the import-time test runs on normal environments but is skipped on MIN_REQ builds where legacy dependencies may still pull in pkg_resources.\n\nAfter these changes, napari imports faster, does not require xarray/dask/zarr just to pick an equality operator, and CI now includes an import-time smoke test ensuring pkg_resources is not unexpectedly imported.",
        "semantic_memory": "This fix illustrates several generalizable best practices around import performance and type-dependent behavior:\n\n1. **Avoid heavyweight imports in core or frequently-imported modules**: Utility functions that are imported as part of a library's top-level import path should not eagerly import optional or heavy dependencies, especially if they are only needed for type checks or rarely executed code paths. Instead, use strategies such as lazy imports, optional imports, or metadata-based checks.\n\n2. **Use namespace-based or structural checks instead of direct imports for optional types**: When behavior depends on the type of an object from optional dependencies (e.g., xarray.DataArray, dask.array.Array, zarr.core.Array), it's possible to infer the type from the object's class without importing the library module. Here, using `type(obj).mro()` and inspecting `base.__module__` and `base.__name__` allowed mapping object types to behaviors without importing the corresponding packages.\n\n3. **Beware of transitive import costs**: Importing one library (e.g., xarray) can transitively import other expensive libraries (e.g., pkg_resources). These transitive imports may be invisible at the call site but significantly impact startup time. Performance-aware code should either avoid importing such libraries at import time or confine those imports to execution paths that are not part of the initial startup.\n\n4. **Testing import performance and dependencies**: You can use Python's `-X importtime` flag to measure which imports cost time and verify that certain problematic modules (like pkg_resources) are not imported. While absolute timing thresholds are often too flaky for CI, asserting absence/presence of specific modules in the import log is a robust way to detect regressions.\n\n5. **CI-aware performance tests**: Performance tests must account for differing environments (e.g., minimum requirements, older dependencies) that can have different import behaviors. Using environment variables and pytest markers (`pytest.mark.skipif`) allows selectively skipping fragile performance checks in environments where failures would be misleading.\n\n6. **MRO-based subclass handling**: When mapping classes to behaviors, iterating over `type_.mro()` enables correct behavior for subclasses. This pattern is useful when you want to support not only base classes (like numpy.ndarray) but also user-defined subclasses without explicit registration.\n\nOverall, the pattern is: minimize imports in hot paths, use introspection over objects instead of importing their defining modules when only type identity is needed, and add targeted tests to guard against regressions in import-time performance and unwanted dependency import chains.",
        "procedural_memory": [
            "Diagnosing and fixing slow import times caused by optional/heavy dependencies and type checks:",
            "Step 1: Detect and measure slow imports.",
            " - Use Python's `-X importtime` flag to profile imports: `python -X importtime -c \"import your_package\"`.",
            " - Inspect the stderr output to see which modules contribute heavily to import time.",
            " - Look for unexpected or heavy transitive imports (e.g., `pkg_resources`, GUI backends, or scientific stacks that shouldn't be required at startup).",
            "Step 2: Identify where the heavy imports are triggered.",
            " - Search your codebase for direct imports of the heavy modules (e.g., `import xarray`, `import dask`, `import zarr`).",
            " - Pay particular attention to modules that are imported during your packageâ€™s top-level import (e.g., __init__.py or common utilities).",
            " - Cross-check with the importtime log to correlate specific imports with time spikes.",
            "Step 3: Determine whether those imports are actually needed at import time.",
            " - Ask: Is the imported library required for normal startup, or only for specific functionality (e.g., type checks, plugin support, optional IO formats)?",
            " - If it is only used for type-based behavior (e.g., selecting an equality operator), consider alternatives that don't require importing the library itself.",
            "Step 4: Replace direct imports with introspection or lazy strategies where possible.",
            " - If you only need to know \"what kind of object is this?\", prefer inspecting the object's type rather than importing its defining module.",
            " - Example pattern:\n        - Define a mapping of `\"top_level_module.ClassName\" -> behavior` (e.g., function, operator).\n        - For the object `obj`, compute `type_ = type(obj) if not inspect.isclass(obj) else obj`.\n        - Iterate over `type_.mro()` and for each base class:\n            - Compute `key = f\"{base.__module__.split('.', maxsplit=1)[0]}.{base.__name__}\"`.\n            - Look up `key` in the mapping; if found, return the associated behavior.\n        - Fall back to a default behavior (e.g., `operator.eq`) if nothing matches.\n      - This avoids importing the heavy modules (xarray, dask, zarr) just to perform `issubclass` checks.",
            "Step 5: Ensure subclass compatibility via MRO.",
            " - Use `type_.mro()` instead of checking only the concrete class so subclasses (e.g., custom numpy.ndarray subclasses) inherit the expected behavior.",
            " - Add tests that create simple subclasses of key array types to verify they use the same equality operator/behavior as the base class.",
            "Step 6: Add tests to guard against import-time regressions.",
            " - Write a test that spawns a subprocess and runs `python -X importtime -c 'import your_package'`.",
            " - Capture stderr and parse it:\n        - Optionally print the total import time for information.\n        - Assert that specific undesirable modules (e.g., `pkg_resources`) do not appear in the log.\n      - Avoid strict timing assertions in CI (they are often flaky); focus on module presence/absence or relative comparisons, unless your CI environment is tightly controlled.",
            "Step 7: Make the tests robust in CI environments.",
            " - Identify environments where dependencies differ (e.g., minimum requirements builds, old Python versions) and where legacy dependencies may still import slow modules.",
            " - Use environment variables and pytest marks to skip fragile performance tests in such environments, e.g.:\n        - Set `MIN_REQ` in CI matrix for minimum requirements jobs.\n        - In the test: `@pytest.mark.skipif(bool(os.environ.get('MIN_REQ')), reason='skip import time test on MIN_REQ')`.",
            " - Ensure that CI configuration (GitHub Actions, tox.ini) passes these environment variables through so pytest can see them.",
            "Step 8: Verify and document the change.",
            " - Run the full test suite to ensure functionality is unchanged and import-time tests pass.",
            " - Optionally compare pre- and post-fix import times locally to confirm the improvement.",
            " - Add comments to the refactored code explaining the rationale (e.g., \"checking namespaces instead of issubclass to avoid slow imports\"), so future maintainers understand why this indirection exists.",
            "Step 9: Monitor for regressions.",
            " - Keep the import-time test in CI so that future changes that accidentally reintroduce heavy imports (like xarray or pkg_resources) during top-level import are caught early.",
            " - When adding new optional integrations, check that they don't unintentionally affect import-time by only being imported lazily or via introspection where appropriate."
        ]
    }
}