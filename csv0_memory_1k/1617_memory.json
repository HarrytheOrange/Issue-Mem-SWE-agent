{
    "search_index": {
        "description_for_embedding": "Added a CI matrix job that tests napari against its minimum supported dependency versions by pinning install_requires from >= to == at test time, fixed related failures with old numpy and dask (NEP-18/NUMPY_EXPERIMENTAL_ARRAY_FUNCTION), upgraded napari-svg and ipykernel to avoid runtime errors with numpy.nan_to_num and jupyter_client ChannelABC, and kept briefcase/pyproject requirements and manifest/check-manifest configuration in sync.",
        "keywords": [
            "CI",
            "test matrix",
            "minimum requirements",
            "min dependencies",
            "setup.cfg",
            "tools/minreq.py",
            "numpy 1.16",
            "NEP-18",
            "NUMPY_EXPERIMENTAL_ARRAY_FUNCTION",
            "dask array",
            "napari-svg",
            "nan_to_num nan keyword",
            "TypeError ChannelABC takes no arguments",
            "ipykernel>=5.2.0",
            "jupyter_client",
            "briefcase",
            "pyproject.toml",
            "check-manifest",
            "test_bundle_requirements",
            "multichannel dask test"
        ]
    },
    "agent_memory": {
        "episodic_memory": "The team wanted to ensure that napari is actually compatible with the minimum versions of its declared dependencies. To do this, they added a new CI matrix entry that runs tests with Python 3.7 and all dependencies pinned to their minimum versions.\n\nImplementation: A new script tools/minreq.py was introduced. It reads setup.cfg with ConfigParser and replaces all occurrences of \">=\" with \"==\" in install_requires and in the qt extras (pyside2, pyqt5). CIRRUS CI is configured so that, for one Linux job, the environment variable MIN_REQ=1 is set. The test setup then runs `python tools/minreq.py` before installing napari, effectively pinning dependencies to their minima at test-time without changing the committed setup.cfg. The script is guarded by `os.environ.get('MIN_REQ', '') == '1'` so it is a no-op for normal jobs.\n\nThis change exposed several issues:\n\n1) napari-svg with old numpy\n\nTests failed with:\n`TypeError: nan_to_num() got an unexpected keyword argument 'nan'` in napari_svg/hook_implementations.py when running against numpy 1.16. The napari-svg plugin was using np.nan_to_num(..., nan=0), but the `nan` keyword is only available in newer numpy. The plugin was fixed in napari-svg 0.1.4, and napari's dependency on napari-svg was bumped from >=0.1.3 to >=0.1.4 in both setup.cfg and pyproject.toml (briefcase requires), resolving that error.\n\n2) ipykernel / Jupyter console error with ChannelABC\n\nThe Qt console integration failed with:\n`TypeError: ChannelABC() takes no arguments` when jupyter_client tried to construct a control channel for a QtInProcessKernelClient. This is a known compatibility issue with older ipykernel/jupyter_client. The fix was to bump napari's ipykernel minimum from >=5.1.1 to >=5.2.0 in setup.cfg and pyproject.toml, which aligns with upstream recommendations and fixes the ChannelABC construction error.\n\n3) check-manifest and coverage exclusions\n\nAdding tools/minreq.py caused `check-manifest` to complain that it was missing from the sdist. Rather than shipping this internal script, they added it to the [tool.check-manifest] ignore list in pyproject.toml and to the coverage exclude regex in the same file, so that check-manifest stops flagging it and coverage tools ignore it.\n\n4) briefcase bundle requirements mismatch\n\nThe minimum-requirements pinning changed the exact requirement strings in setup.cfg (e.g. `numpy>=1.16.0` becomes `numpy==1.16.0`), and a test (test_bundle_requirements) compares the install_requires entries from setup.cfg to the list under [tool.briefcase.app.napari.requires] in pyproject.toml. Initially, napari-svg was still at 0.1.3 in pyproject.toml, so the sets did not match, causing the test to fail. Updating pyproject.toml's briefcase requires to use `napari-svg>=0.1.4` fixed that mismatch for this PR. There was also a broader discussion about whether to dynamically derive briefcase requirements from setup.cfg to avoid duplication, but that refactor was deferred to a future PR.\n\n5) Dask multichannel test with numpy 1.16 (NEP-18)\n\nA test, test_multichannel_dask_array, expected that adding a multichannel dask array results in each channel layer's .data still being a dask array. Under numpy 1.16 without NEP-18's array_function protocol enabled, certain numpy functions (e.g. numpy.take used inside add_layer_mixin) return plain numpy arrays instead of dispatching to dask, causing the assertion `isinstance(layer.data, da.Array)` to fail. Initially the test tried to account for type differences by checking da.core.Array, but the underlying issue was the missing NEP-18 behavior.\n\nThe fix involved two changes:\n- The test was relaxed to assert `isinstance(viewer.layers[i].data, type(data))`, meaning the layer data must have the same type as the input array (dask array in this test), without hard-coding the exact dask class.\n- For the minimum-requirements matrix (numpy 1.16), the environment variable `NUMPY_EXPERIMENTAL_ARRAY_FUNCTION=1` was set in .cirrus.yml for the MIN_REQ:1 job. This enables NEP-18's experimental array_function protocol in numpy 1.16 so numpy.take and similar functions dispatch correctly to dask, preserving the dask array type and making the test pass.\n\n6) Pre-commit configuration fixes\n\nWhile here, they updated the pre-commit configuration to use the new isort repository location (pycqa/isort), set default_language_version.python to python3.8, and fixed a typo in the key name (`defalut_language_version` -> `default_language_version`).\n\nAfter these changes, the new CI job running napari with pinned minimum dependency versions and numpy 1.16 passes, confirming that the library actually supports the declared minimum versions.",
        "semantic_memory": "Generalizable insights from this incident:\n\n1) Testing against minimum supported dependency versions is critical.\nDeclaring `package>=X.Y` in install_requires does not guarantee that your project actually works with X.Y. If you never test against the minimums, your code often silently starts depending on features introduced in newer versions. A dedicated CI job that pins dependencies to their minimum versions and runs the full test suite is a robust way to validate claims about supported versions.\n\n2) You can dynamically pin requirements for CI only.\nInstead of committing a separate 'min requirements' file, you can create a small tool that rewrites your configuration (e.g., setup.cfg) to pin `>=` to `==` at CI runtime when a specific environment variable is set. This keeps your repository's canonical metadata simple while allowing your CI to exercise minimum versions. The key is to ensure that this script is only run in controlled environments (e.g., when MIN_REQ=1) and excluded from distribution/coverage if it is not part of the public API.\n\n3) Keep dependency version declarations in sync across files.\nProjects often duplicate dependency lists in multiple places: setup.cfg/pyproject.toml, briefcase/packaging configs, docs, etc. This duplication easily leads to mismatches, especially around version bumps (like napari-svg 0.1.3 -> 0.1.4 or ipykernel 5.1.1 -> 5.2.0). Either centralize dependencies in a single source of truth or add tests that assert the lists are in sync (as test_bundle_requirements does). When those tests fail, it is often a sign that one config was updated and another wasn't.\n\n4) Old dependency versions can require environment flags to enable newer protocols (NEP-18 and numpy 1.16).\nNumpy's NEP-18 array_function protocol is not fully enabled in numpy 1.16 by default. If you intend to support array libraries relying on array_function (like dask for numpy.take), you may need to set the `NUMPY_EXPERIMENTAL_ARRAY_FUNCTION=1` environment variable before importing numpy. This is a subtle but important compatibility issue when you claim to support older numpy versions. More broadly, features that are 'experimental' in old versions may require runtime configuration to behave in the modern way.\n\n5) Tests should assert on behavior, not brittle internal types.\nThe dask test was originally asserting isinstance(layer.data, da.Array), which broke due to class location/implementation differences and numpy/dask interaction for older versions. A more robust approach is to check that the output has the same type as the input (type(data)) or to assert on semantic properties (e.g., that it is lazily evaluated or has specific attributes), rather than depending on a particular class path. This makes tests more resilient to internal library refactors.\n\n6) Minimum-version testing will surface transitive dependency bugs.\nThe nan_to_num `nan` keyword error came from napari-svg, a plugin dependency of napari. Testing napari at its minimum numpy version exposed that napari-svg implicitly required a newer numpy API. Similar issues are common: your project may be compatible with old versions, but plugins/transitive dependencies might not be. Minimum dependency CI helps you catch these alignment problems early, prompting you to bump or constrain plugin versions.\n\n7) Fixing tricky ecosystem issues sometimes requires bumping minimums.\nIn the Jupyter ecosystem, certain combinations of jupyter_client, qtconsole, ipykernel, etc., are simply broken (e.g., ChannelABC() takes no arguments). In such cases, maintaining support for an older version can require large workaround patches. It is often more pragmatic to raise your minimum dependency version (ipykernel>=5.2.0) to a known-good combination.\n\n8) Linting and tooling configs should be kept current with their ecosystems.\nTools like pre-commit hooks and isort migrate or deprecate old repositories and configuration keys. Regularly updating these references (repo URLs, key names like default_language_version) prevents your developer tooling from silently breaking or using outdated defaults.",
        "procedural_memory": [
            "Step-by-step pattern for adding and debugging a 'minimum requirements' CI job and related ecosystem issues:",
            "Step 1: Add a CI matrix entry that runs with minimum dependency versions.",
            " - Define a new job in your CI config (e.g., .github/workflows, .cirrus.yml) that uses one supported Python version and sets an environment variable such as MIN_REQ=1.",
            " - Ensure this job installs test requirements and then modifies your dependency spec to pin to minimum versions before installation.",
            "Step 2: Implement a script to pin requirements at CI runtime.",
            " - Create a small script (e.g., tools/minreq.py) that uses ConfigParser (or tomlkit, etc.) to read your configuration (setup.cfg or pyproject.toml).",
            " - Replace version specifiers from \">=\" to \"==\" in the install_requires and any relevant extras (e.g., GUI backends) so that your CI job actually installs the minimum versions.",
            " - Guard the script with an environment variable check (e.g., only run when os.environ.get('MIN_REQ') == '1') so it is a no-op in normal development or release flows.",
            " - In CI, run this script before `pip install -e .[...]`.",
            "Step 3: Keep tooling and distributions aware of the new script.",
            " - If the script is internal and not meant for distribution, add it to the check-manifest ignore list and to test coverage exclusions to avoid noise (e.g., in pyproject.toml [tool.check-manifest].ignore and coverage omit regex).",
            "Step 4: Run the min-req job and collect failures.",
            " - Trigger the new CI job and note any failing tests or runtime errors. These will usually be version-compatibility issues with your own code or with plugins/transitive dependencies.",
            "Step 5: Fix transitive dependency issues by bumping plugin versions and aligning configs.",
            " - If a plugin fails due to using newer APIs than your minimum (e.g., napari-svg using np.nan_to_num(nan=...), which does not exist in old numpy), fix the plugin or require a newer plugin version.",
            " - Update all places that declare that dependency (e.g., setup.cfg install_requires and pyproject.toml tool.briefcase.app.<app>.requires) to the new minimum version.",
            " - Rerun tests and confirm that related errors disappear.",
            "Step 6: Resolve Jupyter / kernel / console compatibility errors by aligning ecosystem versions.",
            " - For errors like `TypeError: ChannelABC() takes no arguments` from jupyter_client when using QtConsole, consult upstream issues to find the minimal working ipykernel/jupyter_client versions.",
            " - Bump the minimum version of ipykernel (or other relevant packages) in all requirement lists (setup.cfg and pyproject.toml) to that known-good version (e.g., ipykernel>=5.2.0).",
            "Step 7: Maintain synchronization between multiple dependency declarations.",
            " - If your project also declares dependencies in other files (e.g., briefcase app requirements, conda environment files), update them to match your canonical install_requires when you bump versions.",
            " - Optionally, add or maintain tests that compare these lists (e.g., test_bundle_requirements) to ensure they stay in sync (remember to normalize strings if necessary when doing set comparisons).",
            "Step 8: Address array library compatibility (numpy/dask/NEP-18).",
            " - If tests involving dask or other array libraries fail under older numpy (e.g., dask arrays becoming numpy arrays after operations like numpy.take), check whether numpy's array_function protocol (NEP-18) is enabled.",
            " - For numpy 1.16, enable NEP-18 in the relevant CI job by setting `NUMPY_EXPERIMENTAL_ARRAY_FUNCTION=1` before numpy is imported.",
            " - Ensure your CI config sets this environment variable for the minimum numpy job only, if newer numpy versions do not require it.",
            "Step 9: Make tests robust to minor type differences.",
            " - When testing compatibility with backend arrays (dask, cupy, etc.), avoid hard-coding class paths (like da.Array).",
            " - Prefer checks like `isinstance(result, type(input_array))` or property-based assertions (lazy evaluation, presence of specific attributes) to make tests more resilient across versions and refactors.",
            "Step 10: Update developer tooling configs as needed.",
            " - If you modify Python versions or tools, align your pre-commit configuration: set `default_language_version.python` to your desired default (e.g., python3.8).",
            " - Update repos for hooks that moved or were renamed (e.g., isort now at pycqa/isort) and fix typos in config keys so pre-commit runs correctly.",
            "Step 11: Re-run the full CI matrix and iterate.",
            " - After each round of fixes, run all CI jobs, especially the min-req job, and ensure no new failures appear.",
            " - Once the min-req job is green, you can be much more confident that the project truly supports the declared minimum dependency versions."
        ]
    }
}