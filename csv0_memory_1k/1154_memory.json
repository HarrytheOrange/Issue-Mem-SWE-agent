{
    "search_index": {
        "description_for_embedding": "Added an `interval` parameter to Optuna's PyTorchLightningPruningCallback so pruning is only evaluated every N epochs instead of at every epoch. The callback now skips pruning checks on epochs that are not multiples of the interval, and tests were added/parametrized to verify pruning behavior for different combinations of `interval` and `max_epochs`.",
        "keywords": [
            "Optuna",
            "PyTorchLightningPruningCallback",
            "pytorch_lightning",
            "pruning interval",
            "skip pruning epochs",
            "on_epoch_end",
            "hyperparameter optimization",
            "integration tests",
            "DeterministicPruner",
            "early stopping"
        ]
    },
    "agent_memory": {
        "episodic_memory": "In this PR, the Optuna integration for PyTorch Lightning's pruning callback was extended to support skipping pruning checks on some epochs. Previously, `PyTorchLightningPruningCallback` evaluated pruning at the end of every epoch. This could be inefficient or undesirable for users who wanted to let models run for several epochs before making pruning decisions.\n\nThe change introduced an `interval` argument to `PyTorchLightningPruningCallback.__init__`, defaulting to 1 to preserve backward-compatible behavior. A new instance variable `self._interval` was added, and `on_epoch_end` was updated to return early unless `(epoch + 1) % self._interval == 0`. This means pruning is only considered on epochs that are exact multiples of `interval` (1-based indexing).\n\nExample: with `interval=7` and `max_epochs=7`, pruning will be checked once at epoch 7. With `interval=7` and `max_epochs=6`, pruning is never checked.\n\nThe tests in `tests/integration_tests/test_pytorch_lightning.py` were enhanced with a parametrized test `test_pytorch_lightning_pruning_callback_with_interval`. Using `pytest.mark.parametrize(\"interval, max_epochs\", [(6, 6), (7, 6), (7, 7)])`, the test verifies:\n- When `interval == max_epochs` (e.g., `(6, 6)` or `(7, 7)`), pruning is evaluated and the trial is PRUNED (with a `DeterministicPruner(True)` to force pruning when checked).\n- When `interval > max_epochs` (e.g., `(7, 6)`), pruning is never triggered and the trial completes successfully with a value of 1.0.\n\nThe test sets up a `pl.Trainer` with `early_stop_callback=PyTorchLightningPruningCallback(trial, monitor=\"accuracy\", interval=interval)` and disables unrelated checkpoint callbacks. A simple `Model` is trained. The study's trial state is then asserted to be either `PRUNED` or `COMPLETE` depending on the interval and max_epochs combination.\n\nThe PR was also partially motivated by broader design discussions and PRs around PyTorch Lightning version compatibility, but this specific change focused solely on adding interval-based pruning to the existing callback signature and updating tests.",
        "semantic_memory": "This change illustrates a common pattern in training-loop integrations with hyperparameter optimization frameworks: providing configurable pruning or early-stopping frequency instead of checking at every iteration/epoch.\n\nKey generalizable concepts:\n1. **Configurable evaluation frequency**: Rather than performing expensive or disruptive checks (like pruning or validation) at every step, expose an `interval` or `frequency` parameter (e.g., every N epochs or steps). This can significantly reduce overhead and allow the model to make more progress before decisions are made.\n\n2. **Backward-compatible defaults**: When introducing such parameters, default them to values that preserve existing behavior (e.g., `interval=1` means check every epoch). This avoids breaking existing user code and tests.\n\n3. **Early return guard in callbacks**: Implement interval-based behavior by adding a simple guard at the start of the callback method, e.g.,\n   - `if (epoch + 1) % interval != 0: return`. This keeps the rest of the callback logic untouched and easy to reason about.\n\n4. **Boundary and edge-case testing**: When adding interval logic, test the boundaries explicitly:\n   - `interval == max_epochs`: pruning/check happens exactly once at the end.\n   - `interval > max_epochs`: pruning is never triggered.\n   - Potentially `interval < max_epochs`: multiple pruning checks.\n   Parametrized tests (`pytest.mark.parametrize`) are well-suited to systematically cover these combinations.\n\n5. **Deterministic testing of pruning logic**: Use deterministic pruners (or mocks) in tests to ensure that when the pruning callback is invoked, the outcome is predictable (always prune or never prune). This isolates and validates the timing logic rather than the pruning algorithm itself.\n\n6. **API ergonomics in callbacks**: When adding features to training framework integrations (like PyTorch Lightning callbacks), keep the signature clean and the documentation clear about what each parameter controls (e.g., `interval` controls when pruning is checked, not how pruning decisions are made). This separation of concerns reduces user confusion and implementation complexity.",
        "procedural_memory": [
            "Step-by-step instructions on how to diagnose and fix similar issues.",
            "Step 1: Identify the pain point or inefficiency.\n- Observe that a callback (e.g., pruning, early stopping) is being run too frequently (e.g., every epoch) and may be causing overhead or premature decisions.\n- Confirm with users or issue reports that they want more control over how often the callback should run (e.g., only every N epochs).",
            "Step 2: Design a minimal API extension.\n- Add an `interval` (or similar) parameter to the callback's constructor, defaulting to the current behavior (e.g., `interval=1`).\n- Document the parameter clearly: describe what it controls, its default value, and how it affects runtime behavior.\n- Ensure type hints are updated (e.g., `(trial: Trial, monitor: str, interval: int) -> None`).",
            "Step 3: Implement interval-based guarding logic.\n- In the callback hook that is called repeatedly (e.g., `on_epoch_end`), add an early-exit guard:\n  - Use 1-based epoch semantics if that matches the framework: `if (epoch + 1) % self._interval != 0: return`.\n- Keep the existing logic (logging, metric lookup, pruning decision) unchanged after the guard.\n- Ensure that the state (e.g., `self._trial`, `self._monitor`) is left intact when skipping.",
            "Step 4: Add or update tests with parametrization.\n- Create or expand integration tests that exercise different combinations of `interval` and total training duration.\n- Use `pytest.mark.parametrize` to cover cases such as:\n  - `interval == max_epochs` → pruning check should run exactly once; with a deterministic pruner that always prunes, the trial should end as PRUNED.\n  - `interval > max_epochs` → pruning callback should never run; the trial should complete successfully.\n  - Optionally, `interval < max_epochs` → pruning should be evaluated multiple times.\n- Use a deterministic or stubbed pruner (e.g., `DeterministicPruner(True)`) so that when pruning is checked, the outcome is predictable.",
            "Step 5: Verify backward compatibility.\n- Run existing tests that use the callback without the `interval` parameter to ensure behavior is unchanged (since `interval` defaults to 1).\n- Confirm that code using the old constructor signature still works or that Python’s default argument preserves compatibility.",
            "Step 6: Check framework version compatibility.\n- Ensure the callback method signature (`on_epoch_end`, etc.) matches the version(s) of the external framework you target (e.g., PyTorch Lightning).\n- If framework APIs have changed (e.g., different arguments passed to `on_epoch_end`), either adapt the callback or clearly scope the change to a specific version and document the constraints.",
            "Step 7: Document the new behavior and limitations.\n- Update docstrings to describe `interval`, including how it interacts with metrics and pruning decisions.\n- Mention performance implications and typical use cases (e.g., setting a larger interval to let models warm up before pruning)."
        ]
    }
}