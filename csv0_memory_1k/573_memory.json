{
    "search_index": {
        "description_for_embedding": "Added basic performance benchmarks to the napari project using airspeed velocity (asv): benchmark suites for image layers and Qt viewer operations, an asv.conf.json configuration, documentation on writing/running/profiling benchmarks, and gitignore rules to avoid committing benchmark result artifacts.",
        "keywords": [
            "napari",
            "benchmarking",
            "airspeed velocity",
            "asv",
            "asv.conf.json",
            "performance regression",
            "Qt viewer",
            "Image layer",
            "profiling",
            "snakeviz",
            ".asv",
            "benchmark_image_layer",
            "benchmark_qt_viewer",
            "benchmark_qt_viewer_image"
        ]
    },
    "agent_memory": {
        "episodic_memory": "In this change set, the napari project introduced a structured performance benchmarking framework using airspeed velocity (asv). A new BENCHMARKS.md file explains how contributors should write benchmarks, how to run them locally (asv dev, asv run -E existing, asv continuous master), and how to use the built-in cProfile integration with optional visualization via snakeviz. The PR added an asv.conf.json file configuring the project name, repo, environment type (virtualenv), Python versions (3.7), and result/cache directories (.asv/env, .asv/results, .asv/html).\n\nSeveral benchmark suites were implemented under the new benchmarks/ package:\n- benchmark_image_layer.py defines Image2DSuite and Image3DSuite classes that benchmark Image layer operations on 2D and 3D random data of varying sizes. They measure timings for layer creation, setting the view slice (_set_view_slice), updating thumbnails (_update_thumbnail), and getting the current value (get_value). They also include mem_layer and mem_data benchmarks using asv's memory benchmark type.\n- benchmark_qt_viewer.py defines QtViewerSuite with a benchmark for creating a napari.Viewer, properly ensuring a QApplication exists in setup.\n- benchmark_qt_viewer_image.py defines three suites: QtViewerViewImageSuite (measures napari.view_image on varying sizes), QtViewerAddImageSuite (measures viewer.add_image), and QtViewerEditImageSuite (measures editing operations such as _set_view_slice, _update_thumbnail, and get_value on an image layer in a viewer). All these create a QApplication if needed and close the viewer in teardown to avoid leaking GUI resources between runs.\n\nInitially, the author accidentally committed machine-specific asv results under .asv/results (benchmarks.json, machine.json, and commit-specific result JSON). A follow-up patch removed these files and updated .gitignore to ignore the entire .asv/ directory, ensuring benchmark artifacts are not tracked in version control. The documentation was also corrected: the example benchmark was changed from a scikit-image transform-based example (TransformSuite with hough_line) to a napari-specific ViewImageSuite that uses QApplication and napari.view_image, and a typo in 'airpseed' was fixed to 'airspeed'. Unused imports (QTimer) were removed from the Qt benchmarks.\n\nAfter these changes, the maintainers ran asv show and shared sample results for the new benchmarks, confirming that timings for view_image > add_image > create_layer made sense, that thumbnail updates are fast, and that the framework works with asv profiling tools. The PR was merged as the initial benchmarking infrastructure, with plans to add more benchmarks for additional layer types and complete refresh/update paths.",
        "semantic_memory": "This change encapsulates how to introduce and maintain a performance benchmarking and profiling framework in a Python GUI/scientific application project using airspeed velocity (asv):\n\n- Use asv for regression testing: Configure asv via asv.conf.json with project metadata, environment creation strategy, Python versions, and storage paths. This enables consistent, repeatable performance measurement across commits and environments.\n- Organize benchmark code as small, focused classes: Each benchmark suite is a class in a benchmarks/ package, with a setup method (and optional teardown) and one or more methods prefixed with time_ (for timing) or mem_ (for memory). setup prepares all data and objects so the timed method measures only the operation of interest.\n- Benchmark core operations at multiple levels: For a visualization library, this means both lower-level layer operations (creating layers, updating thumbnails, changing view slices, reading values) and higher-level GUI operations (creating viewers, viewing images, adding images), all under realistic Qt application contexts.\n- Properly handle GUI dependencies in benchmarks: When benchmarking Qt-based components, always ensure a QApplication exists in setup, and diligently close windows or viewers in teardown to avoid resource leaks or side effects between benchmark runs.\n- Do not commit benchmark artifacts: Benchmark result data (e.g., .asv/results, machine config files) are environment- and machine-specific. They should be ignored via .gitignore to keep the repository clean and portable.\n- Provide contributor-friendly documentation: Explain how to define new benchmarks, how to sanity-check them (asv dev), how to run them in the current environment (asv run -E existing), how to compare changes against master (asv continuous master), and how to leverage profiling tools like cProfile and visualization tools like snakeviz.\n- Use asv’s parametrization to explore scaling: Parameterized benchmarks (e.g., powers-of-two image sizes) expose how performance and memory usage scale with problem size, which is important for interactive visualization and real-time constraints (e.g., aiming for < 16–33 ms update times).\n\nThese ideas generalize to any project that needs to track performance regressions and maintain interactive responsiveness, especially where GUI frameworks and large numerical arrays are involved.",
        "procedural_memory": [
            "How to add and manage asv benchmarks and profiling in a Python/Qt project:",
            "Step 1: Add asv to your development environment.",
            "  - Activate your dev environment (venv or conda).",
            "  - Install airspeed velocity: `pip install asv` or `conda install asv`.",
            "  - Optionally run `asv machine` to record machine characteristics for result normalization.",
            "Step 2: Create an asv.conf.json configuration.",
            "  - In the project root, add an asv.conf.json with at least:",
            "    - `project` and `project_url` describing your project.",
            "    - `repo` pointing to the source (often `.` for the current repo).",
            "    - `branches` listing the branch(es) to benchmark (e.g. [\"master\"]).",
            "    - `environment_type` (e.g. \"virtualenv\" or \"conda\").",
            "    - `pythons` specifying the Python versions to test (e.g. [\"3.7\"]).",
            "    - `env_dir`, `results_dir`, and `html_dir` under something like `.asv/`.",
            "    - `build_cache_size` to limit cached builds.",
            "Step 3: Ensure benchmark artifacts are not version-controlled.",
            "  - Add `.asv/` to .gitignore so that asv environment, result, and HTML files are not committed.",
            "  - If you previously committed `.asv` content, delete those files from the repo and commit the removal.",
            "Step 4: Structure benchmark suites.",
            "  - Create a Python package `benchmarks/` (with __init__.py).",
            "  - For each benchmark area, add a file like `benchmark_image_layer.py` or `benchmark_qt_viewer.py`.",
            "  - Within each file, define one or more classes representing benchmark suites.",
            "  - Each suite should have:",
            "    - A `setup(self, ...)` method that creates any data structures or objects used in timed methods.",
            "    - Optional `teardown(self, ...)` to release GUI resources or large allocations.",
            "    - Methods named `time_<something>(...)` for timing benchmarks.",
            "    - Methods named `mem_<something>(...)` for memory benchmarks if needed.",
            "  - Use `params` and param names to create parameterized benchmarks (e.g., image sizes `[2**i for i in range(4, 13)]`).",
            "Step 5: Handle Qt/GUI correctly in benchmarks.",
            "  - In `setup`, ensure a QApplication exists:\n    `app = QApplication.instance() or QApplication([])`.",
            "  - For viewer-based benchmarks, create/close viewers inside the suite:\n    - For create-viewer benchmarks: `self.viewer = napari.Viewer()` in `time_create_viewer`.\n    - For viewing/add-image benchmarks: `self.viewer = napari.view_image(self.data)` or `self.viewer.add_image(self.data)`.",
            "  - In `teardown`, close the viewer window: `self.viewer.window.close()` to avoid resource leakage between runs.",
            "Step 6: Write concrete benchmark methods.",
            "  - For array/layer operations:",
            "    - In `setup`, create deterministic random data: `np.random.seed(0); self.data = np.random.random((n, n))`.",
            "    - Create the layer instance: `self.layer = Image(self.data)`.",
            "    - In `time_create_layer`, construct a new layer from data: `Image(self.data)`.",
            "    - In `time_set_view_slice`, call internal methods like `self.layer._set_view_slice()`.",
            "    - In `time_update_thumbnail`, call `self.layer._update_thumbnail()`.",
            "    - In `time_get_value`, call `self.layer.get_value()`.",
            "    - For mem benchmarks, return the object whose memory you want measured (e.g., `return self.data` or `return Image(self.data)`).",
            "  - For higher-level viewer benchmarks:",
            "    - `QtViewerViewImageSuite.time_view_image`: time `napari.view_image(self.data)`.",
            "    - `QtViewerAddImageSuite.time_add_image`: time `self.viewer.add_image(self.data)` on an existing viewer.",
            "    - `QtViewerEditImageSuite` methods should operate on `self.viewer.layers[0]` for common operations.",
            "Step 7: Sanity-check benchmarks with asv.",
            "  - Use `asv dev -b <BenchmarkClassOrRegex>` to run benchmarks once in your current environment, primarily to catch import or logic errors.",
            "  - Example: `asv dev -b benchmark_image_layer.Image2DSuite`.",
            "Step 8: Run benchmarks in the current environment.",
            "  - Use `asv run -E existing -b <BenchmarkPattern>` to run benchmarks against your current environment without rebuilding.",
            "  - Example: `asv run -E existing -b benchmark_qt_viewer_image.QtViewerViewImageSuite`.",
            "Step 9: Compare performance with master (regression detection).",
            "  - Use `asv continuous master -b <BenchmarkPattern>` to compare your current branch/commit against master:",
            "    - `asv continuous master -b ViewImageSuite`.",
            "  - Inspect the output to see whether your changes significantly improved or regressed performance.",
            "Step 10: Use profiling to understand performance hotspots.",
            "  - Use `asv profile` together with cProfile and an external viewer like snakeviz:",
            "    - Install snakeviz: `pip install snakeviz`.",
            "    - Profile a specific benchmark: `asv profile benchmark_qt_viewer.QtViewerSuite.time_create_viewer -g snakeviz`.",
            "    - For parameterized benchmarks, use a regex to select variants:\n      `asv profile \"benchmark_image_layer.Image2DSuite.time_create_layer(.*)\" -g snakeviz`.",
            "  - Analyze the snakeviz output to identify bottlenecks and guide optimization.",
            "Step 11: Maintain and extend the benchmark suite.",
            "  - When adding new performance-sensitive features, include or update benchmarks that capture their typical workloads.",
            "  - Ensure that benchmarks are stable (low variance) and represent realistic usage.",
            "  - Keep documentation (like BENCHMARKS.md) up to date with new patterns and best practices.",
            "Step 12: Avoid common pitfalls.",
            "  - Do not include one-off environment or machine result files in the repository; always rely on .gitignore for .asv/.",
            "  - Ensure setup includes all necessary context (e.g., GUI apps) so timed functions focus on the operation under test.",
            "  - Clean up GUI resources in teardown to ensure benchmarks remain reliable and don't interfere with each other."
        ]
    }
}