{
    "search_index": {
        "description_for_embedding": "Added a new Mean Decrease Impurity (MDI) hyperparameter importance evaluator in Optuna that uses sklearn’s RandomForestRegressor and OneHotEncoder. Fixed a bug in how one-hot encoded categorical feature columns were mapped back to original hyperparameters when aggregating feature_importances_, which previously caused incorrect importance values for categorical parameters.",
        "keywords": [
            "Optuna",
            "hyperparameter importance",
            "MeanDecreaseImpurityImportanceEvaluator",
            "MDI",
            "RandomForestRegressor",
            "OneHotEncoder",
            "ColumnTransformer",
            "categorical encoding",
            "feature_importances_",
            "column mapping bug",
            "aggregation of feature importances",
            "sklearn dependency"
        ]
    },
    "agent_memory": {
        "episodic_memory": "This change introduced a new MeanDecreaseImpurityImportanceEvaluator to Optuna’s hyperparameter importance module and made it the default evaluator. The evaluator fits a RandomForestRegressor on trial parameter configurations and objective values, then uses sklearn’s feature_importances_ (MDI) as a proxy for hyperparameter importance.\n\nThe core implementation detail is that trial parameters are assembled into a matrix, categorical distributions are encoded via sklearn’s OneHotEncoder inside a ColumnTransformer, then feature_importances_ are aggregated back from one-hot columns to the original parameter level. To do this, the code builds a cols_to_raw_cols array mapping each transformed-feature column index back to the original parameter index, and then uses numpy.add.at to sum importances for all columns belonging to the same parameter.\n\nA subtle bug existed in the first implementation of this mapping. The code collected:\n- categorical_cols: indices of categorical parameters in the original matrix,\n- categories: a list of category indices for each categorical parameter.\n\nAfter transformation, it constructed cols_to_raw_cols like this:\n\n    i = 0\n    for categorical_col in categorical_cols:\n        for _ in range(len(categories[i])):\n            cols_to_raw_cols[i] = categorical_col\n            i += 1\n\nThe problem was that i here is the running index over the transformed columns, but it was also incorrectly used as an index into the categories list. Since i increments by the number of one-hot columns per categorical parameter, it quickly diverges from the logical index into categories (which should be 0, 1, 2, ... over categorical parameters). This leads to using the wrong categories length when multiple categorical parameters are present, which in turn produces an incorrect cols_to_raw_cols mapping, and therefore wrong aggregated importances per hyperparameter.\n\nThe fix was to align the loops over categorical columns and their category sets explicitly:\n\n- When building categories, store Python lists instead of numpy ranges:\n\n    categories.append(list(range(len(distribution.choices))))\n\n- When building the mapping, iterate over categorical_cols and categories together:\n\n    i = 0\n    for categorical_col, cats in zip(categorical_cols, categories):\n        for _ in range(len(cats)):\n            cols_to_raw_cols[i] = categorical_col\n            i += 1\n\nThis guarantees that the number of one-hot columns assigned to each categorical parameter matches its number of categories, and that all transformed categorical columns are correctly mapped back to their originating parameter index.\n\nAdditional changes:\n- The default importance evaluator in get_param_importances was switched from FanovaImportanceEvaluator to MeanDecreaseImpurityImportanceEvaluator.\n- Tests were updated to parametrize across both Fanova and MDI evaluators, ensuring consistent behavior, correct normalization (sum of importances ~ 1.0 with tolerance), and that parameters are returned in non-increasing importance order.\n- Documentation was added for the new evaluator, including notes on the sklearn dependency and a reference to RandomForestClassifier.feature_importances_.\n- A minor simplification of memory allocation for the importance array was done (using numpy.zeros(len(distributions)) instead of specifying dtype explicitly).",
        "semantic_memory": "Generalizable lessons from this change:\n\n1. **Mapping back from transformed features to original variables is error-prone.**\n   When using feature transformations such as one-hot encoding, column transformers, or any pipeline that expands or reorders features, you must carefully track how transformed columns relate to original features. A small indexing mistake in the mapping (e.g., mixing up indices over transformed columns vs. indices over feature groups) can silently produce wrong results.\n\n2. **Use aligned iteration for parallel data structures.**\n   If you maintain multiple lists that conceptually line up element-wise (e.g., `categorical_cols` and `categories`), iterate over them with `zip` or equivalent, rather than relying on shared mutable indices that are used for multiple purposes. This avoids bugs where one index variable tries to serve both as a position in one list and a counter in another domain (like total flattened columns).\n\n3. **Aggregate feature importances carefully after encoding.**\n   When using ensemble feature_importances_ (MDI, Gini gain, etc.) on data that has undergone one-hot encoding, the per-feature importance must be aggregated (summed) per original variable. Doing so requires a correct mapping from each one-hot column back to the original feature. Mis-aggregation can lead to misleading interpretations of model behavior.\n\n4. **Test using multiple categorical parameters and varying category counts.**\n   Bugs in encoding and mapping logic often only appear when there are multiple categorical features, especially with different numbers of categories. Tests should include such cases to validate mapping logic and the consistency of results.\n\n5. **Validate both types and invariants of outputs.**\n   Tests here not only assert types (e.g., str for names, float for importances) but also invariants like:\n   - the presence of all expected parameters,\n   - ordering of importance values (non-increasing),\n   - sum of normalized importances approximately equal to 1.0.\n   These constraints catch logical bugs even when the code runs without raising errors.\n\n6. **Guard optional dependencies explicitly.**\n   For optional features depending on third-party libraries (like scikit-learn), encapsulate import checks and raise a clear error with installation advice when the dependency is missing. This avoids confusing ImportError traces and improves user experience.\n\n7. **Keep default algorithms simple and with minimal dependencies.**\n   Switching the default importance evaluator to an MDI-based approach using RandomForestRegressor, rather than fANOVA, reflects a general principle: default options should be easy to understand and rely on broadly available, lightweight dependencies when possible.",
        "procedural_memory": [
            "Step-by-step instructions on how to diagnose and fix similar issues.",
            "Step 1: Identify suspicious downstream behavior.\nIf a model-based feature-importance tool is giving odd results (e.g., some categorical hyperparameters appear with zero importance despite being clearly influential, or importance rankings change unexpectedly when adding another categorical parameter), suspect a bug in how transformed features are mapped back to original variables.",
            "Step 2: Inspect the feature transformation pipeline.\nExamine how input features (e.g., hyperparameter values) are transformed before being passed to the model. For categorical features, look for OneHotEncoder, ColumnTransformer, or custom preprocessing that changes the number and order of columns.",
            "Step 3: Trace the mapping logic from transformed columns to original features.\nLocate any code that attempts to reverse-map transformed feature indices back to original parameters (e.g., `cols_to_raw_cols` or similar mapping arrays). Pay close attention to loops and index variables. Look for cases where one index is used for two different logical sequences (e.g., both flattened columns and feature groups).",
            "Step 4: Confirm the size and alignment of mapping structures.\nAdd assertions or logging:\n- that the mapping array has the same length as the number of transformed columns,\n- that each transformed column maps to a valid original index,\n- that the sum of counts per original feature matches the total columns.\nFor one-hot encodings, each categorical feature should contribute exactly N columns, where N is the number of categories.",
            "Step 5: Use aligned iteration (`zip`) for parallel lists.\nIf you have separate lists like `categorical_cols` and `categories`, change your mapping loop to:\n\n    for categorical_col, cats in zip(categorical_cols, categories):\n        for _ in range(len(cats)):\n            cols_to_raw_cols[i] = categorical_col\n            i += 1\n\ninstead of relying on a shared index variable that increments based on flattened column counts. This guarantees correct pairing between each categorical feature and its category set.",
            "Step 6: Re-run unit tests with multiple categorical features.\nCreate or extend tests to include:\n- multiple categorical parameters,\n- different numbers of categories per parameter,\n- some numerical and some categorical parameters mixed.\nValidate that aggregated importances:\n- cover all params,\n- are non-negative,\n- are ordered as expected,\n- and sum to approximately 1.0 when normalized.",
            "Step 7: Add invariants to tests for robustness.\nIn tests of importance evaluators:\n- Assert that the output is an OrderedDict or equivalent ordered mapping.\n- Assert that the keys match the expected parameter names.\n- Assert that importances are floats and monotone non-increasing in the iteration order.\n- Assert that `math.isclose(1.0, sum(importances), abs_tol=1e-5)` holds when the evaluator is supposed to normalize them.",
            "Step 8: Guard optional dependencies clearly.\nIf your evaluator or feature importance logic depends on an optional library (e.g., sklearn), wrap imports in a try/except, set a module-level flag, and provide a `_check_<lib>_availability()` function that raises a descriptive ImportError including installation instructions. Call this in the evaluator’s constructor so users get immediate feedback.",
            "Step 9: Document and expose the new functionality.\nUpdate public API modules (__init__.py) to export the new evaluator, adjust defaults if desired (e.g., make the new MDI evaluator the default), and add reference documentation explaining dependencies, algorithm behavior, and links to underlying library docs (like sklearn’s feature_importances_).",
            "Step 10: Benchmark and compare evaluators.\nFor confidence, run benchmarks comparing the new evaluator (e.g., MDI-based) against existing ones (e.g., fANOVA) on representative examples. Plot or inspect importance rankings to ensure the new evaluator behaves sensibly and consistently."
        ]
    }
}