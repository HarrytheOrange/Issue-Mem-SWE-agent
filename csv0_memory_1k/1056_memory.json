{
    "search_index": {
        "description_for_embedding": "Implemented Optuna–Catalyst integration via a CatalystPruningCallback, added tests and an example, and fixed CI issues around optional Catalyst dependency and Python version incompatibilities by using lazy imports, pytest.importorskip, and per-version test/example ignore lists.",
        "keywords": [
            "optuna",
            "catalyst",
            "pruning",
            "Callback",
            "CatalystPruningCallback",
            "optional dependency",
            "lazy import",
            "try_import",
            "pytest.importorskip",
            "python 3.5",
            "integration tests",
            "examples.yml",
            "CircleCI",
            "extras_require",
            "MNIST example"
        ]
    },
    "agent_memory": {
        "episodic_memory": "This PR added first-class integration between Optuna and the Catalyst deep learning framework by introducing a CatalystPruningCallback that prunes unpromising Optuna trials based on Catalyst validation metrics. Initially, the PR only contained a stub of the callback. The author then implemented the callback to hook into Catalyst's training loop via on_epoch_end, read a specified metric from state.valid_metrics, report it to the Optuna trial, and raise optuna.TrialPruned when the trial should be pruned. To make this integration robust, Catalyst was treated as an optional dependency. The code switched from manual try/except ImportError flags to Optuna's try_import helper, wrapping the Catalyst import and calling _imports.check() in the callback __init__. When Catalyst is unavailable, Callback falls back to object to keep imports safe while still raising a clear error at use-time.\n\nThe PR added an example script examples/catalyst_simple.py that trains an MLP on MNIST using Catalyst's SupervisedRunner, AccuracyCallback, and Optuna's CatalystPruningCallback. It demonstrates optimizing the network structure, using accuracy01 as the pruning metric, and optionally enabling pruning via a --pruning flag. The example uses torchvision.datasets.MNIST via torchvision.datasets (rather than direct MNIST import) and shuffles the training data.\n\nThe PR also introduced tests in tests/integration_tests/test_catalyst.py that construct a toy dataset, run a short Catalyst training loop inside a temporary directory, and verify that DeterministicPruner(True) causes the trial to end in PRUNED and DeterministicPruner(False) leads to COMPLETE with the expected value. To handle environments without Catalyst and versions where Catalyst is unsupported (Python < 3.6), the tests use pytest.importorskip(\"catalyst\") and @pytest.mark.skipif(sys.version_info < (3, 6), ...).\n\nSeveral CI issues arose due to Python 3.5 and example/test execution. Catalyst does not support Python 3.5, so the PR adjusted setup.py extras_require to only include catalyst when sys.version_info[:2] > (3,5) and updated the testing extras accordingly. It also modified .circleci/config.yml to ignore tests/integration_tests/test_catalyst.py for the Python 3.5 job, and updated .github/workflows/examples.yml to skip running catalyst_.* examples for Python 3.5. The test was updated to use a temporary directory (tempfile.mkdtemp and shutil.rmtree) instead of writing to a fixed logdir path, preventing pollution of the workspace. Formatting issues (black/flake8) in the new files were iteratively fixed. In the end, the Catalyst integration worked, tests passed (or were properly skipped), and the example was excluded where Catalyst is unavailable, resolving CI failures and completing the integration feature.",
        "semantic_memory": "This change illustrates several general best practices for integrating optional framework-specific functionality into a core library while maintaining robust CI and cross-version support.\n\n1) Optional dependency integration pattern: When integrating with an external framework (e.g., Catalyst, PyTorch Lightning, fastai), the integration code must not hard-fail on import in environments where that framework is not installed. A recommended pattern is to use a lazy import helper (like Optuna's try_import) to attempt to import the external package, store the import state, and only enforce its presence in the code paths that actually require it (e.g., in the callback's constructor). If import fails, the integration class can still be defined but backed by a dummy base type (e.g., Callback = object) to keep type checking and imports working, while raising a meaningful error when users attempt to instantiate it without the dependency.\n\n2) Version-specific optional dependencies: Some integrations depend on minimum Python versions (e.g., Catalyst requires Python 3.6+). extras_require and test requirements should conditionally include such packages based on sys.version_info to avoid installation failures or incompatible combinations on older interpreters. Tests and examples that rely on those packages should also be skipped or ignored in jobs targeting unsupported versions.\n\n3) Test robustness with external frameworks: Integration tests that involve third-party frameworks and file-system side effects should minimize environmental dependencies. Using pytest.importorskip(\"package_name\") cleanly skips tests in environments missing the optional dependency, avoiding brittle import errors. When frameworks write logs or checkpoints to disk, tests should create and clean up temporary directories with tempfile and shutil instead of using fixed paths. This keeps CI environments clean and avoids interference between test runs.\n\n4) Built-in pruning hooks in training loops: To support early stopping based on hyperparameter optimization, callbacks can hook into the training loop at epoch boundaries, gather a validation metric from the framework's state (e.g., state.valid_metrics[metric]), pass it to an HPO trial object via trial.report, and then call trial.should_prune to decide whether to raise a pruning exception. The metric name should be configurable so users can select any metric that the framework computes.\n\n5) CI integration and example handling: Continuous integration setups need to be aware of optional integrations. For jobs that cannot satisfy all dependencies (e.g., oldest Python versions), tests and example scripts that require those dependencies should be explicitly ignored. This can be done via test ignore lists in CI commands (pytest --ignore ...) and pattern-based ignores for examples in workflows. This approach keeps the main test matrix green while still exercising integrations in compatible environments.\n\n6) Consistent style and imports: When adding integrations and examples, it's important to maintain consistent import ordering (stdlib, third-party, then local), avoid unused type_checking branches when not needed, and satisfy code formatters (black) and linters (flake8) to keep the codebase coherent. This often requires minor refactors like using torchvision.datasets instead of from torchvision.datasets import MNIST for style consistency.",
        "procedural_memory": [
            "Step-by-step instructions on how to diagnose and fix similar issues involving optional integration with external ML frameworks and CI failures.",
            "Step 1: Identify the integration points and the external dependency.\nDetermine where in your code you want to integrate with the external framework (e.g., callback classes, runner wrappers). Confirm the minimum version requirements for both Python and the external library (e.g., Catalyst requires Python >= 3.6).",
            "Step 2: Implement the core integration logic with a clean API.\nDesign the integration object (e.g., CatalystPruningCallback) with clear parameters (trial, metric name, etc.). Use the framework's callback or hook system (on_epoch_end, on_batch_end, etc.) to access relevant state/metrics. Report the metric to the optimization trial and use trial.should_prune to decide whether to stop the trial. Raise the library's pruning exception (e.g., optuna.TrialPruned) when a trial should be pruned.",
            "Step 3: Use lazy and safe imports for the external dependency.\nWrap imports of the external framework with a helper like try_import or a manual try/except ImportError block. For example, use `with try_import() as _imports: from catalyst.dl import Callback`. After that, define a fallback class, e.g., `if not _imports.is_successful(): Callback = object`, so the module imports cleanly even without the dependency. In the integration class __init__, call `_imports.check()` before invoking any external API or super().__init__ that depends on the framework, so users get a clear error only when they try to use the integration without having installed the dependency.",
            "Step 4: Handle Python-version-specific constraints.\nIf the external library only supports certain Python versions, adjust extras_require in setup.py (or equivalent) so that the dependency is only added when sys.version_info satisfies the requirement. For example, `+ (['catalyst'] if (3, 5) < sys.version_info[:2] else [])`. This avoids pip trying to install unsupported packages on older Python versions.",
            "Step 5: Design robust integration tests that tolerate missing dependencies.\nWrite tests that perform a minimal real run of the framework plus integration. Use pytest.importorskip('external_pkg') at the top of the test module to gracefully skip tests when the package isn't installed. If the framework writes logs or artifacts, create a temporary directory with tempfile.mkdtemp for logdir or output paths, and delete it afterward with shutil.rmtree to keep the environment clean.",
            "Step 6: Add version-aware test skipping.\nFor Python versions that the external framework doesn’t support, add an appropriate skip marker to the tests, e.g., `@pytest.mark.skipif(sys.version_info < (3, 6), reason='external_pkg requires Python 3.6 or higher')`. Combine this with pytest.importorskip so that both missing dependency and unsupported Python are handled gracefully.",
            "Step 7: Update CI test configuration to ignore incompatible tests/examples.\nInspect your CI job matrix (e.g., CircleCI config.yml, GitHub Actions workflows). For jobs targeting older Python versions, add pytest --ignore entries for integration tests that require unsupported libraries. Similarly, in workflows that run examples, update ignore patterns (e.g., IGNORES='...|catalyst_.*') so examples that require the external framework are not run on incompatible environments.",
            "Step 8: Provide a runnable example for users.\nCreate a minimal example script that demonstrates how to use the new integration in a real training scenario. Use commonly available datasets (e.g., torchvision.datasets.MNIST) and show how to configure the callback, optimizer, criterion, and runner. Include a CLI flag to enable/disable pruning and ensure the example is consistent with style guidelines (black, flake8).",
            "Step 9: Verify formatting and style.\nRun black and flake8 (or your project's preferred tools) on the modified files to ensure code style compliance. Fix import ordering, spacing, and unused imports/type hints as necessary. This prevents style-related CI failures.",
            "Step 10: Re-run tests locally and monitor CI.\nRun the test suite locally, focusing on the new integration tests and examples. Push the changes and watch all CI jobs, confirming that: (a) tests/examples are executed and pass for supported environments, and (b) tests/examples are skipped or ignored for environments where the external dependency or Python version is unsupported. Iterate if any job still fails due to missing dependencies or version issues."
        ]
    }
}