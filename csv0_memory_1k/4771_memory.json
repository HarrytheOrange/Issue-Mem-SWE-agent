{
    "search_index": {
        "description_for_embedding": "Fixed a map-chat MQTT handling bug where non-chat MQTT messages were parsed as catchable Pokémon data without verifying the topic, and simplified the MQTT topics by removing a secondary pgomapcatch channel and reusing the pgo/all/catchable JSON feed while stripping sensitive encounter_id fields before publishing.",
        "keywords": [
            "MQTT",
            "topic filter",
            "payload parsing",
            "map-chat",
            "pgo/all/catchable",
            "pgomapcatch",
            "JSON vs CSV payload",
            "websocket",
            "social_handler.py",
            "encounter_id removal",
            "privacy",
            "channel consolidation",
            "JavaScript MQTT client",
            "Python event handler",
            "bugfix"
        ]
    },
    "agent_memory": {
        "episodic_memory": "In this incident, the web-based map-chat client was subscribed broadly to the MQTT broker and attempted to parse any non-chat message as a catchable Pokémon payload, without first checking that the message came from the correct topic. The client subscribed to \"pgomapcatch/#\" and \"pgochat/chat\". In the message handler, it only checked for chat messages (topic starting with \"pgochat\") and then fell through into a branch that unconditionally treated all other messages as comma-separated catchable Pokémon data. This could cause parsing errors or incorrect behavior when other non-chat topics were introduced or reused.\n\nThe pull request implemented two main changes:\n\n1. **Topic guard for catchable parsing (initial fix):**\n   - In `map-chat/javascript/main.js`, the message handler was updated so that the catchable parsing logic only runs when the topic matches the specific catchable pattern:\n     - Previously: `else { /* parse as catchable */ }`\n     - After fix: `else if (/^pgomapcatch\\/all\\/catchable/i.test(topic)) { /* parse as catchable */ } else { console.debug(topic); }`\n   - This ensured that only messages on the intended catchable channel were parsed as Pokémon data.\n\n2. **Channel consolidation and JSON usage:**\n   - The legacy `pgomapcatch` topic was removed and the system was updated to use the `pgo/all/catchable` JSON feed for the map as well.\n   - On the **web client**:\n     - Subscription changed from `client.subscribe(\"pgomapcatch/#\")` to `client.subscribe(\"pgo/#\")`.\n     - The catchable handler was updated from parsing a CSV string payload off `pgomapcatch/all/catchable` to parsing JSON from `pgo/all/catchable`:\n       - Old: split `payload.toString()` by commas into latitude, longitude, encounter_id, pokemon_id, expiration, pokemon_name, build an icon path, and call `displayMessageOnMap(...)` with those values.\n       - New: `pokemon = JSON.parse(payload)` and use `pokemon.latitude`, `pokemon.longitude`, `pokemon.pokemon_id`, `pokemon.expiration_timestamp_ms`, `pokemon.pokemon_name` to compute the icon path and call `displayMessageOnMap(...)`.\n   - On the **Python bot side** (`pokemongo_bot/event_handlers/social_handler.py`):\n     - The publish call to `\"pgomapcatch/all/catchable/...\"` with CSV data was removed.\n     - The handler now only publishes JSON to `\"pgo/all/catchable/<pokemon_id>\"`.\n\n3. **Privacy/safety adjustment (encounter_id removal):**\n   - There was a concern about exposing `encounter_id` to the web client. Initially, both channels included `encounter_id` in the payload.\n   - The final patch explicitly removes `encounter_id` from the data before JSON serialization and publish:\n     - `data.pop('encounter_id', None)` is called before `json.dumps(data)` and publishing to `pgo/all/catchable/...`.\n   - This ensures that the public-facing MQTT topic no longer leaks the `encounter_id` field.\n\n4. **Temporary debugging and cleanup:**\n   - Some debug `print` statements were added to `mqtt_on_message` to log topics and whether a Pokémon entry was new or duplicate, then removed in a subsequent patch.\n\nThe net result: the map-chat client now correctly filters by MQTT topic before parsing, uses a single JSON-based topic (`pgo/all/catchable`) for catchable Pokémon instead of a CSV-based secondary channel, and the backend strips sensitive `encounter_id` data before publishing to public topics, addressing both correctness and privacy concerns.",
        "semantic_memory": "This case illustrates several generalizable lessons about message-based systems, web clients, and data privacy:\n\n1. **Always validate topics/channels before parsing payloads.**\n   In pub/sub systems like MQTT, it is unsafe to assume that any non-chat message has a particular structure. Handlers should explicitly check the topic or routing key (e.g., via regex or exact match) before applying format-specific parsing. Failing to do so can cause hard-to-diagnose runtime errors when new message types are added or existing topics change semantics.\n\n2. **Design topic hierarchies and subscriptions narrowly.**\n   Subscribing broadly (e.g., `\"pgomapcatch/#\"`) and then treating all non-chat messages as a specific type is brittle. It's better to:\n   - Use clear, semantically meaningful topics (`pgo/all/catchable`) for specific message types.\n   - Subscribe to only the namespaces you truly need.\n   - Use explicit topic matches or well-defined patterns in the client.\n\n3. **Prefer structured payloads (JSON) over ad-hoc CSV strings.**\n   The change from CSV-style strings to JSON payloads improved clarity, robustness, and extensibility. JSON makes it explicit what fields exist (`latitude`, `longitude`, `pokemon_id`, etc.) and is less prone to errors than relying on positional comma-separated values.\n\n4. **Avoid leaking sensitive identifiers in public channels.**\n   IDs like `encounter_id` can sometimes be sensitive (e.g., they may enable abuse, replay attacks, or data correlation). When publishing to public or semi-public channels (e.g., web clients), sensitive fields should be explicitly removed or masked before serialization. This should be enforced on the server side, not only by convention among consumers.\n\n5. **Separate internal and external data contracts.**\n   The bot may maintain a rich internal representation (including `encounter_id` and `spawn_point_id`), but the external/public API should expose only what is necessary. Explicitly sanitizing or transforming the data object before publish helps maintain this separation of concerns.\n\n6. **Centralize message schema evolution.**\n   When changing the schema of messages (e.g., switching from CSV to JSON, or removing a field like `encounter_id`), update all producers and consumers in tandem and ensure compatibility. Reusing a single well-defined JSON topic for multiple consumers simplifies support and reduces duplication.\n\n7. **Use logging strategically and remove debug noise.**\n   Adding temporary logs (topics, duplicate detection) is useful while iterating on message behavior, but they should be removed or downgraded to appropriate log levels before merging to prevent log spam and performance issues.",
        "procedural_memory": [
            "Step-by-step approach to diagnose and fix similar MQTT topic and payload handling issues:",
            "Step 1: Reproduce and inspect message flow\n- Run both the backend (publisher) and frontend (subscriber) in a test environment.\n- Enable logging of MQTT topics and payloads on both sides (temporarily log `msg.topic` and payload for incoming messages).\n- Reproduce the problematic behavior (e.g., exceptions, misrendered data, wrong message types being parsed).",
            "Step 2: Verify subscription patterns\n- Check what topics the client subscribes to (e.g., `client.subscribe(\"pgomapcatch/#\")`).\n- Ensure the subscription is not overly broad; identify exactly which topics are intended for each message type (chat, catchable entities, etc.).",
            "Step 3: Audit message handling conditions\n- Inspect the client’s `on('message', ...)` handler (or equivalent) and list each conditional branch.\n- Look for generic fall-through branches like `else { parse as X }` that treat all non-matching messages as a particular format.\n- Confirm that each branch checks the topic (or routing key) before assuming a specific payload structure.\n- If topic checks are missing, add explicit guards (e.g., `else if (/^pgo\\/all\\/catchable/i.test(topic)) { ... }`) for each message type.",
            "Step 4: Validate payload formats per topic\n- For each topic, confirm the agreed payload format (JSON, CSV, protobuf, etc.).\n- Compare the producer’s serialization code with the consumer’s parsing logic.\n- If the producer sends JSON but the client parses CSV (or vice versa), refactor to use a single consistent format, preferably JSON for web clients.\n- Update the consumer to parse fields by name rather than by fixed position where possible.",
            "Step 5: Consolidate or rename topics if needed\n- If multiple topics carry essentially the same data (e.g., `pgomapcatch/all/catchable` with CSV and `pgo/all/catchable` with JSON), consider consolidating to one canonical topic.\n- Update producers to publish only to the canonical topic.\n- Update all consumers to subscribe to and handle that topic appropriately.",
            "Step 6: Review and sanitize sensitive fields\n- Enumerate all fields in the published message (e.g., `encounter_id`, `spawn_point_id`, coordinates, etc.).\n- Identify fields that should not be exposed to public/web clients for security or privacy reasons.\n- Before serializing and publishing, explicitly remove or mask these fields (e.g., `data.pop('encounter_id', None)` in Python) in the backend code.\n- Verify that the frontend does not rely on removed fields; if it does, refactor the UI or use non-sensitive identifiers instead.",
            "Step 7: Add minimal logging and clean up debug code\n- During development, keep helpful logs that identify new vs duplicate messages or unexpected topics.\n- Once the behavior is stable, remove or reduce debug logs to appropriate log levels to avoid clutter.\n- Ensure logging does not include sensitive payloads on production systems if that violates privacy requirements.",
            "Step 8: Test end-to-end\n- With the new topic filters and payload schemas in place, test the full flow:\n  - Backend publishes catchable entity messages to the canonical topic.\n  - Web client receives only relevant messages and parses them without errors.\n  - UI correctly displays expected entities (e.g., Pokémon) on the map based on JSON fields.\n  - No sensitive identifiers are present in the payloads observed in the browser dev tools.",
            "Step 9: Document the message contracts\n- Document each MQTT topic’s purpose, payload schema, and any fields intentionally omitted for privacy.\n- Share this with team members to prevent reintroducing multiple overlapping channels or leaking sensitive data in future changes."
        ]
    }
}