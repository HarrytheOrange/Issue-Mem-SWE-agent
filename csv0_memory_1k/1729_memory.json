{
    "search_index": {
        "description_for_embedding": "Introduced an experimental batch evaluation API in Optuna via the new optuna.batch module. Added BatchStudy/BatchMultiObjectiveStudy and BatchTrial/BatchMultiObjectiveTrial to evaluate multiple trials at once while reusing the existing Study.optimize loop by monkeypatching _run_trial_and_callbacks. Disabled pruning for batch trials, ensured proper failure handling and callback invocation per underlying trial, and switched the API from n_batches to n_trials with an explicit batch_size argument.",
        "keywords": [
            "Optuna",
            "batch evaluation",
            "BatchStudy",
            "BatchTrial",
            "MultiObjective",
            "optuna.batch",
            "_run_trial_and_callbacks",
            "pruning disabled",
            "n_trials vs batch_size",
            "API design"
        ]
    },
    "agent_memory": {
        "episodic_memory": "This change set added an experimental batch evaluation API to Optuna, addressing the need to evaluate multiple candidate parameter sets in a single objective call without redesigning the core Study interface.\n\nOriginally, a separate BatchStudy class was proposed that inherited from Study, which would have required modifying Study and created confusion between `Study` and `BatchStudy` usage. In this PR, the author instead introduced a new `optuna.batch` namespace and kept the core `Study` class untouched. Two wrapper classes were introduced: `optuna.batch.study.BatchStudy` for single-objective optimization and `optuna.batch.multi_objective.study.BatchMultiObjectiveStudy` for multi-objective optimization. Corresponding batched trial types, `BatchTrial` and `BatchMultiObjectiveTrial`, were added under `optuna.batch.trial` and `optuna.batch.multi_objective.trial`.\n\nThe main challenge was integrating batched evaluation into the existing `Study.optimize` loop, which is designed for a scalar-returning objective function and a single `Trial` at a time. The final solution monkeypatches the internal method `_run_trial_and_callbacks` on the underlying `Study` object at optimize-time. Instead of letting Optuna call the user objective directly for each trial, the patched executor:\n\n1. Creates `batch_size` underlying `Trial` objects at once (one per logical trial).\n2. Wraps them in a `BatchTrial` (or `BatchMultiObjectiveTrial`) that exposes vectorized `suggest_*` APIs returning NumPy arrays.\n3. Executes the user-provided batched objective function once per batch, returning a NumPy array (single-objective) or sequence of arrays (multi-objective).\n4. Distributes the returned vector(s) back onto the underlying individual trials, validating each element, marking them COMPLETE/FAIL as appropriate, and logging completion.\n5. Still calls any registered callbacks once per underlying trial by reusing the standard callback mechanism; the `BatchStudy.optimize` and `BatchMultiObjectiveStudy.optimize` calls pass a dummy objective function (`lambda _: 0` or `lambda _: [0] * n_objectives`) because the real work is done inside the patched `_run_trial_and_callbacks`.\n\nThe interface was refined so that `batch_size` is specified at `optimize` time instead of during study creation. The previous `n_batches` parameter was removed in favor of the standard `n_trials` argument; the batched executor converts `n_trials` into the actual number of optimizer loop iterations by dividing by `batch_size`.\n\nThe pruning feature was explicitly disabled for batched trials. `BatchTrial.should_prune()` always returns `False` and emits a warning because pruning semantics are not intuitive when multiple models are evaluated together in one batch, and the authors wanted to avoid surprising behavior. Pruner arguments were also removed from `optuna.batch.create_study` and `optuna.batch.load_study`, forcing `pruner=None` under the hood.\n\nThe wrappers delegate almost all attributes and methods to the underlying `Study`/`MultiObjectiveStudy` via `__getattr__`, only adding the `optimize` overload and, in the multi-objective case, handling the multi-dimensional value reporting via `_report_complete_values`. `BatchTrial` and `BaseBatchTrial` forward `suggest_*`, user/system attrs, datetime, params, and distributions across the underlying trials, returning arrays or lists where appropriate.\n\nExtensive tests were added:\n- `tests/batch_tests/test_study.py` and `test_trial.py` cover basic optimization, resuming, parallelism, callbacks, enqueued trials, and attribute propagation for single-objective batch optimization.\n- `tests/batch_tests/multi_objective_tests/test_study.py` and `test_trial.py` ensure multi-objective optimization works with batches, including Pareto front calculation and intermediate value reporting.\n\nExamples were updated so that users call:\n\n```python\nstudy = optuna.batch.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=40, batch_size=4)\n```\n\nand similarly for multi-objective with `optuna.batch.multi_objective.create_study([...])`. Overall, this PR introduced a clean, experimental `optuna.batch` API for batched evaluation, avoided modifying the core Study interface, and carefully integrated with existing logging, callbacks, and storage semantics.",
        "semantic_memory": "Generalizable lessons from this change revolve around designing batch APIs around existing sequential infrastructure, balancing API clarity with internal hackery, and carefully handling trial lifecycle semantics when moving from scalar to vectorized objectives.\n\nKey concepts and patterns:\n\n1. **Wrapper-based batch API instead of subclassing core types**\n   - Rather than subclassing `Study` and modifying its internals for `BatchStudy`, the code introduces `optuna.batch.study.BatchStudy` that holds a reference to a regular `Study` and delegates nearly everything through `__getattr__`.\n   - This avoids invasive changes to a widely used core class, keeps the public API surface clearly separated (`optuna.batch` namespace), and allows experimentation via the `_experimental` decorator.\n   - Pattern: prefer composition+delegation for experimental or alternate behavior instead of deep inheritance from stable core types.\n\n2. **Batching by monkeypatching the execution runner, not the control loop**\n   - Instead of rewriting `Study.optimize`, the code overrides the internal `_run_trial_and_callbacks` method to run a batch of trials in one shot. The external `Study.optimize` signature and control flow stay intact.\n   - A dummy objective is passed to `Study.optimize`, and the real batch objective is injected inside `_run_trial_and_callbacks`. This allows reuse of timeout handling, logging, and joblib-based parallelism without duplicating logic.\n   - Pattern: when introducing batch semantics into a sequential framework, patch or wrap the lowest-level execution primitive (the 'run a single trial' function), and keep the outer orchestration unchanged.\n\n3. **Vectorized parameters with a batched trial wrapper**\n   - `BaseBatchTrial` exposes the same `suggest_*` methods as a regular `Trial` but returns NumPy arrays by iterating over underlying trials and collecting values.\n   - This is conceptually a thin adapter between a vectorized user objective and a scalar sampler/persistence stack.\n   - Pattern: for batched APIs, create a wrapper that presents vectorized views (arrays, lists) to the user while internally delegating to scalar operations.\n\n4. **Decoupling batch sampling from batch evaluation**\n   - The PR discussion separates \"batch sampling\" (samplers that generate multiple points at once) from \"batch evaluation\" (user objective that computes multiple losses at once). The new API focuses on batch evaluation.\n   - Users who need complex conditional search spaces are advised to use batch sampling only (through `BatchedSampler` with normal `Trial`), not the batch evaluation API, because batch evaluation expects largely homogeneous computation across the batch.\n   - General guidance: if per-item control flow diverges strongly (e.g. conditional parameters), it's often better to use sequential evaluation or pure batch sampling rather than batch evaluation.\n\n5. **Careful trial lifecycle handling for vectorized results**\n   - When a batched objective returns a vector of results, each element is validated (`float`-castable, not NaN), and the corresponding trial is either marked `COMPLETE` with a value or `FAIL` with `fail_reason` recorded.\n   - In multi-objective mode, the vector of vectors is transposed to align trials with their value tuples; each trial gets its `values` via `_report_complete_values` and is still assigned a dummy scalar `value` for compatibility with storage APIs.\n   - Pattern: when mapping vectorized outputs back to scalar records, enforce per-element validation and explicit error handling just as you would for scalar runs.\n\n6. **Explicitly disabling features whose semantics are unclear in batch mode (pruning)**\n   - Pruning (early stopping based on intermediate values) is not obviously defined for a batch of trials that share a single call to the objective. The implementation opts to disable pruning entirely for `BatchTrial` and warns the user instead of guessing semantics.\n   - The pruner argument is stripped from the batch APIs; underlying studies are created/loaded with `pruner=None`.\n   - Pattern: if a feature’s semantics become ambiguous in a new abstraction (like batching), disable or restrict it deliberately with clear warnings instead of silently adding surprising behavior.\n\n7. **API alignment and minimizing cognitive load**\n   - The batch API mirrors the standard Optuna API: `create_study`, `load_study`, `optimize`, and `n_trials` are all retained. The main addition is `batch_size` in `optimize` and different objective signatures (`BatchTrial`/`BatchMultiObjectiveTrial` and NumPy outputs).\n   - The earlier `n_batches` concept was removed to avoid a second iteration counter; `n_trials` remains the unit the user thinks in, while the batch executor converts it to the underlying loop count.\n   - Pattern: extend existing APIs by adding a minimal number of new parameters and keep core concepts (like `n_trials`) consistent.\n\n8. **Testing a batched layer on top of a mature sequential core**\n   - Tests verify not just correctness of objective values but also propagation of attributes (user/system attrs), params, distributions, datetime fields, callbacks, and resume behavior across batches.\n   - Multi-objective tests ensure the Pareto front is computed correctly despite using a dummy scalar `value` under the hood.\n   - Pattern: when building layered functionality, tests should focus heavily on ensuring that the new layer preserves all the crucial invariants and behaviors of the underlying core.",
        "procedural_memory": [
            "Step-by-step approach for adding or debugging a batched evaluation layer on top of a sequential optimization framework (like Optuna):",
            "Step 1: Define a batched trial abstraction.\n- Create a wrapper class (e.g., BaseBatchTrial) that holds a sequence of underlying trial objects.\n- Implement `suggest_*` methods by delegating to each underlying trial and collecting the results into a NumPy array or list.\n- Provide convenience properties (params, distributions, user_attrs, system_attrs, datetime, number) that combine or list the values for each underlying trial.\n- If some features (like pruning) are not clearly defined in batch mode, explicitly disable them and warn the user instead of trying to emulate ambiguous behavior.",
            "Step 2: Wrap the study with a batch-aware facade.\n- Create a `BatchStudy` class that stores an underlying `Study` instance and delegates most attributes/methods via `__getattr__`.\n- Add an `optimize` method with the same high-level API as the original (`timeout`, `n_trials`, `n_jobs`, `callbacks`, etc.), but also accept a `batch_size` and a batched objective signature (e.g., `objective(batch_trial) -> np.ndarray`).\n- Keep the creation/loading APIs (`create_study`, `load_study`) as thin wrappers that construct a regular study (optionally forcing certain configurations, such as `pruner=None`) and then wrap it in `BatchStudy`.",
            "Step 3: Patch the execution core, not the outer optimization loop.\n- Identify the lowest-level function that actually executes a single trial and calls callbacks (e.g., `_run_trial_and_callbacks`).\n- Write a batched executor function that:\n  - For each outer loop iteration, creates `batch_size` underlying trials (respecting any waiting/enqueued trials).\n  - Wraps them in a `BatchTrial` and calls the batched objective.\n  - Validates the vector of results and writes them back to each trial, setting states (`COMPLETE`/`FAIL`) and values.\n  - Handles exceptions by marking all trials in the batch as failed, recording `fail_reason`, and re-raising if the exception is not in `catch`.\n  - Optionally invokes callbacks once per underlying trial (after storing the trial), using the framework’s standard callback interface.\n- Monkeypatch the study’s executor method with your batched version (e.g., using `types.MethodType` and `functools.partial`), and pass a dummy objective to the outer `optimize` call so the framework’s loop runs unchanged.",
            "Step 4: Align the counting semantics and API.\n- Keep the standard `n_trials` parameter as the user-facing way to specify the number of logical trials.\n- Internally, compute the number of outer loop iterations as `ceil(n_trials / batch_size)`.\n- Document that `n_trials` counts the total number of individual trials and not the number of batches.\n- Do not introduce competing counters (like `n_batches`) unless absolutely necessary; if you had them before, migrate tests and users to `n_trials`.",
            "Step 5: Carefully handle multi-objective results.\n- For multi-objective batched evaluation, make the batched objective return a sequence of arrays (one dimension per objective).\n- After computing the results, stack and transpose the arrays so that each trial is associated with its tuple of objective values.\n- Use the core multi-objective trial API (e.g., `_report_complete_values`) to assign values to each trial, and if needed, set a dummy scalar `value` for storage compatibility.\n- Confirm that higher-level features like Pareto front calculation still work correctly with the new batch layer.",
            "Step 6: Decide what to do with unsupported features (e.g., pruning) and document it.\n- If pruning or other trial-level control-flow features don’t map cleanly to batch semantics, choose one of:\n  - Disable them entirely in batch mode and raise/warn when the user tries to use them.\n  - Define clear batch semantics (e.g., prune only if all members in the batch should be pruned), and implement them explicitly.\n- In this PR, pruning was disabled for `BatchTrial` and pruners were set to None when creating/loading batch studies.",
            "Step 7: Write comprehensive tests.\n- Unit test the batched trial wrapper: `suggest_*` methods, attribute propagation, user/system attrs, datetime fields, `number`, and distributions.\n- Test single-objective batch optimization: basic runs, resuming, enqueueing, callbacks, and failure handling across different storages.\n- Test multi-objective batch optimization: correct shape and content of `values`, intermediate values via `report`, and Pareto front computation.\n- Ensure that `n_trials` and `batch_size` interact as expected (e.g., `n_trials=0`, non-multiples of `batch_size`, parallel jobs).",
            "Step 8: Communicate experimental status and limitations.\n- Mark new batch APIs as experimental (e.g., via an `@experimental` decorator) to signal potential future changes.\n- Document clearly: the new module (`optuna.batch`), the difference between batch evaluation and batch sampling, disabled features (pruning), and how `n_trials` and `batch_size` interact.\n- Provide simple examples demonstrating typical usage:\n  - Single-objective: `study = optuna.batch.create_study(...); study.optimize(objective, n_trials=40, batch_size=4)`.\n  - Multi-objective: `study = optuna.batch.multi_objective.create_study([...]); study.optimize(objective, n_trials=40, batch_size=4)`.\n\nFollowing this pattern allows a developer to add or debug a batched evaluation layer on top of an existing sequential optimization framework with minimal disruption to the core APIs while preserving important semantics like callbacks, logging, and storage behavior."
        ]
    }
}