{
    "search_index": {
        "description_for_embedding": "Optuna previously lacked a safe, public way to manually construct and insert trials into a study; tests and utilities relied on an internal Study._append_trial API. This change introduces an experimental optuna.trial.create_trial() factory to build validated FrozenTrial instances and a public Study.add_trial() method to add them to a study/storage. It updates visualization tests to use the new API, fixes a Python 3.5 syntax issue in create_trial’s signature, aligns timestamps for finished trials, and improves documentation.",
        "keywords": [
            "optuna",
            "create_trial",
            "Study.add_trial",
            "FrozenTrial",
            "Study._append_trial deprecation",
            "manual trial creation",
            "custom studies",
            "hyperparameter optimization",
            "experimental API",
            "Python 3.5 syntax error",
            "visualization tests",
            "trial validation",
            "optuna.trial"
        ]
    },
    "agent_memory": {
        "episodic_memory": "In this pull request, the Optuna maintainers addressed a usability and API design gap around creating custom studies and manually inserting trials. Previously, there was no official way for users to construct arbitrary trials and attach them to a study; the internal Study._append_trial method was used in tests and utilities but was not part of the public API. This made use cases like importing pre-computed results, constructing synthetic studies, or reusing trials across studies awkward and fragile.\n\nTo solve this, the PR introduces two experimental, low-level APIs:\n\n1. optuna.trial.create_trial():\n   - A factory function that constructs a FrozenTrial from user-specified fields: state, value, params, distributions, user_attrs, system_attrs, and intermediate_values.\n   - It sets datetime_start and, if the state is finished, datetime_complete (using the same timestamp for both to avoid artificial durations).\n   - It validates the constructed FrozenTrial by calling trial._validate(), ensuring consistency between params and distributions, state/value combinations, etc.\n   - It is marked experimental(\"2.0.0\") and documented as a low-level API, not the usual way trials are created in optimize(). Docstrings explain parameters and link to Study.add_trial() as a common usage pattern.\n\n2. Study.add_trial(trial: FrozenTrial):\n   - A new experimental public method that replaces the internal _append_trial for adding trials to a study.\n   - It calls trial._validate() and then passes the trial to storage.create_new_trial with template_trial=trial, letting the storage backend assign a real trial_id and number.\n   - The docstring emphasizes that this is primarily for adding already evaluated trials (state.is_finished() == True) and points users to Study.enqueue_trial for queuing trials to be evaluated later.\n   - The public example demonstrates creating a study, using optuna.trial.create_trial() to build a trial with params/distributions/value, adding it via study.add_trial(), then optimizing further and copying trials between studies.\n\nThe internal Study._append_trial implementation, which previously handled constructing a FrozenTrial from raw pieces, is removed in favor of the new create_trial + add_trial path. Any code in the repo that used _append_trial is updated to use add_trial(create_trial(...)). This mainly affects testing utilities and visualization tests:\n\n- optuna.testing.visualization.prepare_study_with_trials now uses study.add_trial(create_trial(...)) instead of study._append_trial(...), to construct synthetic studies for plotting.\n- Visualization tests for contour, parallel_coordinate, slice, and utils modules are updated accordingly, constructing trials via create_trial and adding them via add_trial.\n\nAdditional cleanups and fixes included in this PR:\n\n- A simple test for the new create_trial API is added in tests/test_trial.py. It parameterizes over state being None, COMPLETE, and FAIL to ensure that:\n  - A FrozenTrial is returned.\n  - The state defaults to TrialState.COMPLETE when None is passed.\n  - All fields (value, params, distributions, attrs, intermediate_values) are preserved.\n  - datetime_start is always set, and datetime_complete is set only when the state is finished or defaulted.\n\n- An existing test in tests/test_study.py that used _append_trial is rewritten to test Study.add_trial using optuna.create_trial (imported from the top-level) to create a trial with value=0.8, verifying that the trial is stored and best_value is correct.\n\n- A Python 3.5 SyntaxError is fixed in the create_trial function signature: in Python 3.5, keyword-only arguments with type annotations cannot end with a trailing comma after the last parameter. The signature is adjusted to remove the problematic trailing comma and slightly reorder arguments per review suggestions.\n\n- A subtle behavior fix ensures that when creating a finished trial via create_trial, datetime_complete is set to the same value as datetime_start, avoiding artificial nonzero duration between start and completion when the trial is created programmatically.\n\n- Documentation updates:\n  - optuna.trial.create_trial is added to the trial module reference in docs/source/reference/trial.rst and removed from the top-level optuna reference list to clarify that it lives under optuna.trial.\n  - Docstrings for create_trial and Study.add_trial are expanded with detailed parameter descriptions, examples, and seealso links between each other.\n  - A note is added to create_trial’s docstring clarifying that it is a low-level API and that normal trials are created inside Study.optimize.\n  - The Study.add_trial docstring adds a seealso explaining that enqueue_trial should be used for queuing unevaluated trials.\n\n- Minor example corrections ensure that example objective functions and the values passed to create_trial are consistent (e.g., value and params such that x ** 2 matches the objective).\n\nThe PR was discussed as an experimental but flexible alternative to a previous proposal for custom studies. Reviewers raised concerns about usability and API placement, which were addressed via documentation improvements, moving create_trial into optuna.trial namespace, and clearly marking the API as experimental. After multiple reviews, conflict resolution, and CI fixes, the change was merged with the understanding that future refinements may consolidate these APIs (e.g., via a higher-level study.copy_trials interface).",
        "semantic_memory": "This change illustrates several generalizable patterns and best practices for evolving APIs, especially in libraries managing domain objects like trials or experiments:\n\n1. Introduce public, validated factory functions for complex domain objects.\n   - When users need to construct complex objects (e.g., a FrozenTrial with numerous fields and invariants), providing a dedicated create_X function is safer than exposing low-level constructors directly.\n   - The factory can enforce validation (e.g., via trial._validate()) and handle default values, time stamps, and invariants, centralizing correctness logic.\n   - Marking such APIs as experimental allows for iterative refinement based on user feedback without committing to a stable contract too early.\n\n2. Replace internal, underscored methods with clearer public APIs.\n   - Internal methods like Study._append_trial were used in tests and utilities, creating a soft dependency on a private API.\n   - Elevating the concept to a public method (Study.add_trial) with a well-defined contract (only validated trials, mostly finished ones) makes usage explicit and stable.\n   - Public APIs should encapsulate storage-layer details; here, add_trial passes a template_trial to storage.create_new_trial, delegating ID/number assignment to the storage backend.\n\n3. Separate responsibilities: creation vs. persistence.\n   - create_trial() is responsible for building and validating an in-memory FrozenTrial.\n   - Study.add_trial() is responsible for persisting that trial into a study/storage and ensuring it is consistent with the study’s lifecycle.\n   - This separation simplifies testing and reuse (e.g., you can create trials, inspect them, and only later decide to add them to a study).\n\n4. Be explicit about the intended usage of low-level APIs.\n   - The docstrings explicitly state that create_trial is low-level and that normal users should rely on Study.optimize for trial creation.\n   - Study.add_trial is documented as primarily for adding already evaluated trials, with enqueue_trial recommended for queuing unevaluated ones.\n   - Such guidance reduces misuse and clarifies where an API sits in the abstraction hierarchy.\n\n5. Maintain cross-version Python compatibility with typed signatures.\n   - Using Python typing features in function signatures can surface subtle version-specific issues (e.g., Python 3.5’s stricter handling of trailing commas in annotated keyword-only parameter lists).\n   - It is important to run tests across supported Python versions or use linting tools that catch such compatibility issues.\n\n6. Keep tests and documentation aligned with API changes.\n   - When replacing an internal method with a new public API, all internal usages (tests, helper utilities) should be migrated to use the new API. This both dogfoods the new API and protects it from regressions.\n   - Documentation and reference pages should reflect the new entry points, and examples should be technically correct (e.g., objective function outcomes matching values in example trials).\n\n7. Consider timestamp semantics for synthetic or imported data.\n   - When constructing completed entities post hoc (e.g., finished trials), it’s often more meaningful to have datetime_start == datetime_complete than an artificial, minuscule duration. This reflects that the trial wasn’t executed in real time within the system.\n   - Such design decisions affect derived metrics like duration and should be chosen consciously.\n\nIn broader terms, this work shows how to expose internal capabilities (manual trial creation and insertion) as deliberate, validated public APIs that are safe to use for advanced workflows (like importing results or constructing synthetic studies) while preserving the integrity of the system and keeping room for future redesign via experimental markers.",
        "procedural_memory": [
            "Step-by-step instructions on how to design and implement a public API for manually constructing and inserting domain objects (like trials) into a system while preserving validation and compatibility.",
            "Step 1: Identify internal behaviors that users need but cannot access safely.\n- Scan the codebase for internal methods (e.g., names starting with an underscore like Study._append_trial) that are used outside their original narrow scope, especially in tests or utilities.\n- Identify user requests or issues (e.g., custom studies, importing trials) that require manual creation or insertion of domain objects.\n\nStep 2: Design a factory function that encapsulates object construction and validation.\n- Create a dedicated create_X function (e.g., create_trial) that:\n  - Accepts all relevant fields as keyword arguments with type hints.\n  - Applies sensible defaults (e.g., default state to COMPLETE if None, initialize timestamps).\n  - Calls a central validation method (e.g., trial._validate()) to enforce invariants (matching params and distributions, acceptable state/value combos, etc.).\n- Mark this function as experimental if you anticipate API changes, and clearly document that it is low-level and primarily for advanced use cases.\n\nStep 3: Introduce a public method to add the object to the system.\n- Implement a public method (e.g., Study.add_trial) that:\n  - Accepts the validated object (FrozenTrial).\n  - Calls its validation method again defensively if necessary.\n  - Passes it to the underlying persistence layer (e.g., storage.create_new_trial(template_trial=trial)) to assign IDs and register it.\n- Decide and document the intended usage: e.g., mostly for adding finished/evaluated trials vs. registering trials to be evaluated later.\n\nStep 4: Replace internal method usage with the new API.\n- Search for all occurrences of the internal method (Study._append_trial) in the codebase, especially in tests and helper modules.\n- Replace calls like study._append_trial(...) with the pair study.add_trial(create_trial(...)).\n- Ensure that all arguments are correctly mapped: values, params, distributions, attrs, and intermediate_values.\n\nStep 5: Update tests to cover the new functionality.\n- Add unit tests for the new factory function:\n  - Test multiple states (None, COMPLETE, FAIL, etc.).\n  - Verify that all fields are preserved and that timestamps behave as expected (datetime_start always set; datetime_complete set only for finished states).\n- Update existing tests that depended on the internal method to use the new public API and ensure they still pass.\n- Add tests for the new add_trial method to confirm that trials are stored correctly (IDs assigned, best_value updated, counts correct).\n\nStep 6: Address compatibility and syntactic issues across supported environments.\n- Run the test suite across all supported Python versions or environments.\n- Look for syntax errors or incompatibilities, particularly in function signatures with type annotations and keyword-only parameters.\n- Fix issues such as trailing commas in Python 3.5 annotated signatures by adjusting the parameter list (e.g., moving or removing the trailing comma, reordering arguments when needed).\n\nStep 7: Refine semantics like timestamps and default behavior.\n- Decide how to set timestamps for programmatically constructed objects:\n  - If the object represents a past, already completed entity, consider using the same timestamp for start and complete to avoid artificial durations.\n- Ensure default states (e.g., defaulting None to COMPLETE) are clearly documented and tested.\n\nStep 8: Update documentation and public references.\n- Add the new functions/methods to the appropriate reference pages (e.g., optuna.trial.create_trial to the trial module reference).\n- Remove or adjust any top-level references if you want the API to live under a module namespace (e.g., optuna.trial.create_trial instead of optuna.create_trial).\n- Write or refine docstrings:\n  - Provide clear examples showing how to create an object and how to add it to the system.\n  - Add seealso links between related API points (create_trial ↔ Study.add_trial, and Study.enqueue_trial for queuing).\n  - Add notes warning that this is a low-level or experimental API and indicate the typical high-level alternative (Study.optimize).\n\nStep 9: Ensure consistency and correctness in examples.\n- Review documentation examples and code comments to ensure that values used in examples actually satisfy the described behavior (e.g., objective functions, squared values, etc.).\n- Adjust any inconsistent numbers or parameter values to avoid confusing users.\n\nStep 10: Monitor usage and iterate.\n- After releasing the experimental API, observe user feedback and actual usage patterns.\n- Consider higher-level abstractions (e.g., study.copy_trials or study.import_trials) that might build on this low-level API and hide unnecessary complexity.\n- Plan for future deprecation or stabilization of the experimental API based on what works well in practice."
        ]
    }
}