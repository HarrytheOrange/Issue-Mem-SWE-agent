{
    "search_index": {
        "description_for_embedding": "Implementation and refinement of a Home Assistant integration for Tado thermostats, including a shared throttled data store for climate and sensor platforms, correct handling of climate operation state, temperature unit conversion, safe iteration over mutable sensor collections, and guards against empty data returned during fast startup.",
        "keywords": [
            "home-assistant",
            "tado",
            "climate",
            "sensor",
            "PyTado",
            "DATA_TADO",
            "Throttle",
            "hass.data",
            "operation_mode",
            "temperature_unit",
            "race condition",
            "dict changed size during iteration",
            "unofficial API",
            "shared data store"
        ]
    },
    "agent_memory": {
        "episodic_memory": "This pull request introduces and stabilizes a new Home Assistant integration for Tado heating systems using the unofficial myTado.com API via the PyTado library. Initially, the integration was added as a `tado_v1` component with separate climate and sensor platforms, each talking to the API. The climate device originally overrode `state` to return the current temperature, violating the established climate pattern where the state reflects the current operation mode. Reviewers asked to conform to the base `ClimateDevice` behavior, so the custom `state` property was dropped and the built-in state handling (based on `current_operation`) is now used.\n\nThe implementation underwent several refactors:\n- The component was renamed from `tado_v1` to `tado`, and the hass.data key was centralized as a constant (`DATA_TADO = 'tado_data'`).\n- A shared `TadoDataStore` hub object was introduced. It wraps the PyTado API (`getZones`, `getCapabilities`, `getMe`, `resetZoneOverlay`, `setZoneOverlay`, etc.) and maintains a dict of logical sensors keyed by `data_id`. Its `update` method is throttled with Home Assistant's `Throttle(MIN_TIME_BETWEEN_UPDATES)` decorator to avoid excessive API calls and is shared across both climate and sensor platforms.\n- Originally, the climate platform listened to state changes from separate sensor entities via `track_state_change` and parsed their attributes. This was replaced by direct use of the shared `TadoDataStore`: both climate entities and sensors register themselves via `add_sensor(data_id, sensor_info)`, and in their `update` methods they pull their current data with `get_data(data_id)`.\n- To avoid runtime errors from modifying the sensors dict during iteration in the store's `update` method, the loop was changed from `for data_id, sensor in self.sensors.items()` to `for data_id, sensor in list(self.sensors.items())`.\n- Fast Home Assistant startup sometimes triggered entity updates before any data had been fetched from Tado, causing errors. To address this, both climate and sensor `update` methods now check if `data` is `None` and return early with a debug log if no data is available.\n- Operation modes were cleaned up: internal Tado constants (e.g., `SMART_SCHEDULE`, `OFF`, `MANUAL`, `TIMER`, `TADO_MODE`) are now mapped to human-readable strings via an `OPERATION_LIST` mapping. `current_operation` returns the readable value, `operation_list` exposes the human-readable options, and `set_operation_mode` translates back from the readable string to the internal constant before calling the API.\n- Temperature handling was aligned with Home Assistant's unit system: raw Tado temperatures (always in Celsius) are converted using `hass.config.units.temperature(...)` for both climate and sensor entities, and the climate platform implements `temperature_unit` (which base `ClimateDevice` does not) so the UI knows the correct unit. Min/max temperatures from Tado capabilities are likewise converted to the configured unit.\n- Miscellaneous cleanups included removing default empty strings from `CONF_USERNAME`/`CONF_PASSWORD` in the config schema (making them truly required), adopting `homeassistant.helpers.discovery.load_platform` instead of the old import path, updating `requirements_all.txt` with the PyTado zip requirement, excluding the new component from coverage in `.coveragerc`, and removing unnecessary pylint directives and redundant `schedule_update_ha_state` calls.\n\nAfter these changes the Tado integration provides a `tado` component acting as a hub plus `climate.tado` and `sensor.tado` platforms that: share a throttled API data store, respect established climate state semantics, handle operation modes cleanly, convert temperatures correctly, and avoid race conditions and dictionary mutation errors during updates.",
        "semantic_memory": "This PR exemplifies several general best practices for building and stabilizing Home Assistant integrations (or similar multi-entity API integrations):\n\n1. **Centralized data store for multi-platform integrations**: When multiple platforms (e.g., climate and sensor) share the same physical device/API, create a hub or store object (here `TadoDataStore`) that wraps the API client, throttles requests, and maintains a shared cache. Expose this store via `hass.data` under a named constant (e.g., `DATA_TADO`) so all platforms can reuse it. This reduces API load, avoids duplicated logic, and keeps data consistent across entities.\n\n2. **Respecting base class semantics**: Home Assistant base classes (e.g., `ClimateDevice`) define expectations for how attributes and state should behave. Overriding core properties like `state` to return something different (e.g., current temperature instead of operation mode) can break UI expectations and automations. Prefer to use or extend the base semantics; only override when absolutely necessary, and then do so consistently across all platforms.\n\n3. **User-facing vs internal representation**: For modes or states that have internal constants (like Tado's overlay/operation modes), provide a mapping to human-readable strings for the frontend (`OPERATION_LIST`) and keep internal logic keyed by constants. Then implement `operation_list`, `current_operation`, and `set_operation_mode` so they expose readable values but convert internally to the API's expected constants.\n\n4. **Consistent unit handling**: When an external API uses fixed units (e.g., Celsius) but the host system can be configured for different units, always convert at the boundaries. Use a central utility (here `hass.config.units.temperature`) for both sensor and climate entities, and ensure min/max and target temperatures all go through the same conversion. Also explicitly expose `temperature_unit` if the base class doesn't.\n\n5. **Guarding against early or missing data**: In event-driven systems, entities can be updated before the underlying data store has retrieved data. Always treat `None` or empty data returns as possible and guard against them in `update` methods, logging at debug level and returning early rather than throwing exceptions.\n\n6. **Safe iteration over mutable collections**: When a data structure (like the `sensors` dict in a shared store) can be mutated while iterating (for example, if new sensors are added during runtime), iterate over a copy (`list(self.sensors.items())`) to avoid `RuntimeError: dictionary changed size during iteration` and related issues.\n\n7. **Configuration and discovery patterns**: Use a domain-specific config schema with `vol.Required` for mandatory options (no silent defaults like empty strings for credentials), store the initialized API client/store in `hass.data`, and use `homeassistant.helpers.discovery.load_platform` to initialize dependent platforms, passing the global config and any discovery info as needed.\n\n8. **API wrapper classes**: Even when using an existing client library, it can be useful to wrap it in a thin adapter (`TadoDataStore`) that presents a more Home-Assistant-friendly interface (camelCase to snake_case, central throttling, common error handling) and hides implementation details from the platforms.\n\nOverall, the PR demonstrates how to evolve a feature from a naive direct-API-per-entity implementation to a robust, shared, and idiomatic integration that fits the host framework’s conventions and avoids subtle runtime errors.",
        "procedural_memory": [
            "Step-by-step instructions on how to design and fix similar multi-entity Home Assistant (or similar framework) integrations:",
            "Step 1: Introduce a hub component for shared API access.\n- Create a component module (e.g., `components/tado.py`) that parses configuration (username, password, etc.).\n- Initialize the API client (e.g., PyTado) in `setup()` and fail fast with clear logging if authentication or connectivity fails.\n- Wrap the client in a hub/store class (e.g., `TadoDataStore`) that will handle throttling and caching.",
            "Step 2: Expose the hub/store via hass.data.\n- Define a constant key (e.g., `DATA_TADO = 'tado_data'`).\n- In `setup()`, store the hub/store in `hass.data[DATA_TADO]`.\n- Use `homeassistant.helpers.discovery.load_platform` to load dependent platforms (e.g., `sensor`, `climate`) with the full config.",
            "Step 3: Implement a shared throttled data store.\n- In the hub/store class, maintain `self.sensors = {}` and `self.data = {}` keyed by logical `data_id`s (e.g., `zone Living Room 1`, `device Home 1`).\n- Implement an `update` method decorated with `@Throttle(MIN_TIME_BETWEEN_UPDATES)` to limit API calls.\n- In `update`, iterate over a copy of the sensors dict (`for data_id, sensor in list(self.sensors.items()):`) to avoid mutation during iteration.\n- For each sensor entry, decide whether to call API zone state or device state methods and store the raw response in `self.data[data_id]`.",
            "Step 4: Provide registration and lookup helpers.\n- Implement `add_sensor(data_id, sensor_info)` on the store to register sensors/climate devices and initialize `self.data[data_id] = None`.\n- Implement `get_data(data_id)` to return cached data, defaulting to `None` or a clear sentinel if not available.",
            "Step 5: Implement sensor and climate platforms using the store.\n- In `sensor.tado.setup_platform`, retrieve `tado = hass.data[DATA_TADO]`, query zones/devices via the store's wrappers, and for each create a `data_id` and register with `add_sensor()`.\n- Instantiate entity classes (e.g., `TadoSensor`) with a reference to the store and their `data_id`.\n- In the entity `update()` methods, call `self._store.update()`, then `data = self._store.get_data(self._data_id)`.\n- If `data` is `None`, log a debug message and return early to avoid errors on startup.",
            "Step 6: Handle units and value conversion consistently.\n- When the external API uses fixed units (e.g., Celsius), store raw values in that unit, but convert to the host’s configured units before exposing them in entity state.\n- Use a central utility (like `self.hass.config.units.temperature(value, TEMP_CELSIUS)`) for all temperature conversions (current, target, min, max).\n- Implement `temperature_unit` in climate entities when the base class does not, returning `self.hass.config.units.temperature_unit` or the appropriate constant.",
            "Step 7: Follow base class semantics for state and operations.\n- Avoid overriding the `state` property unless absolutely necessary; rely on base class behavior so the UI and automations behave as expected.\n- For climate entities, implement `current_operation`, `operation_list`, `set_operation_mode`, `current_temperature`, `target_temperature`, `current_humidity`, etc., according to the framework’s expectations.\n- If the underlying API uses internal enums/constants for modes, define a mapping (like `OPERATION_LIST`), and use it to present human-readable mode names in `operation_list` and `current_operation`, while converting back in `set_operation_mode`.",
            "Step 8: Handle overlay/override logic carefully.\n- When a system supports scheduled vs manual vs timer overrides, track both the current operation and any overlay mode.\n- In updates, derive `_current_operation` and `_overlay_mode` based on the API response (`overlay` present, `termination` type, `power` state).\n- In control methods (`set_temperature`, `set_operation_mode`), decide whether to reset schedule (`reset_zone_overlay`) or set an overlay (`set_zone_overlay`) based on the desired mode.",
            "Step 9: Prevent race conditions and startup errors.\n- Recognize that entities may get updated before the hub/store has fetched data.\n- In each entity’s `update`, check if `data` is `None` or missing expected keys before parsing; if so, log at debug level and return.\n- Avoid calling `schedule_update_ha_state()` at the end of `update` if the framework already does that; reserve it for cases where you change state in response to external triggers outside the normal polling/update cycle.",
            "Step 10: Keep configuration and dependencies tidy.\n- Use `vol.Required` for configuration keys that must be supplied (e.g., `CONF_USERNAME`, `CONF_PASSWORD`) with no empty-string defaults.\n- Add new dependencies to `REQUIREMENTS` in the hub component and to `requirements_all.txt` using the correct URL/hash notation if pulling from a GitHub release archive.\n- Exclude complex hardware integrations from coverage in `.coveragerc` if they are hard to test automatically.\n- Rename components and constants to generic names (like `tado` instead of `tado_v1`) so that future API changes can be handled internally without breaking callers.",
            "Step 11: Validate integration behavior.\n- Confirm that all platforms (climate, sensor, device_tracker, etc.) obtain data via the shared store and respect throttling.\n- Verify that UI displays human-readable operation modes and correct temperature units.\n- Ensure that toggling modes and setting temperatures causes expected API calls and that overlay behavior matches Tado’s semantics.\n- Watch logs for debug messages about missing data or connection errors to fine-tune throttling or error handling."
        ]
    }
}