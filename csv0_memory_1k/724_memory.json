{
    "search_index": {
        "description_for_embedding": "Refactor and parametrize napari viewer tests by introducing a pytest fixture that consistently sets up and tears down the Viewer, consolidating repeated layer tests into a single parametrized test with shared assertions. This reduces duplication, improves reliability of GUI/Qt tests, and ensures the viewer is always closed after each test.",
        "keywords": [
            "napari",
            "pytest",
            "parametrize",
            "fixture",
            "qtbot",
            "GUI tests",
            "Viewer",
            "resource leak",
            "test refactor",
            "layer tests"
        ]
    },
    "agent_memory": {
        "episodic_memory": "In this pull request, the contributor focused on the napari test suite related to the GUI `Viewer`. Originally, there was a file `test_viewer.py` containing many similar tests that each manually instantiated a `Viewer`, registered its Qt widget with `qtbot`, executed some operations (adding images, labels, points, vectors, shapes, volumes, surfaces), and then manually closed the viewer. The setup and teardown logic was duplicated across many tests and the closing of the viewer was not entirely uniform, which can lead to resource leaks or flaky GUI tests in a Qt environment.\n\nTo address this, the file was renamed to `test_mutating_viewer.py` to better reflect that these tests modify the viewer state. A new pytest fixture `setup_viewer` was introduced:\n- It instantiates a `Viewer`, obtains `viewer.window.qt_viewer`, adds the widget to `qtbot`, and yields `(viewer, view)`.\n- After the test using the fixture completes, it automatically closes the viewer via `viewer.window.close()`.\n\nAll tests that previously created and closed the viewer manually were updated to use `setup_viewer`. This centralizes viewer lifecycle management, making it more reliable and less error-prone.\n\nAdditionally, a number of nearly identical tests for different layer types (image, labels, points, vectors, shapes) were consolidated. Instead of separate tests (e.g., `test_add_image`, `test_add_labels`, etc.), a single parametrized test `test_all_layer_types` was created with `@pytest.mark.parametrize(\"layer_name, data\", test_data)`, where `test_data` holds the synthetic numpy arrays for each layer type. This test calls the appropriate `add_<layer_name>` method on the viewer, then runs a shared assertion helper `_asserts_per_layer` that checks:\n- the underlying data is correct,\n- the number of layers and layout widgets is correct,\n- viewer dimensions and number of sliders are as expected,\n- 2D/3D display mode toggling behaves correctly.\nIt also iterates through `layer.class_keymap` to exercise all class keybindings, matching the behavior of the original tests.\n\nOther tests like `test_add_volume`, `test_add_pyramid`, and `test_add_surface` were simplified to reuse `_asserts_per_layer`, passing the appropriate `ndim` and slider expectations (e.g., 3D surface having one displayed slider). The screenshot test was also refactored to use `setup_viewer` and reuse the same `test_data` to add multiple layers before taking a screenshot.\n\nOverall, this PR does not change runtime behavior of napari itself but refactors the tests to be more maintainable, less duplicated, and safer with respect to Qt viewer lifecycle. It also ensures parity with the existing test suite by keeping the same logical checks while changing the structure to use fixtures, parametrization, and shared helpers.",
        "semantic_memory": "This PR illustrates several best practices for structuring and maintaining a GUI-oriented test suite in Python using pytest:\n\n1. **Centralized resource management via fixtures**: When tests repeatedly create and tear down the same kind of resource (e.g., GUI viewers, database connections, network servers), it is safer and cleaner to encapsulate that logic in a pytest fixture with `yield`. The fixture ensures consistent setup and teardown, reducing the chances of resource leaks and flaky tests.\n\n2. **Parametrizing similar tests**: When multiple tests differ only in parameters (such as data type, shape, or the specific method called), using `pytest.mark.parametrize` and a single generalized test function removes duplication and makes the test intent clearer. For napari, various `add_*` layer methods (image, labels, points, vectors, shapes) can be tested uniformly by parametrizing on the layer type and input data.\n\n3. **Shared assertion helpers**: Abstracting common validation logic into helper functions (like `_asserts_per_layer`) keeps tests concise and ensures that all cases are validated consistently. This is especially useful when asserting GUI-related state (number of sliders, layout elements, dimensions) that must be checked in many places.\n\n4. **Consistent teardown in GUI/Qt tests**: In GUI-based test suites, failing to close windows or widgets after tests can result in lingering state, memory usage, or interference between tests. Using fixtures that always call `close()` in teardown mitigates this class of problems and makes the test suite more deterministic.\n\n5. **Keeping test behavior but improving structure**: A refactor can maintain parity with the original testsâ€™ logical checks while significantly simplifying structure. This is important in existing projects where test behavior is a form of specification: you want to improve maintainability without unintentionally weakening coverage.\n\nThese patterns apply broadly to any Python project using pytest, especially those involving GUI frameworks like Qt, or any environment where instantiated resources must be reliably cleaned up between tests.",
        "procedural_memory": [
            "When you notice duplication and inconsistent teardown in tests that create similar resources, refactor using pytest fixtures and parametrization.",
            "Step 1: Identify repeated setup/teardown logic in tests.",
            "Step 2: Create a pytest fixture that encapsulates resource setup and teardown. For GUI/Qt tests, the fixture should:\n- Instantiate the widget or viewer.\n- Register it with any test helpers (e.g., `qtbot.addWidget(view)`).\n- Yield the necessary objects for the test to use.\n- After the `yield`, perform cleanup (e.g., `viewer.window.close()`).",
            "Step 3: Replace direct instantiations in tests with the fixture argument. For example, change `viewer = Viewer()` and manual close calls to `viewer, view = setup_viewer`.",
            "Step 4: Locate groups of tests that differ only by parameters (e.g., different layer types, input shapes, or configuration flags). Create a parameter list (e.g., `test_data = [(layer_name, data), ...]`) and a single parametrized test using `@pytest.mark.parametrize` that loops through these cases.",
            "Step 5: Factor out common assertions into helper functions. For example, create `_asserts_per_layer(viewer, view, data, ndim=2, sliders=0)` to check that:\n- The layer data matches the input data.\n- The number of layers and layout widgets is as expected.\n- Viewer dimensions, number of sliders, and displayed sliders are correct.\n- Toggling dimension display modes (2D/3D) behaves as expected.",
            "Step 6: Ensure behavioral parity with the original tests. For each old test, confirm that:\n- The same actions are performed (e.g., calling all `layer.class_keymap` functions).\n- The same conditions are asserted, either directly or via your helper functions.",
            "Step 7: Update any tests that build on these patterns (like screenshot or update tests) to reuse the fixture and any test data lists, rather than manually recreating viewers and layers.",
            "Step 8: Run the full test suite (including any GUI test markers) multiple times or on CI to confirm that tests are stable, no windows are left open, and no new flakiness is introduced.",
            "Step 9: If applicable, update documentation or developer guidelines to recommend the new fixture and patterns, so future tests follow the same structure."
        ]
    }
}