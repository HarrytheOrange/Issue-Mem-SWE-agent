{
    "search_index": {
        "description_for_embedding": "Home Assistant recorder component: database purge was hard-coded to run every 2 days after startup, which meant short-lived or frequently restarted instances would never purge and databases would grow indefinitely. The PR adds a configurable purge_interval (in days), triggers a purge immediately on Home Assistant start, and normalizes the purge cutoff time to midnight (00:00:00) to reduce the volume of rows affected each run.",
        "keywords": [
            "Home Assistant",
            "recorder",
            "database purge",
            "purge_days",
            "purge_interval",
            "maintenance interval",
            "scheduled task",
            "async_track_time_interval",
            "SQLite",
            "data retention",
            "configuration option",
            "background job"
        ]
    },
    "agent_memory": {
        "episodic_memory": "In this PR, the Home Assistant recorder component had a usability and maintenance issue around database purging. Previously, the system only scheduled a purge job every 2 days using a hard-coded maintenance interval. Because the purge schedule was tied to the time since startup, and many users frequently restart Home Assistant (especially in development), the purge would often never run. This led to databases growing indefinitely during short-lived or frequently restarted sessions, confusing users who expected purge_days to guarantee regular cleanup.\n\nTo address this, the contributor introduced a new configuration option `purge_interval` in `recorder:` configuration, representing the interval (in days) between purge runs. The schema was extended with:\n\n- `CONF_PURGE_INTERVAL` with a default of 2 and a minimum value of 1.\n\nThe `Recorder` class was updated to accept `purge_interval` as a constructor argument and store it as an instance attribute. In the recorder startup logic, instead of hard-coding `timedelta(days=2)`, the purge scheduling uses `timedelta(days=self.purge_interval)`. Additionally, the PR triggers the purge once immediately after Home Assistant has started by calling `do_purge(dt_util.utcnow())` right after registering the interval with `async_track_time_interval`.\n\nSeparately, in `purge_old_data`, the purge cutoff time (`purge_before`) was adjusted to align to midnight. After computing `purge_before = dt_util.utcnow() - timedelta(days=purge_days)`, the code replaces the time portion with 00:00:00.000000, effectively purging whole days instead of partial days. This reduces the number of rows touched during purges and makes retention behavior more predictable.\n\nTests for the recorder initialization were updated to pass the new `purge_interval` parameter when constructing a `Recorder` in `test_recorder_setup_failure`.\n\nIn review discussion, maintainers expressed concerns about purging on startup due to potential long startup times and SD card wear on Raspberry Pi devices. They debated alternatives such as scheduling purges at midnight, exposing a purge service for automations, and avoiding behavior changes solely to accommodate development workflows. Ultimately, the PR was closed as stale without merger, but it illustrates the design considerations around configurability, startup behavior, and background maintenance tasks in the recorder subsystem.",
        "semantic_memory": "This case illustrates several generalizable software engineering patterns about scheduled maintenance tasks and configuration:\n\n1. **Configurable maintenance intervals vs. hard-coded schedules**: Hard-coding a maintenance interval (e.g., purge every 2 days) can lead to poor behavior across diverse usage patterns. Providing a configuration parameter (like `purge_interval`) allows users to tune maintenance frequency based on their environment (development vs. production, hardware constraints, etc.).\n\n2. **Startup-triggered maintenance vs. steady-state operations**: Running heavy maintenance (database purges, migrations, compaction) during startup can significantly degrade startup performance and even trigger hardware issues (e.g., SD card wear on constrained devices). A better pattern is to either:\n   - Schedule maintenance asynchronously after startup, ideally during low-activity periods (like midnight), or\n   - Expose maintenance as a service or command that can be invoked by automation.\n\n3. **Time normalization for retention boundaries**: When implementing retention policies (e.g., keep N days of data), truncating timestamps to a consistent boundary (like midnight) simplifies reasoning about what data is kept and can reduce the rows affected each run. Instead of purging up to an arbitrary timestamp (e.g., `now - N days to the second`), aligning to day boundaries makes purges more predictable and often reduces churn.\n\n4. **Impact of restarts on scheduled tasks**: When a scheduler’s interval is defined relative to the last startup, frequent restarts reset the timer and may prevent long-interval tasks from ever executing. Maintenance tasks relying solely on `every X days since start` can effectively be disabled by frequent restarts. Solutions include:\n   - Checking the data itself to determine whether a purge is due (e.g., based on the oldest entry or last purge marker), or\n   - Scheduling at a specific wall-clock time (e.g., daily at 00:00) rather than relative intervals.\n\n5. **Backward compatibility and test updates when changing API**: Adding a new required argument (like `purge_interval`) to a constructor requires updating all call sites and associated tests. Tests are an early indicator of such API changes and prevent silent breakage.\n\n6. **Separating developer convenience from production defaults**: Changes motivated by development workflows (short-lived processes, frequent restarts) should be carefully weighed against production behavior. Often, the right approach is to offer configurability or explicit tools (services, CLI commands) instead of changing production defaults that might hurt stability or performance.\n\nThese principles apply broadly to any system that performs periodic cleanup, compaction, or retention enforcement on databases or logs.",
        "procedural_memory": [
            "Step-by-step instructions on how to diagnose and fix similar issues involving scheduled maintenance tasks and data retention:",
            "Step 1: Identify the maintenance behavior and symptoms",
            "• Determine what maintenance task is involved (e.g., database purge, log rotation, cache cleanup).\n• Gather reports from users: Is the database/log growing unexpectedly? Is the maintenance never running, or running at inconvenient times (e.g., during startup)?\n• Inspect configuration options and defaults related to retention (e.g., `purge_days`) and scheduling.",
            "Step 2: Examine the scheduling logic",
            "• Locate where the maintenance task is scheduled in the code (e.g., calls to scheduling helpers like async_track_time_interval, cron, timers).\n• Check if the interval is hard-coded or configurable.\n• Analyze how restarts affect the schedule: is the interval relative to startup or bound to a wall-clock time?\n• Verify if maintenance runs at startup, only after a delay, or only under certain conditions.",
            "Step 3: Reproduce the problem in a controlled environment",
            "• Use a small test or dev environment and configure short retention (e.g., 1–2 days) to observe behavior quickly.\n• Restart the application frequently to see if long-interval tasks (e.g., every 2 days) ever execute.\n• Inspect the database/log size over time, and confirm whether old entries are actually removed.",
            "Step 4: Decide on the desired behavior and configuration surface",
            "• Clarify product expectations: Should maintenance run at fixed intervals, at specific times (like midnight), on startup, or via explicit user action (service/CLI)?\n• Decide what should be configurable vs. fixed. Typically:\n  – Retention horizon (e.g., `purge_days`) should be configurable.\n  – Maintenance frequency (`purge_interval`) can be configurable with sane defaults.\n• Consider exposing a manual trigger (e.g., a service endpoint or CLI command) so advanced users can control when heavy maintenance runs.",
            "Step 5: Implement configurability and safer scheduling",
            "• Add a configuration option for the maintenance interval, with validation (e.g., positive integer, min 1 day).\n• Wire the config into the scheduler: replace hard-coded intervals (e.g., `timedelta(days=2)`) with user-configured values.\n• Avoid heavy work directly in startup; instead:\n  – Schedule the maintenance asynchronously after startup, or\n  – Optionally perform a lightweight check at startup to see if maintenance is due based on timestamps in the data or a status marker.\n• If appropriate, normalize retention cutoff times to a boundary (e.g., `purge_before.replace(hour=0, minute=0, second=0, microsecond=0)`) for predictability.",
            "Step 6: Update APIs and tests consistently",
            "• If constructors or functions gain new parameters (e.g., `purge_interval`), update all call sites.\n• Modify tests to pass the new parameters and add tests covering:\n  – Scheduling with default vs. custom intervals.\n  – Behavior when the application is restarted frequently.\n  – Correct calculation of purge cutoff times (e.g., truncated to midnight).",
            "Step 7: Validate performance and resource impact",
            "• Measure startup time before and after changes, especially if any purge logic is executed near startup.\n• For constrained hardware (like Raspberry Pis with SD cards), consider IO impact:\n  – Ensure maintenance is not too frequent.\n  – Optionally provide guidance or presets optimized for such environments.\n• Run load tests or simulations with large databases to ensure purge completes in acceptable time.",
            "Step 8: Document the behavior for users",
            "• Document new configuration options (e.g., `purge_interval`) and their defaults.\n• Clearly explain how retention (`purge_days`) interacts with the maintenance interval (e.g., purge every N days based on configuration).\n• Note how restarts affect scheduling and whether purges happen at startup, on schedule, or only when explicitly triggered.",
            "Step 9: Monitor and iterate",
            "• After deployment, monitor logs and user feedback for:\n  – Unexpectedly large databases.\n  – Complaints about long startup times or IO wear.\n  – Confusion about when purges occur.\n• Adjust defaults or add further controls (such as explicit purge services) as necessary to balance usability, performance, and reliability."
        ]
    }
}