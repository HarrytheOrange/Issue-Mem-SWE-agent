{
    "search_index": {
        "description_for_embedding": "Home Assistant Google Plus device_tracker platform initially required users to manually update a short‑lived Google session token (the 'at' field in POST data) every ~14 days to keep location tracking working. The PR refactors the component to automatically derive this token by scraping the user's Google+ profile page on demand, simplifies configuration (removing frequently changing parameters), and adds basic validation and error logging around the HTML/response parsing.",
        "keywords": [
            "home-assistant",
            "device_tracker",
            "google plus",
            "google maps",
            "web scraping",
            "session token",
            "expiring token",
            "configuration burden",
            "HTML parsing",
            "BeautifulSoup",
            "requests",
            "data_at",
            "f.req",
            "gplus",
            "location tracking",
            "third-party API without official support",
            "fragile integration",
            "config validation"
        ]
    },
    "agent_memory": {
        "episodic_memory": "This PR introduces and iteratively refines a Home Assistant device_tracker platform (`gplus`) that locates a user by scraping Google Plus/Maps data via an authenticated browser-like session.\n\nOriginally, the integration depended on hard-coded cookies and POST parameters captured from browser traffic (e.g., `SID`, `HSID`, `SSID`, `f.req`, and an `at` token). A major usability issue was that the `at` value and sometimes the URL/POST data expired every ~14 days, forcing users to periodically re-snoop network traffic and update `configuration.yaml`. That made the platform fragile and high-maintenance.\n\nOver several commits, the author and contributors made these changes:\n- Moved user-specific sensitive values (cookies and payload fields) out of the code and into configuration.\n- Tightened the config schema using Home Assistant's `config_validation` (strings, positive integers, and sane scan interval ranges).\n- Adjusted headers and cookies to use a safer model with a 'dummy' Google account that only has shared location from the main account, reducing risk if cookies leak.\n- Transitioned from a brittle regex-based location extraction to parsing the response: first by searching for a line containing `www.google.com/maps/` and splitting by commas to read latitude, longitude, and accuracy.\n- Added error handling for cases where Google responds unexpectedly: missing map lines, too few comma-separated fields, or non-OK HTTP status.\n\nThe key functional fix is in later commits: instead of requiring a static `data_at` value in the config, the component now computes `data['at']` dynamically. It does this by:\n- Accepting a new `home_url` setting that points to the tracked user's Google+ profile page.\n- On demand (when `data['at']` is None or when Google stops returning location), issuing a GET to `home_url` with the same cookies/headers.\n- Parsing the first `<script>` tag in the HTML with BeautifulSoup, slicing out an embedded JSON blob, and reading `SNlM0e` as the new `at` token.\n- Storing this token in `data['at']` and using it for subsequent POST requests to the tracking endpoint.\n\nIf a location query fails (missing map link, unexpected structure, or non-OK response), the component logs errors and re-runs the `update_data_at()` discovery logic so that the next scheduled call will use a fresh token. This turns a manual 14-day maintenance task into an automatic recovery mechanism.\n\nDespite these improvements, the Home Assistant maintainer ultimately judged that the platform would likely require too much ongoing maintenance due to Google changing HTML/parameters, and closed the PR. Still, the code illustrates how to manage expiring web-scraping tokens and reduce user config churn.",
        "semantic_memory": "This case illustrates several important patterns when integrating with third-party web UIs or undocumented APIs:\n\n1. **Avoid static configuration for ephemeral tokens**: Values like anti-CSRF tokens, session identifiers, or hidden form fields often expire regularly. Hard-coding or requiring users to update them in configuration files leads to brittle and high-maintenance systems. Instead, derive such values programmatically from the source page whenever needed.\n\n2. **Bootstrap tokens with a preliminary request**: When a subsequent API call requires a transient token (e.g., `at`), you can perform an initial GET to a known page (like a profile or dashboard) and parse a JSON or script block to extract the token. This is similar to how browser-based apps bootstrap their own API clients.\n\n3. **Use structured parsing instead of ad‑hoc regex**: Parsing complex HTML or script responses with regex is fragile. Libraries like BeautifulSoup (or using explicit JSON where possible) provide more resilience and clarity. Even when you end up splitting strings (like `line.split(',')`), you should guard against varying formats and lengths.\n\n4. **Defensive programming against external change**: When you depend on undocumented behavior (HTML structure, script tags, implicit token names), expect breakage. Mitigations include:\n   - Validating assumptions (e.g., `len(words) > 15`) before indexing.\n   - Logging clear error messages when expectations fail.\n   - Having a recovery path (refresh tokens, retry later) instead of silently failing.\n\n5. **Reduce user-facing configuration to stable, meaningful inputs**: Users should provide stable data (e.g., account IDs, base URLs, cookies) rather than fast-changing internal parameters like one-time tokens. The more ephemeral / derived data you can compute automatically, the less burden on users.\n\n6. **Respectful polling and scan intervals**: When scraping external services, you must respect rate limits and anti-abuse expectations. The component introduces validation and sensible minimum/maximum scan intervals to avoid overly aggressive polling that might trigger blocks.\n\n7. **Security considerations for scraping with cookies**: If you must store authentication cookies, using a separate 'dummy' account with limited privileges is safer than using a primary account, because credential leakage is less damaging. This applies to any integration that mirrors browser cookies.\n\n8. **Config validation libraries improve robustness**: Using a central validation layer (like Home Assistant's `config_validation`) improves error reporting, avoids type errors at runtime, and constrains parameters like update intervals to safe ranges.\n\nOverall, this case generalizes to: when building unofficial integrations on top of web pages, design with token expiry and HTML churn in mind, use robust parsers, and automate the acquisition of any transient state.",
        "procedural_memory": [
            "When a web-scraping-based integration breaks periodically due to expiring tokens or changing HTML, you can diagnose and fix it by understanding where transient fields come from and automating their retrieval.",
            "Step 1: Identify which configuration or request fields are expiring.",
            "  - Observe the error symptom: integration stops working after a fixed time (e.g., ~14 days).",
            "  - Compare a working request (from browser dev tools) with the failing one (from your code).",
            "  - Look for fields that changed between the two: hidden form fields, query parameters, JSON keys, or cookies.",
            "  - In this case, the `at` POST parameter changed regularly and caused Google to reject location queries.",
            "Step 2: Locate the origin of the ephemeral field in the web UI.",
            "  - Use the browser's network panel to inspect the HTML/JS of pages visited before the API call.",
            "  - Search in HTML/script for the token or nearby data (e.g., find where `SNlM0e` or `f.req` appears).",
            "  - Identify a stable page (like a profile or home URL) that consistently embeds the required token in a script/JSON block.",
            "Step 3: Implement a programmatic token extraction routine.",
            "  - Add a configuration option for the stable page URL (e.g., `home_url`).",
            "  - Perform a GET request to that URL with the appropriate cookies/headers.",
            "  - Use an HTML parser (e.g., BeautifulSoup) to find relevant `<script>` tags or data blocks.",
            "  - Locate and slice out the JSON or structured data segment containing the token.",
            "  - Parse the JSON and store the token in a variable (e.g., `data['at'] = sdict['SNlM0e']`).",
            "Step 4: Integrate token refreshing into the main request flow.",
            "  - Initialize the token field to `None`.",
            "  - Before sending the main API POST, check if the token is missing or known to be stale.",
            "  - If missing/stale, call the token extraction function to refresh it.",
            "  - Use the updated token in the POST data for the location request.",
            "Step 5: Harden response parsing and error handling.",
            "  - Avoid brittle regex when parsing complex responses; prefer structured parsing or at least robust string operations.",
            "  - Validate assumptions: check list lengths before indexing (`if len(words) > 15:`).",
            "  - If the expected pattern (e.g., a line containing `www.google.com/maps/`) is absent, log a descriptive error and trigger a token refresh for the next attempt.",
            "  - Handle non-OK HTTP responses separately and log them as connectivity/authorization issues.",
            "Step 6: Simplify and validate configuration.",
            "  - Remove fast-changing values (like `data_at`) from the user-facing config; compute them internally instead.",
            "  - Use a validation library to enforce types (strings, positive ints) and ranges (e.g., scan interval between 1 and 59 minutes).",
            "  - Keep config focused on stable identifiers (cookies, base URLs, user IDs).",
            "Step 7: Consider security and operational constraints.",
            "  - If cookies must be stored, recommend using a dedicated account with limited permissions.",
            "  - Ensure polling intervals are not too frequent to avoid being rate-limited or blocked.",
            "Step 8: Monitor for upstream changes.",
            "  - Expect that HTML/JS structures and token formats may change over time.",
            "  - Centralize scraping/parsing logic so it’s easy to update when upstream changes occur.",
            "  - Keep logging granular enough that users can report meaningful diagnostics (unexpected HTML shapes, missing fields) when issues arise."
        ]
    }
}