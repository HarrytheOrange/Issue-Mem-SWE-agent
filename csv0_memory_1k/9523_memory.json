{
    "search_index": {
        "description_for_embedding": "Home Assistant recorder component enhancement: adds a `recorder.purge` service with a `keep_days` parameter and reworks the purge timer so purging can be triggered via service calls and/or a configurable `purge_interval` and `purge_keep_days` in configuration. Removes the hard-coded 2‑day purge timer dependency on uptime, adds validation, and updates tests.",
        "keywords": [
            "homeassistant",
            "recorder",
            "database purge",
            "history retention",
            "recorder.purge service",
            "keep_days",
            "purge_keep_days",
            "purge_interval",
            "async_track_time_interval",
            "voluptuous schema",
            "breaking change",
            "scheduled job",
            "service registration",
            "SQLite",
            "data retention"
        ]
    },
    "agent_memory": {
        "episodic_memory": "In this pull request for Home Assistant's `recorder` component, the author addressed usability and correctness issues around database purging and history retention. Previously, purging was controlled solely by a `purge_days` config option and was executed via a hard-coded 2-day interval timer. This design had several problems: purge wouldn’t run if Home Assistant was frequently restarted (because the 2-day interval was never reached), the configuration naming (`purge_days`) was confusing, and there was no flexible way to trigger a purge on demand.\n\nTo fix this, the author implemented a new `recorder.purge` service and reworked the purge scheduling:\n\n1. **New purge service**:\n   - A service `recorder.purge` was added and registered using a service description file `homeassistant/components/recorder/services.yaml`.\n   - The service requires a `keep_days` parameter (`ATTR_KEEP_DAYS`) defined by a voluptuous schema (`SERVICE_PURGE_SCHEMA`), initially `min=1` and later relaxed to `min=0` to allow complete database purges. The service handler now simply calls `instance.do_purge(service.data[ATTR_KEEP_DAYS])`.\n   - The service description was updated to clarify that `keep_days` represents the number of history days to retain after purge and that it accepts values `>= 0`.\n\n2. **Config changes and purge timer rework (breaking change)**:\n   - The old `purge_days` config key was renamed to `purge_keep_days` (more descriptive), and a new `purge_interval` config option was introduced to control how often the automatic purge runs (in days).\n   - These two options are declared as `vol.Inclusive` group `\"purge\"` in the config schema, meaning both must be present and > 0 or neither:\n     ```python\n     vol.Inclusive(CONF_PURGE_KEEP_DAYS, 'purge'): vol.All(vol.Coerce(int), vol.Range(min=1)),\n     vol.Inclusive(CONF_PURGE_INTERVAL, 'purge'): vol.All(vol.Coerce(int), vol.Range(min=1)),\n     ```\n   - Inside `async_setup`, the component reads `purge_keep_days` and `purge_interval`. The global hard-coded 2-day timer was removed. Instead, if **both** `purge_interval` and `purge_keep_days` are configured, a periodic callback is scheduled via `async_track_time_interval(hass, async_handle_purge_interval, timedelta(days=purge_interval))`. That callback invokes `instance.do_purge(purge_days)`.\n   - This changes behavior from a built-in, always-on 2-day purge to an explicitly configured purge schedule and/or service-based purging.\n\n3. **Recorder internals and purge execution**:\n   - The `Recorder` thread no longer receives a `purge_days` argument; it is constructed as `Recorder(hass, uri=db_url, include=include, exclude=exclude)` and internally holds `self.purge_days`, set to `None` initially.\n   - A special `self.purge_task` sentinel object is created. The `do_purge(self, purge_days=None)` method sets `self.purge_days` to the provided value and pushes `self.purge_task` into the processing queue.\n   - In `run()`, the event loop now checks `elif event is self.purge_task:` and calls `purge.purge_old_data(self, self.purge_days)` when this sentinel is encountered, ensuring purging runs in the recorder thread context using the existing DB session handling.\n\n4. **Service registration and descriptions**:\n   - The `services.yaml` file for the recorder component was added to describe the new `purge` service and its `keep_days` field, providing better UI and developer documentation.\n   - Service registration uses:\n     ```python\n     hass.services.async_register(\n         DOMAIN,\n         SERVICE_PURGE,\n         async_handle_purge_service,\n         descriptions.get(SERVICE_PURGE),\n         schema=SERVICE_PURGE_SCHEMA,\n     )\n     ```\n\n5. **Tests updated/added**:\n   - `Recorder` initialization tests (`test_recorder_setup_failure`) were updated to reflect the new constructor signature (no `purge_disable_timer` and no `purge_days` argument).\n   - Purge tests (`tests/components/recorder/test_purge.py`) were updated to initialize recorder with `{'purge_keep_days': 4, 'purge_interval': 2}` and verify purge behavior.\n   - A new test `test_purge_method` verifies that calling `recorder.purge` without service data does nothing (in intermediate state of the PR), and that calling it with `service_data={'keep_days': 4}` actually purges the correct number of states and events. Later refactorings simplified the service handler to always expect `keep_days` as required; the schema ensures presence.\n\n6. **Minor cleanups and refactors**:\n   - The initial attempt to add a `purge_disable_timer` flag was removed after design discussion, in favor of always using explicit configuration and services for purge timing.\n   - Redundant logging and conditionals were simplified by maintainers (e.g., service handler always uses `service.data[ATTR_KEEP_DAYS]` and interval handler directly calls `instance.do_purge(purge_days)`).\n\nThe net effect is a more predictable, flexible purge mechanism: users can configure how often automatic purge runs via `purge_interval` and how many days to keep via `purge_keep_days`, and can manually invoke purging with a service call supplying `keep_days`, including fully wiping history with `0`. This also resolves the original problem where frequent restarts prevented the hard-coded 2-day purge timer from ever firing.",
        "semantic_memory": "This change illustrates several generalizable patterns and best practices when designing scheduled data maintenance in an event-driven, async system:\n\n1. **Service-based operations vs. hard-coded timers**:\n   - Instead of tying critical maintenance (like DB purging) to a fixed internal timer, expose it as a service that can be invoked on demand and orchestrated by users (e.g., via automations, scripts, or external triggers). This avoids coupling retention behavior to uptime or restart patterns.\n   - Service calls with explicit parameters (like `keep_days`) provide clearer semantics and improved testability compared to relying only on config globals.\n\n2. **Config design and breaking changes**:\n   - Use descriptive names for configuration keys (`purge_keep_days` instead of `purge_days`) to clearly communicate intent (retention vs. action).\n   - When two config values must be present together to make sense (e.g., an interval and a retention period), enforce this at the schema level using grouped constraints (like `vol.Inclusive` or equivalents). This yields earlier, clearer errors and prevents half-configured features.\n   - When making breaking config changes, centralize the behavior changes and keep the runtime logic simple (e.g., if both interval and days are set, set up a timer; otherwise don’t schedule the job).\n\n3. **Safe scheduling and threading in async architectures**:\n   - Long-running or IO-heavy tasks (database purge) should not run directly in event loop callbacks. Instead, signal the worker thread via a queue and use sentinel objects to represent internal control messages.\n   - Using a dedicated `purge_task` sentinel object, and checking identity (`event is self.purge_task`) in the worker loop, is a simple and robust pattern for out-of-band control actions sharing the same queue as regular events.\n   - Use framework helpers like `async_track_time_interval` to schedule recurring tasks instead of hand-rolling timers; integrate with the framework’s event loop and time abstractions.\n\n4. **Schema validation and API guarantees**:\n   - Enforce required service parameters via schema (e.g., `SERVICE_PURGE_SCHEMA`) so that the handler can safely assume presence and type, simplifying service code and avoiding defensive checks.\n   - Use numeric ranges in schemas (`vol.Range(min=0)`) to encode domain rules (e.g., allowing `0` for a full purge) and keep those rules close to the API surface.\n\n5. **Service description files and discoverability**:\n   - Maintaining a `services.yaml` per component describing services and their fields improves UX and auto-generated documentation. This supports easier discovery and consistent interfaces.\n\n6. **Testing behavior rather than implementation details**:\n   - Tests create test data, invoke the new service or rely on the configured interval, and then assert on resulting DB state counts, ensuring the scheduling and service invocation pathways both lead to correct data retention behavior.\n   - Including small waits (`sleep`) in tests for background threads is sometimes necessary, but ideally these should be minimized or replaced by explicit synchronization primitives; still, this pattern is often used in event-driven test suites to allow worker threads to catch up.\n\nOverall, the PR reinforces the principle that data lifecycle management (purging, retention) should be explicit, configurable, and decoupled from incidental properties like process uptime, with clear user-facing services and validated inputs.",
        "procedural_memory": [
            "Step-by-step instructions on how to design, implement, and test a configurable purge/maintenance service in an async, event-driven component.",
            "Step 1: Identify shortcomings in existing timer-based maintenance logic.\n- Look for behavior tightly coupled to process uptime or hard-coded intervals (e.g., a purge job only running every 2 days regardless of user needs).\n- Confirm real user issues: purge never runs due to frequent restarts, confusing config naming, or lack of on-demand triggers.",
            "Step 2: Redesign the external API around a service.\n- Decide on a service name (e.g., `recorder.purge`) and the required parameters (e.g., `keep_days` = number of days to keep).\n- Define a validation schema for the service using your framework’s validation utilities (e.g., voluptuous):\n  - Coerce types (e.g., `vol.Coerce(int)`),\n  - Enforce ranges (`vol.Range(min=0)` to allow full purge when 0),\n  - Make parameters required if the service cannot reasonably operate without them.\n- Add a service description file (e.g., `services.yaml`) that documents the service and its parameters for UI and documentation.",
            "Step 3: Adjust configuration schema for automatic scheduling.\n- Introduce configuration keys for retention and interval (e.g., `purge_keep_days`, `purge_interval`).\n- Use grouped validation to ensure they are configured together where appropriate, such as `vol.Inclusive('purge_keep_days', 'purge')` and `vol.Inclusive('purge_interval', 'purge')`.\n- Decide on parameter ranges (e.g., `min=1` days for both) and reflect those in the schema.\n- If renaming existing options (breaking change), migrate naming in code and update docs, making sure old behavior is clearly explained in release notes.",
            "Step 4: Wire the service and interval into setup.\n- In the component’s `async_setup` or equivalent:\n  - Read retention and interval config values.\n  - Create and start your worker/manager instance (e.g., `Recorder`).\n  - Define an async interval handler that calls the worker’s purge method with the configured retention days.\n  - Set up a recurring timer using a framework helper (e.g., `async_track_time_interval(hass, handler, timedelta(days=purge_interval))`), but only if both `purge_interval` and `purge_keep_days` are configured.\n  - Define the service handler which fetches `keep_days` from `service.data` and passes it to the worker: `instance.do_purge(service.data[ATTR_KEEP_DAYS])`.\n  - Register the service with the description and schema: `hass.services.async_register(DOMAIN, SERVICE_PURGE, handler, description, schema=SERVICE_PURGE_SCHEMA)`.",
            "Step 5: Implement the worker-side purge trigger.\n- In the worker class (e.g., `Recorder`), create a sentinel object for purge tasks: `self.purge_task = object()`.\n- Maintain a queue where the worker receives events and control tasks.\n- Implement a `do_purge(self, purge_days)` method:\n  - Store the requested retention on the instance (`self.purge_days = purge_days`).\n  - Push the sentinel into the queue: `self.queue.put(self.purge_task)`.\n- In the worker’s main loop, add a branch for the sentinel:\n  - If `event is self.purge_task`, call the actual purge routine (e.g., `purge.purge_old_data(self, self.purge_days)`), then `continue` to the next iteration.\n- Ensure that `purge_old_data` uses the worker’s DB connection/session managed by the worker thread to avoid concurrency issues.",
            "Step 6: Remove old hard-coded timers and conflicting config.\n- Identify and delete old timer setup code that runs purge on a fixed interval (e.g., previously using `async_track_time_interval` with a hard-coded `timedelta(days=2)`).\n- Remove obsolete config flags (such as a `purge_disable_timer` toggle) and adjust constructor signatures, tests, and any call sites that referenced them.\n- Verify that no remaining code path assumes purging will happen automatically without the new configuration or service calls.",
            "Step 7: Update and add tests.\n- Update existing tests to use the new configuration names and constructor signatures. For example, change `purge_days` to `purge_keep_days` where appropriate.\n- Add tests that validate:\n  - Purge removes the correct number of states/events when triggered by configuration-based interval (if practical to test),\n  - Purge removes the correct amount when triggered by the `recorder.purge` service with `keep_days` set,\n  - Calling the service with invalid data (e.g., negative numbers) is rejected by the schema.\n- In tests involving background threads:\n  - Insert a small wait (e.g., `sleep(0.1-0.5)` depending on environment) after triggering purge to allow the worker to process the queue.\n  - Alternatively, if available, use synchronization helpers like `block_till_done()` to ensure all pending tasks have completed before assertions.\n- Assert on DB state counts (e.g., using ORM queries) to confirm data retention matches the requested `keep_days`.",
            "Step 8: Document and communicate behavior changes.\n- Update docs to reflect the new config keys (`purge_keep_days`, `purge_interval`) and service usage (`recorder.purge` with `keep_days`).\n- Provide example YAML:\n  ```yaml\n  recorder:\n    purge_keep_days: 2\n    purge_interval: 2\n  ```\n- Show an example automation that calls `recorder.purge` with `keep_days` at a desired schedule.\n- Mark the change as breaking where appropriate and explain the impact: databases will no longer be auto-purged without the new config or service automation.",
            "Step 9: Generalize for future similar tasks.\n- For any maintenance feature (log rotation, cache cleaning, compaction):\n  - Prefer explicit services plus optional scheduling based on config,\n  - Validate input rigorously at the API boundary,\n  - Use worker threads/queues for heavy operations,\n  - Document services in a structured way for UI and automation.\n- This pattern helps avoid surprises from implicit timers and gives users fine-grained control over data lifecycle."
        ]
    }
}