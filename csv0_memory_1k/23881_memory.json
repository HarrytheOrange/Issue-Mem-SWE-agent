{
    "search_index": {
        "description_for_embedding": "Home Assistant history.get_states was slow when fetching the state of a single entity at a specific time because the query scanned the full states table and used the same join-heavy query as multi-entity lookups. The fix constrains the query to the current recorder run's time range (run.start to utc_point_in_time) and introduces a simplified, index-friendly query path for single-entity lookups without unnecessary joins.",
        "keywords": [
            "Home Assistant",
            "history",
            "recorder",
            "get_states",
            "performance",
            "slow query",
            "PostgreSQL",
            "MySQL",
            "time-series",
            "SQL optimization",
            "single entity history",
            "recorder run",
            "run.start boundary",
            "join elimination"
        ]
    },
    "agent_memory": {
        "episodic_memory": "In this incident, fetching a single entity's historical state at a specific point in time via Home Assistant's history.get_states was significantly slower than expected (e.g., ~4 seconds on PostgreSQL for a single entity). The cause was twofold: (1) the query did not restrict the search window to the active recorder run, so it could scan a much larger portion of the states table than necessary; and (2) the code used the same generic, join-heavy query path for both multi-entity and single-entity lookups, including a subquery (group by entity_id, select max_state_id) and a join back to the states table, even when only one entity was requested.\n\nThe recorder component tracks 'runs' (roughly between Home Assistant restarts), and for a query like 'state just before time T', only the recorder run active at T is relevant. The multi-entity query was already limited to that run, but the single-entity variant was not explicitly constrained. This missing bound made the database do more work and prevented optimal use of time-based indexes. \n\nThe fix introduced two main changes:\n1. Add a time boundary filter for the recorder run when looking up the most recent state before a point in time: `States.last_updated >= run.start` in addition to `States.last_updated < utc_point_in_time`. This bounds the search to the relevant recorder run, mirroring the behavior already used for multi-entity history queries.\n2. Implement a dedicated, simplified code path for the single-entity case. Instead of building a subquery of most_recent_state_ids and joining it back to States, the new code directly queries the States table:\n   - `query = session.query(States)`\n   - Filter by `States.last_updated >= run.start`, `States.last_updated < utc_point_in_time`, and `States.entity_id.in_(entity_ids)` (where len(entity_ids) == 1)\n   - Order by `States.last_updated.desc()`\n   - `limit(1)`\n   This removes the unnecessary join and group-by logic for the single-entity case and lets the database use indexes efficiently.\n\nThe behavior of the API remains correct: this code path is only used to get the single data point immediately preceding a time window (for example, to generate the 'fake 0' starting point on frontend history graphs). It does not truncate the actual range of history returned for broader time intervals. After the change, the developer observed a substantial speedup, from around 4 seconds to about 0.2 seconds for the single-entity lookup on PostgreSQL, without changing externally visible semantics.",
        "semantic_memory": "This fix illustrates several generalizable lessons about performance and correctness in database-backed time-series/history queries:\n\n1. **Narrow the time window using known boundaries**: If you know that the data of interest lies within a specific partition or run (e.g., a 'recorder run' between service restarts), explicitly constrain the query using that boundary (e.g., `timestamp >= run_start AND timestamp < query_time`). This reduces the search space and improves index utilization.\n\n2. **Specialize queries for simple cases**: A one-size-fits-all query pattern (subquery + join + group-by) may be necessary for multi-entity aggregation, but it's often overkill for single-entity requests. Providing a specialized, simpler query for the single-entity case (single table, primary or composite index, `ORDER BY ... LIMIT 1`) can dramatically improve performance while maintaining correctness.\n\n3. **Avoid unnecessary joins and grouping**: Joins and group-bys are expensive, especially on large time-series tables. When you only need 'the latest row for this single key before time T', a direct indexed scan of the base table with an `ORDER BY` and `LIMIT` is almost always faster than computing a grouped subquery and joining back.\n\n4. **Leverage domain knowledge about how data is stored**: The notion of 'recorder runs' encodes domain-specific lifecycle information (Hass restarts, database sessions) that can be turned into query constraints. Architectures that track such boundaries (runs, shards, partitions) should use them in queries to optimize performance.\n\n5. **Preserve semantics while optimizing**: When optimizing queries for performance, it is crucial to verify that the filtered ranges and specialized paths do not change logical behavior. Here, bounding by the recorder run and simplifying the single-entity query preserved the semantics (finding the latest state before T) and avoided truncating intended history ranges.\n\n6. **Frontend vs backend behavior understanding**: UI features (like showing a history graph that starts with a 'fake 0' baseline) may rely on specific backend patterns (single-point queries outside the main range). Understanding these interactions helps identify where performance matters and which code paths are actually hit in production (e.g., via API calls used by the frontend).\n\n7. **Use of database indices on time-series**: For time-series/history tables, a common access pattern is \"for entity X, get the most recent record before time T\". The optimal pattern usually is: `WHERE entity_id = X AND timestamp < T ORDER BY timestamp DESC LIMIT 1`, optionally additionally constrained by a broader known boundary (e.g. `timestamp >= run_start`).\n\nThese patterns apply broadly to any system that stores historical states or events grouped by entity/ID and queried over time windows.",
        "procedural_memory": [
            "When diagnosing and fixing slow history/time-series queries (especially for 'latest state before time T' lookups), follow these steps:",
            "Step 1: Identify the slow code path.\n- Instrument the application or add logging to see which function and query are used for the slow operation (e.g., Home Assistant's history.get_states when requesting a single entity at a point in time).\n- Use realistic data volumes (large DB, many history entries) to reproduce the slow behavior.",
            "Step 2: Inspect the generated SQL and execution plan.\n- Enable SQL logging or use ORM tools to print the exact SQL query.\n- Run `EXPLAIN` / `EXPLAIN ANALYZE` on the query in the DB (PostgreSQL, MySQL, etc.).\n- Look for full table scans, large index scans, expensive joins, and group-by operations that could be avoided.",
            "Step 3: Check for missing or overly broad time constraints.\n- For time-series tables, confirm that the query constrains the time column as narrowly as possible.\n- If you have domain concepts like 'runs', 'partitions', or 'shards', ensure queries use them: e.g., `timestamp >= run_start AND timestamp < query_time`, instead of only `timestamp < query_time`.\n- Add or tighten time bounds when you know the relevant time range by design.",
            "Step 4: Special-case simple query scenarios.\n- Identify whether the slow path is often called with a simple key set (e.g., a single entity ID).\n- Implement a dedicated, optimized query path for these cases.\n  - Example: For a single entity ID, instead of using a subquery and join, use `SELECT * FROM states WHERE entity_id = ? AND last_updated >= ? AND last_updated < ? ORDER BY last_updated DESC LIMIT 1`.\n- Maintain the generic path only for the complex scenario (e.g., multiple entities, grouping).",
            "Step 5: Remove unnecessary joins and subqueries.\n- If your current implementation does a subquery that finds max IDs per group and then joins back to the main table, see if this can be replaced by a simpler query in the single-key case.\n- Refactor so that multi-entity logic (group-by, joins) is only used when actually needed.",
            "Step 6: Re-run performance tests and regression tests.\n- Benchmark before and after changes with realistic DB sizes.\n- Confirm that response times improve (e.g., from several seconds to sub-second) for the targeted queries.\n- Run existing unit/integration tests to ensure logical behavior hasn't changed (correct states returned, no truncated history, edge cases around run boundaries still behave correctly).",
            "Step 7: Validate frontend behavior and API semantics.\n- For APIs used by frontends (like history graphs), verify that the UI still behaves correctly after the change (e.g., the 'fake 0' baseline point renders correctly and history isn't truncated).\n- If the optimization uses internal concepts (like recorder runs), document the assumptions for future maintainers.",
            "Step 8: Codify the pattern for future development.\n- Document the preferred query patterns for common history/time-series operations.\n- Encourage future code to use: bounded time ranges, specialized single-entity queries, and avoiding unnecessary joins/group-bys.\n- Consider adding helper functions or utilities that encapsulate these optimized patterns."
        ]
    }
}