{
    "search_index": {
        "description_for_embedding": "Introduces an asynchronous chunk loading system for single-scale napari image layers and a revamped performance monitoring (perfmon2) and Qt event tracing infrastructure. A global ChunkLoader manages async loading with a worker pool and LRU cache, while QtChunkReceiver safely routes loaded chunks back to layers on the GUI thread. ImageSlice and Image layers are refactored to support async loading with a loaded flag and integration into vispy visibility. New perfmon config, patcher, and Qt tracing utilities allow configurable function/event tracing, and tests are split into sync_only vs async_only modes.",
        "keywords": [
            "async loading",
            "chunk loader",
            "ChunkLoader",
            "ChunkCache",
            "LRUCache",
            "QtChunkReceiver",
            "image layer",
            "ImageSlice",
            "single-scale images",
            "worker pool",
            "ProcessPoolExecutor",
            "thread safety",
            "GUI thread",
            "Qt signals",
            "perfmon",
            "performance tracing",
            "QApplicationWithTracing",
            "Qt event tracing",
            "patch_callables",
            "pytest markers",
            "sync_only",
            "async_only",
            "napari"
        ]
    },
    "agent_memory": {
        "episodic_memory": "This change set introduced two major capabilities into the napari codebase: a new asynchronous chunk-loading mechanism for single-scale images, and a more powerful performance monitoring/tracing framework (perfmon2).\n\nOn the async side, the core motivation was to avoid blocking the GUI thread when loading large or remote image data (e.g. dask-backed arrays), while still keeping rendering deterministic and safe for Qt. Previously, image data access happened synchronously in the GUI thread, which could cause jank or freezes during expensive I/O or computation. The solution is a global ChunkLoader responsible for loading arbitrary array chunks either synchronously or asynchronously.\n\nChunkLoader:\n- Uses a ProcessPoolExecutor (configurable via NAPARI_CHUNK_WORKERS) to offload work into worker processes.\n- Wraps individual load operations in a ChunkRequest, which carries a (data_id, indices) key, the source array, and metadata like layer_id and timing info.\n- Maintains an LRU-based ChunkCache sized as a fraction of system RAM (via cachetools and psutil) keyed on (data_id, indices) so repeated views of the same slice can be served without recomputation.\n- Has a `synchronous` flag and a small SyncChunkLoader class: for tests or debugging, a purely synchronous loader can be used that bypasses thread pools and caches.\n- When async mode is enabled, `load_chunk` either returns immediately with a loaded request (if the data is in-memory or cached) or submits a worker job that calls `_chunk_loader_worker`, which materializes the array `np.asarray(request.array)` and returns the updated request.\n- Tracks in-flight futures per data_id and cancels obsolete ones when new requests come in (so only the latest slice requests remain queued), while acknowledging that jobs already running in the worker cannot be cancelled.\n\nBecause workers cannot safely call into Qt or layer code, a QtChunkReceiver object lives in the GUI thread:\n- ChunkLoader emits an `events.chunk_loaded(layer=..., request=...)` event when a worker finishes.\n- QtChunkReceiver listens to this event, then emits a `chunk_loaded_gui` Qt signal to itself, ensuring that the actual `layer.chunk_loaded(request)` call is performed from the GUI thread regardless of where the worker callback ran.\n- QtChunkReceiver is owned/constructed by QtLayerList and is closed when the viewer is closed to disconnect signals cleanly.\n\nThe image layer pipeline was reworked to use this chunk loader for single-scale images:\n- A new ImageSlice class encapsulates the per-view slice state. It holds two ImageView instances (for main image and thumbnail) plus an ImageProperties record (multiscale flag, RGB flag, ndim, and display order).\n- ImageSlice implements `load_chunk(request)` and `chunk_loaded(request)` to orchestrate async loading. It tracks `current_indices` so it can ignore stale chunk completions that don't match the current view, and a `loaded` flag so vispy can hide layers until their data is ready.\n- For non-multiscale images, `Image._load_single_scale` constructs a ChunkRequest via `CHUNK_LOADER.create_request`, calls `self._slice.load_chunk(request)`, and then emits `events.loaded()` regardless of sync/async outcome. If `CHUNK_LOADER.load_chunk` returns a satisfied request, it is applied immediately; otherwise it will arrive later through `Image.chunk_loaded`.\n- In `Image.chunk_loaded`, the layer checks `request.data_id` against `id(self.data)` to avoid applying chunks from stale data, forwards to `self._slice.chunk_loaded(request)`, emits `events.loaded()`, and then triggers a vispy data update via `events.set_data()`.\n- `Image._update_thumbnail` now returns early if `self._slice.loaded` is false, so thumbnails aren't generated from uninitialized data.\n- The layer’s `_get_empty_image` was simplified to return a minimal 1-pixel/voxel array appropriate to ndim and RGB/multiscale state, used for initial placeholders until real data arrives.\n- The Image layer exposes a `loaded` property delegating to `self._slice.loaded`.\n\nVispy integration was updated to honour the new loaded state:\n- Base layer events gained a `loaded` event in `Layer.events`.\n- In VispyBaseLayer, `_on_visible_change` and a new `_on_loaded_change` combine visibility with loaded state, setting `node.visible = layer.visible and layer.loaded`. That prevents vispy from rendering layers before their first slice is ready.\n\nTo support tracing and debugging, perfmon2 was added/refactored:\n- `napari.utils.perf` gained a config-driven initialization via a global `perf_config` (PerfmonConfig), controlled by `NAPARI_PERFMON`. It accepts either `NAPARI_PERFMON=1` for legacy Qt event tracing only, or the path to a JSON config file for richer behavior.\n- PerfmonConfig reads a JSON format that can specify `trace_qt_events` and named `callable_lists`. It uses a new `napari.utils.patcher` helper to dynamically patch functions/methods listed in the config, wrapping them with `perf_timer` using wrapt.patch_function_wrapper.\n- `perf_timer` and `perf_func` now generate PerfEvent objects that include args, pid, tid, event type (\"X\" for spans and \"I\" for instants), and optional printed durations. `add_instant_event` provides a small helper to add instant events to the trace for important moments (like chunk_loaded_worker/gui).\n- PerfTraceFile was changed to buffer events in memory and flush them on close, so that trace I/O does not distort timing, and it now supports instant events in the chrome://tracing compatible JSON format.\n- Qt-specific tracing: `qt_event_timing.py` was renamed to `qt_event_tracing.py` and now defines QApplicationWithTracing and convert_app_for_tracing. These wrap QApplication.notify in a perf_timer based on `_get_event_label(receiver,event)` for better visualization of Qt event structure.\n- Viewer and event loop creation were updated to use PerfmonConfig: `event_loop._create_application` now creates a QApplicationWithTracing if `perf_config.trace_qt_events` is true, and the Viewer class uses `_convert_app_for_tracing` plus `perf_config.patch_callables()` when perfmon is active.\n- The Qt debug menu gains the ability to automatically start trace recording if `NAPARI_TRACE_FILE` is set, and creates traces via `perf.timers.start_trace_file` and `stop_trace_file`.\n\nTests and configuration were adapted for async behavior:\n- A new pytest.ini defines a `sync_only` marker for tests that must be run with synchronous loading.\n- Cirrus CI configuration was updated: Linux matrix includes an environment variable `PYTEST_FLAGS` with an `--async_only` variant, and tests are invoked as `pytest napari -v ... $PYTEST_FLAGS` to toggle modes.\n- In conftest.py, a session-scoped fixture `configure_loading` uses a new context manager `synchronous_loading(True/False)` around the global CHUNK_LOADER to enable sync loading by default and async loading only when `--async_only` is set. Another autouse fixture `skip_sync_only` skips `@pytest.mark.sync_only` tests when running in async mode.\n- Many tests that depend on synchronous model updates, immediate data availability, or are currently incompatible with async are marked `@pytest.mark.sync_only` (e.g. various value tests, multichannel tests, label mouse interactions, viewer update tests, and some screenshot tests).\n\nFinally, minor perf/structure tweaks were added: splitting small lambdas into named methods (e.g. slider valueChanged callback) to enable targeted perf instrumentation, adding a helper `_data_astype` in the vispy image layer for perfmon, and temporarily simplifying `calc_data_range` to return [0, 1] for speed in async mode.\n\nOverall, the system shifted from a strictly synchronous rendering path to one that can asynchronously load image chunks while preserving GUI thread safety and offering richer performance tracing, with careful test gating to separate sync-only behavior from experimental async paths.",
        "semantic_memory": "This work highlights a set of general patterns and best practices for introducing asynchronous data loading and performance tracing into an existing GUI-heavy scientific application.\n\nKey generalizable concepts:\n\n1. Centralized chunk loading and caching\n- Introduce a single global or shared chunk loading service (e.g. ChunkLoader) rather than per-viewer loaders when you want to share resources like thread pools and caches. This helps control memory usage (by sizing a single cache as a fraction of RAM) and avoids over-provisioning worker threads.\n- Use a cache keyed by an immutable representation of the underlying data identity (e.g. id(data) plus a normalized representation of indices) so that identical slices can benefit from cached results, regardless of where the request originated.\n- Provide both synchronous and asynchronous modes, especially in early stages or test environments, to simplify debugging and to preserve deterministic behavior where needed.\n\n2. GUI-thread safety for async results\n- Worker threads or processes should not directly interact with GUI objects or model state that assumes single-threaded access. Instead, use an event bridge or dispatcher (e.g. QtChunkReceiver) that receives worker completion events and rebroadcasts them as GUI-thread-safe signals.\n- Maintain minimal data in the messages passed between worker and GUI threads, such as plain arrays and scalar metadata. Keep non-picklable references (like GUI objects) out of worker arguments when using multi-processing.\n- Use identifiers (e.g. layer_id, data_id, indices) to map results back to their owners, and resolve them via weakrefs in the main process to avoid preventing garbage collection.\n\n3. Handling stale async results\n- When the user rapidly interacts with the UI (e.g., sliding through a stack), old async requests may complete after the user has moved on. Introduce sequence or index checks such as `current_indices` or a generation counter to detect stale results and ignore them.\n- Optionally cancel queued but not-yet-started futures when new requests come in to reduce wasted work. Accept that in-flight jobs may not be cancellable with standard futures and design the logic to safely ignore late results instead.\n\n4. Layer/model state for asynchronous readiness\n- Add an explicit `loaded` or `ready` state to visual elements or layers. This state can be bound to rendering logic (e.g., node.visible = visible && loaded) so that incomplete or uninitialized layers don’t flicker or show incorrect content.\n- Use events like `events.loaded()` to notify views and other parts of the system whenever the layer’s data becomes ready. This decouples the data loading completion from rendering updates.\n\n5. Minimal placeholder data\n- When initializing views that expect array-shaped data (e.g., image textures), use minimal placeholder arrays (e.g., a single pixel or voxel) with correct shape rather than full-size zero arrays. This reduces unnecessary memory usage and makes initial state well-defined even before real data arrives.\n\n6. Configurable performance tracing\n- Implement a performance monitoring system that can be enabled or disabled via environment variables to avoid overhead in normal runs. A simple flag (NAPARI_PERFMON) can control whether timers and tracing are active.\n- Support configuration via JSON files that specify which callables to trace, which categories to use, and whether to trace Qt events. Use dynamic patching (via importlib and wrapt) to insert timers around selected functions and methods based on string paths like \"module.Class.method\".\n- Emit performance events in a standard format (e.g., chrome://tracing JSON with completed and instant events) for easy analysis of call hierarchy and timing across threads.\n\n7. Testing strategies for async features\n- Distinguish tests that require synchronous behavior (e.g., direct assertions on immediate values after state changes) from those that are asynchronous-friendly. Use custom pytest markers (e.g., `sync_only`) and command-line flags (e.g., `--async_only`) to split test suites accordingly.\n- Provide a global toggle (e.g., a context manager around your chunk loader or scheduler) that controls sync vs async mode for an entire test session, ensuring consistent assumptions within each run.\n\n8. Modularization for instrumentation\n- When you know you want to instrument certain actions (like slider movements or data casting), refactor lambda callbacks or inline expressions into named methods, so they can easily be referenced in a tracing config and patched individually.\n- Encapsulate repeated patterns (e.g., array conversion `np.asarray` calls) into small helper methods to allow targeted timing or behavior tweaks without cluttering core logic.\n\nThese patterns apply broadly to scientific visualization tools, interactive data viewers, and any application where expensive data access must be decoupled from a responsive GUI, and where runtime performance insights are valuable for both developers and users.",
        "procedural_memory": [
            "Step-by-step instructions on how to diagnose and fix similar issues.",
            "Step 1: Identify synchronous bottlenecks in the GUI path.\n- Instrument or log key code paths where the GUI feels sluggish (e.g., slider-driven slicing, zooming, data refresh). Use temporary timers or profiling tools to locate operations that are doing heavy I/O or computation in the GUI thread (e.g., np.asarray on dask arrays, disk or network reads).\n- Confirm that these operations are indeed blocking the event loop by comparing UI responsiveness when they’re skipped or mocked.",
            "Step 2: Design a chunk abstraction for data access.\n- Define a request object (e.g., ChunkRequest) that encapsulates everything needed to load a piece of data: a stable data identity (data_id), indices/slices, and the source array or loader object.\n- Ensure the request can be serialized or pickled if you plan to use multi-processing. Avoid storing GUI or complex object references directly in the request; instead, store lightweight identifiers (ids, keys) and reconstruct references on the main side using mappings and weakrefs.",
            "Step 3: Implement a global or shared chunk loader.\n- Create a ChunkLoader service that exposes `load_chunk(request)` and optionally `create_request(layer, indices, array)` methods.\n- Configure it with a worker pool (ThreadPoolExecutor or ProcessPoolExecutor) for async mode, and a simple direct path for sync mode (for tests and debugging).\n- Add a parameter or environment-controlled flag (e.g., synchronous or NAPARI_CHUNK_ASYNC) so you can run in both modes without code changes elsewhere.",
            "Step 4: Add a cache layer to avoid recomputation.\n- Implement an LRU (or appropriate) cache keyed by (data_id, normalized indices) where the value is the realized array data.\n- Use a getsizeof function that returns array.nbytes for memory-based eviction policies, and size the cache as a fraction of system RAM.\n- Consult the cache before submitting a worker job; if a hit is found, return that immediately and skip async work.",
            "Step 5: Integrate async completion with the GUI thread.\n- Make your chunk loader emit a generic completion event (e.g., using an event/emitter system) that includes the target layer id and request.\n- Implement a GUI-thread receiver (e.g., QtChunkReceiver) that subscribes to the loader’s completion events and re-emits them as GUI-thread-safe signals or callbacks.\n- In the receiver, resolve the target object (e.g., layer) from a mapping of ids to weakrefs, and then call a method like `layer.chunk_loaded(request)` on the GUI thread.",
            "Step 6: Guard against stale results.\n- In your per-view objects (e.g., ImageSlice), track the latest requested indices or a sequence counter.\n- When processing a completed request, verify that it matches the currently desired view (e.g., `if current_indices != request.indices: ignore`). This prevents older async results from overwriting newer views when the user interacts quickly.\n- Optionally: keep track of futures per data_id and cancel any pending ones when a new request comes in, recognizing that in-flight jobs may not be cancelable and must be filtered by the stale check.",
            "Step 7: Refactor layers/views to use the chunk loader.\n- Wrap your renderable data into a slice object (e.g., ImageSlice) that maintains both the raw data, the view data, and any related properties (RGB flag, multiscale flag, display order).\n- Replace direct synchronous array access (`array[indices]`) in the layer with a call to `CHUNK_LOADER.create_request` followed by `slice.load_chunk(request)`, and update a `loaded` flag appropriately.\n- Implement `chunk_loaded(request)` in the layer to forward to the slice, emit events like `loaded` and `set_data`, and trigger view updates.",
            "Step 8: Wire `loaded` state into rendering.\n- Add a `loaded` boolean property and corresponding events to your layer/model classes.\n- In the rendering backend (e.g., vispy, OpenGL), include `layer.loaded` when setting visibility. For example, `node.visible = layer.visible and layer.loaded` ensures nothing is drawn while data is uninitialized.\n- In ancillary logic like thumbnail generation, early-return if `loaded` is false to avoid using placeholder data.",
            "Step 9: Introduce structured performance tracing.\n- Implement a perf_timer context manager that records start/end times with high-resolution clocks (perf_counter_ns), along with optional categories and metadata.\n- Create a PerfEvent type that includes timing, category, pid/tid, and an event type (span vs instant), and a PerfTraceFile writer that produces chrome://tracing-compatible JSON.\n- Provide decorators (perf_func, perf_func_named) for easily timing functions, and a global timers object for aggregating stats and writing traces.\n- Add a configuration layer (e.g., PerfmonConfig) which can read JSON specifying which callables to trace and whether to trace UI events. Use a dynamic patcher (via importlib and wrapt.patch_function_wrapper) to auto-wrap target callables at startup.",
            "Step 10: Make Qt event loop instrumentation optional.\n- Implement a special QApplication subclass that overrides notify and wraps event dispatch in perf_timer with descriptive labels.\n- Provide helper functions (convert_app_for_tracing) to replace the existing QApplication at appropriate times (before other widgets are created), being careful to delete the old app only before any widgets exist to avoid crashes.\n- Gate this behavior behind config flags (e.g., NAPARI_PERFMON, trace_qt_events) so normal runs are unaffected.",
            "Step 11: Adapt tests for async vs sync modes.\n- Define pytest markers such as `sync_only` for tests that rely on synchronous behavior.\n- Add a command-line flag (e.g., `--async_only`) and session-scoped fixture that toggles global async/sync loading for the duration of the test suite by adjusting the chunk loader’s synchronous flag.\n- Implement an autouse fixture that skips sync_only tests when running in async mode.\n- Update CI configuration to run at least one job in sync mode and one in async mode using these flags, ensuring coverage of both behaviors.",
            "Step 12: Phase rollout and monitoring.\n- Initially keep async loading off by default and gate it behind environment variables or config flags, enabling it only for specific environments or exploratory sessions.\n- Use the performance tracing and logging infrastructure to monitor how async loading affects latency, resource usage, and user experience.\n- Gradually refine caching policies, worker counts, and stale-request handling based on observed behavior before making async the default."
        ]
    }
}