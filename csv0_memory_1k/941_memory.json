{
    "search_index": {
        "description_for_embedding": "Added CSV export support for napari points and shapes layers by introducing a generic `to_table` API that returns tabular coordinate data and optionally writes CSV files via a shared `write_csv` utility. Supports exporting all or only selected points/shapes and is designed to interoperate cleanly with pandas.",
        "keywords": [
            "napari",
            "points layer",
            "shapes layer",
            "CSV export",
            "to_table",
            "write_csv",
            "selected_only",
            "pandas interoperability",
            "file I/O separation",
            "layer data export"
        ]
    },
    "agent_memory": {
        "episodic_memory": "In this change set, the goal was to provide a simple way for users, especially non-programmers, to export point and shape coordinates from napari to CSV for downstream analysis (e.g. Excel, pandas). Initially, the implementation wrote CSVs directly in the layer methods (`to_csv`), but maintainers preferred to keep explicit file I/O out of the model layer. The solution was to introduce a `to_table` API on both the Points and Shapes layers, and a shared `write_csv` helper in `napari.utils.io`.\n\nFor the Points layer, a new method `Points.to_table(path=None, *, selected_only=False)` was added. It builds a table (list of lists) of the point coordinates and a list of column names, with one column per dimension (`dim_0`, `dim_1`, ...). The code was adjusted so that no extra index/ID column is written by default; the CSV header is just `dim_0,dim_1,...` and each row is a coordinate. When `selected_only=True` is passed, only the currently selected points are exported; if no points are selected, a `ValueError` is raised. If a `path` is provided, `write_csv(path, table, column_names)` is called to actually save the CSV. Tests were updated to assert the CSV content matches expectations and that an error is raised when `selected_only=True` but nothing is selected. Another test verifies that the CSV produced by `to_table` round-trips through pandas (`DataFrame` + `to_csv`) identically.\n\nFor the Shapes layer, a similar `Shapes.to_table(path=None, *, selected_only=False)` method was implemented. It flattens the nested shapes coordinate data into a tidy table with columns: `shape_id`, `shape_type`, `coord_id`, and one column per dimension (`dim_0`, `dim_1`, ...). Each shape's coordinates are enumerated, and `shape_id` is the index within the layer's `data`. When `selected_only=True`, the method builds this table using only the selected shapes (both geometry and type are subset by the selection indices). If no shapes are selected when `selected_only=True`, it raises a `ValueError`. This method also uses `write_csv` for file output when `path` is provided. Tests verify CSV structure, contents for multiple shapes and types, correctness when exporting only selected shapes, proper error handling when no shapes are selected, and equality of CSVs produced via `to_table` versus pandas.\n\nA new utility `napari.utils.io.write_csv(path, data, column_names=None)` was introduced to centralize CSV writing with Python's `csv` module, using `QUOTE_MINIMAL` and optional header row. Tests ensure that this function produces CSV with the expected header and numeric values.\n\nFinally, an example script `examples/export_to_csv.py` demonstrates how to use `viewer.add_points` and `viewer.add_shapes`, call `to_table` with and without `selected_only`, and convert the resulting tables into pandas DataFrames. The PR explicitly avoids adding pandas as a core dependency; pandas is only used in tests and examples to demonstrate interoperability. The PR was later superseded by another (#1193), but this work establishes the pattern for clean table/CSV export from layers.",
        "semantic_memory": "This work illustrates several generalizable design and implementation patterns around exporting structured data from model objects:\n\n1. **Separate data preparation from file I/O**: Instead of mixing CSV writing directly into model objects, provide methods that prepare a generic tabular representation (e.g. `to_table`) and then delegate actual file writing to a utility function. This decouples data modeling from persistence concerns, makes it easier to test, and allows flexible reuse (e.g. compose multiple tables, or send them to pandas rather than disk).\n\n2. **Use simple, well-defined table schemas for geometric data**: For layered geometric structures (points, shapes with multiple vertices), a flattened, tidy table format is effective: one row per coordinate, with columns for entity identifiers and attributes (e.g. `shape_id`, `shape_type`, `coord_id`, `dim_0`, `dim_1`, ...). This makes it easy to consume in tools like Excel and pandas, and avoids overcomplicated nested formats when only coordinates are needed.\n\n3. **Design for minimal, user-friendly exports by default**: Non-programmer users often want clean CSVs with just the essential data. Adding round-trip metadata (e.g. colors, styling, internal IDs) can make files unwieldy. A good pattern is: export minimal data by default; consider richer, round-trip capable formats (or optional parameters) in separate APIs or plugins.\n\n4. **Support selection-aware export**: Layers frequently have a notion of selection. Providing a `selected_only` flag (or similar) in export methods lets users extract just the subset of interest without extra pre/post-processing. This is especially important when attributes are encoded in a single layer via properties or styling.\n\n5. **Fail early when preconditions are not met**: When an option like `selected_only` is set but no data is selected, raising a clear `ValueError` is better than silently writing an empty file or ignoring the flag. This gives immediate feedback and avoids subtle downstream errors.\n\n6. **Align programmatic exports with ecosystem tools (pandas)**: When designing tabular exports, ensure that the table format and CSV files are trivially consumable by standard tools (pandas, spreadsheets). Tests here explicitly verify that the CSV produced by `to_table` matches pandas' `DataFrame.to_csv` output, which helps guarantee interoperability and a smooth user experience.\n\n7. **Centralize common IO utilities**: A generic `write_csv` utility consolidates CSV-writing behavior (delimiter, quoting, headers). This avoids duplicated logic across layers, enforces consistent CSV formatting, and simplifies tests and future refactors.\n\nThese principles apply broadly whenever adding export capabilities: prepare a clean, well-defined tabular or structured representation in the model, separate it from persistence, and design the default outputs around user needs and ecosystem interoperability.",
        "procedural_memory": [
            "Step-by-step instructions on how to diagnose and fix similar issues.",
            "Step 1: Clarify export requirements and users\n- Identify what data users actually want in the exported file (e.g. just coordinates vs. full styling/metadata).\n- Decide whether round-trip fidelity or human-readability is the primary goal for the specific export feature.\n- Determine if exporting subsets (e.g. selected elements) is required.",
            "Step 2: Design a tabular schema for the data\n- For simple point data, choose columns like `dim_0`, `dim_1`, ..., one row per point.\n- For nested or structured data (e.g. shapes with multiple vertices), flatten it into a tidy table:\n  - Include identifier columns (`shape_id`, `coord_id`, etc.).\n  - Include type/attribute columns (`shape_type`, properties) as needed.\n- Avoid extra index columns unless they are semantically meaningful to the user.",
            "Step 3: Implement a `to_table`-style method on the model\n- Add a method like `to_table(path=None, *, selected_only=False)` to the model or layer class.\n- Inside, handle selection:\n  - If `selected_only` is False, operate on the full dataset.\n  - If `selected_only` is True and there is a selection, subset the core data structure and any associated metadata (e.g. types).\n  - If `selected_only` is True and nothing is selected, raise a clear `ValueError` explaining the problem.\n- Build the table as a list of rows; compute `column_names` independently.\n- Return `(table, column_names)` regardless of whether a file is written.",
            "Step 4: Centralize CSV writing in a utility\n- Create a shared function (e.g. `write_csv(path, data, column_names=None)`) in an IO/utility module.\n- Use Python's `csv` module to handle quoting and delimiters correctly.\n- If `column_names` is provided, write them as the header row.\n- Iterate over `data` (list of lists or arrays) and write each row.\n- Call this helper from `to_table` when a `path` argument is provided.",
            "Step 5: Add tests for export behavior\n- Test that `to_table(path)` actually creates the file at the given path.\n- Open the generated CSV and inspect:\n  - The header line matches the expected columns.\n  - Each subsequent row matches the expected numeric or string values.\n- Add tests for `selected_only=True`:\n  - With a known subset selected, verify the exported rows match that subset.\n  - With no selection, verify that a `ValueError` (or chosen exception) is raised and no file is created.",
            "Step 6: Verify interoperability with pandas or other tools\n- In tests, construct a pandas `DataFrame` from the `(table, column_names)` returned by `to_table`.\n- Use `DataFrame.to_csv` with appropriate options (e.g. `index=None`).\n- Read both CSVs (direct export and pandas export) and assert their content is identical or that the DataFrames are equal.\n- This ensures downstream data consumers will see consistent, predictable output.",
            "Step 7: Provide usage examples\n- Add example scripts or documentation showing:\n  - How to call `to_table` with and without `selected_only`.\n  - How to convert the `(table, column_names)` into a pandas `DataFrame` for further analysis.\n  - How to save DataFrames back to CSV without extra index columns (e.g. `index=None`).\n- This reduces user confusion and demonstrates the intended workflow.",
            "Step 8: Keep model and IO concerns decoupled for future extensibility\n- Avoid adding heavy dependencies (e.g., pandas) to core model code; keep them in examples, plugins, or optional layers when possible.\n- Keep layer methods focused on data preparation, not on file formats; consider separate writer plugins for more complex or round-trip formats (e.g. GeoJSON, ROI formats).\n- This makes it easier to add new formats or change IO behavior without touching the core models."
        ]
    }
}