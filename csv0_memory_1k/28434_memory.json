{
    "search_index": {
        "description_for_embedding": "Introduced a generic speech-to-text (STT) component in Home Assistant with an HTTP streaming API, provider abstraction, metadata validation via a custom X-Speech-Content header, and a demo provider. Fixed a minor bug in a helper method name for extracting metadata from headers.",
        "keywords": [
            "speech to text",
            "STT",
            "Home Assistant",
            "demo integration",
            "HTTP API",
            "streaming audio",
            "aiohttp StreamReader",
            "X-Speech-Content header",
            "metadata validation",
            "HTTP 400",
            "HTTP 404",
            "HTTP 415",
            "Provider abstraction",
            "enum for audio formats",
            "bugfix method rename",
            "stt/__init__.py",
            "stt/const.py"
        ]
    },
    "agent_memory": {
        "episodic_memory": "This pull request implemented a new speech-to-text (STT) framework in Home Assistant and fixed a small naming bug discovered during review. Previously, Home Assistant only had TTS (text-to-speech) support and lacked a generic way to accept audio input and turn it into text. The new `stt` component defines a provider abstraction, an HTTP API, and a demo provider implementation.\n\nThe core of the feature is in `homeassistant/components/stt/__init__.py`. It introduces a `Provider` abstract base class with methods and properties that STT integrations must implement: `supported_languages`, `supported_formats`, `supported_codecs`, `supported_bitrates`, `supported_samplerates`, and `async_process_audio_stream`. The module uses aiohttp's `HomeAssistantView` to expose an HTTP endpoint at `/api/stt/{provider}` that supports both GET and POST.\n\nOn GET, the view returns the provider's capabilities as JSON: supported languages, formats, codecs, samplerates, and bitrates. On POST, the client must stream audio and provide a custom `X-Speech-Content` header containing metadata like `format`, `codec`, `samplerate`, `bitrate`, and `language` (for example: `X-Speech-Content: format=wav; codec=pcm; samplerate=16000; bitrate=16; language=de`). The helper method `_metadata_from_header` parses this header into a `SpeechMetadata` dataclass (using `attr.s`), which includes typed fields tied to enums defined in `stt/const.py` (`AudioFormats`, `AudioCodecs`, `AudioBitrates`, `AudioSamplerates`). The provider's `check_metadata` method verifies that the requested configuration is supported, and if not, the view returns an HTTP 415 Unsupported Media Type. Missing or malformed metadata results in HTTP 400 Bad Request, and unknown providers result in HTTP 404 Not Found.\n\nThe `homeassistant/components/demo/stt.py` file adds a demo STT provider (`DemoProvider`) that supports languages `en` and `de`, WAV/PCM audio, 16-bit bitrate, and samplerates 16000 and 44100. The `async_process_audio_stream` implementation simply consumes the entire incoming stream and returns a static transcription \"Turn the Kitchen Lights on\" with result state \"success\". This serves as a reference implementation and for tests.\n\nThe PR also registers the new STT component in `CODEOWNERS`, adds a manifest and empty `services.yaml`, and enables the demo STT platform via the demo integration's `__init__.py`. Tests (`tests/components/demo/test_stt.py` and `tests/components/stt/test_init.py`) validate correct setup, the GET capabilities endpoint, error status codes for missing/wrong metadata and unknown providers, and the happy-path POST behavior.\n\nDuring code review, a minor bug/typo was found: the helper method for parsing metadata was named `_metadat_from_header`. This was corrected to `_metadata_from_header`, and the reference in `post` was updated accordingly in a follow-up patch. This fix addressed code readability and consistency but did not change behavior.",
        "semantic_memory": "This change illustrates several generalizable patterns and best practices for implementing streaming speech-to-text or similar provider-based APIs in a web-connected system:\n\n1. **Provider Abstraction Layer**: Define an abstract base class for providers with clear capabilities and a single entrypoint for the main operation (`async_process_audio_stream`). This makes it easy to add new STT backends (cloud services, local engines, etc.) by implementing the interface without changing the core HTTP handling logic.\n\n2. **Explicit Capability Discovery**: Provide a read-only endpoint (GET) that exposes provider capabilities (supported languages, formats, sample rates, bitrates, codecs). This allows clients to adapt recording and encoding settings dynamically rather than hard-coding assumptions about what the backend supports.\n\n3. **Metadata via Custom Header**: When streaming binary data (audio), carry configuration metadata (format, codec, bitrate, sample rate, language) in a separate, easy-to-parse channel—in this case, a custom HTTP header (`X-Speech-Content`). This keeps the body purely stream-oriented and simplifies parsing and validation. It also decouples transport from metadata, a pattern that can apply to video, sensor streams, etc.\n\n4. **Enum-based Type Safety and JSON Compatibility**: Represent audio formats, codecs, bitrates, and sample rates using Enum classes inheriting from `str` or `int`. This provides type safety and constrained values in Python while remaining trivially serializable to JSON and comparable to simple scalar values. Using `attr` dataclasses with converters (e.g., `bitrate: AudioBitrates = attr.ib(converter=int)`) allows robust parsing from string-based metadata into typed fields and ensures that comparisons against Enum values behave intuitively.\n\n5. **Robust HTTP Error Handling**: The view explicitly handles different failure modes with appropriate HTTP status codes: 404 for unknown provider, 400 for missing or malformed metadata, and 415 for unsupported media characteristics. This consistent mapping of server-side validation states to HTTP codes is a useful design for any content-processing API.\n\n6. **Streaming Processing Pattern**: The STT provider consumes the request body as an `aiohttp.StreamReader`, iterating with `iter_chunked` to process streaming audio in chunks. Even in the demo, the pattern shows how to build a streaming pipeline rather than buffering entire content up front—important for large or live audio streams.\n\n7. **Test-Driven Endpoint Design**: The tests clearly document expected behaviors for both the GET capabilities endpoint and the POST transcription endpoint, including scenarios for successful processing, unsupported metadata, missing metadata, and unknown providers. This helps lock in the API contract and makes refactors safer.\n\n8. **Attention to Naming and Readability**: The small review-driven fix (renaming `_metadat_from_header` to `_metadata_from_header`) emphasizes the importance of consistent, correct naming. Accurate names improve maintainability and reduce cognitive friction, especially in shared code where others depend on clear semantics.",
        "procedural_memory": [
            "How to diagnose and implement a streaming speech-to-text provider using an HTTP API in a provider-based architecture:",
            "Step 1: Define a provider interface.\n- Create an abstract base class (e.g., `Provider`) that declares the required properties and methods: capability lists (supported languages, formats, codecs, bitrates, sample rates) and a core processing coroutine (e.g., `async_process_audio_stream(metadata, stream)`).\n- Use abstract properties and methods (`@abstractmethod`) to enforce implementation in concrete providers.",
            "Step 2: Define metadata and result models.\n- Create data classes (e.g., using `attr.s`) for request metadata (`SpeechMetadata`) and processing result (`SpeechResult`).\n- Use enums to constrain valid values: `AudioFormats`, `AudioCodecs`, `AudioBitrates`, `AudioSamplerates`, and `SpeechResultState`.\n- Make enums inherit from `str` or `int` so they serialize cleanly to JSON and compare well to primitive values.\n- Use converters (e.g., `converter=int`) in the dataclasses if header values arrive as strings but you want numeric enums.",
            "Step 3: Expose HTTP endpoints.\n- Implement a web view (e.g., subclass `HomeAssistantView` or your framework equivalent) with a URL pattern like `/api/stt/{provider}`.\n- On GET, retrieve the provider instance from a registry and return its capabilities as JSON: languages, formats, codecs, sample rates, and bitrates.\n- On POST, read the metadata from headers and stream the body to the provider for processing. Wrap the result into JSON and respond to the client.",
            "Step 4: Implement metadata parsing and validation.\n- Choose a clear, documented header format for metadata (here: `X-Speech-Content: format=wav; codec=pcm; samplerate=16000; bitrate=16; language=en`).\n- Implement a helper function like `_metadata_from_header(request)` that:\n  - Reads the header (logging a warning and returning `None` if it's missing).\n  - Splits the value on `;`, trims whitespace, and splits each key-value pair at `=`.\n  - Builds a dict and constructs a `SpeechMetadata` object from it, catching `TypeError` to handle missing/extra keys.\n- In the POST handler:\n  - If metadata parsing fails (returns `None`), return HTTP 400.\n  - Call `provider.check_metadata(metadata)` to ensure the requested configuration is supported; if false, return HTTP 415.",
            "Step 5: Implement the provider logic.\n- Create a concrete provider class that inherits from `Provider`.\n- Implement `supported_languages`, `supported_formats`, `supported_codecs`, `supported_bitrates`, and `supported_samplerates` to return lists of allowed enums.\n- Implement `async_process_audio_stream(metadata, stream)`:\n  - Consume the stream with something like `async for chunk in stream.iter_chunked(4096): ...`.\n  - Optionally stream chunks to an external STT service or process locally.\n  - Return a `SpeechResult` with the transcription text and a `SpeechResultState` (e.g., `SUCCESS` or `ERROR`).",
            "Step 6: Register providers and set up the component.\n- In your component setup function (e.g., `async_setup`), use a helper like `config_per_platform` (or equivalent) to iterate over configured platforms.\n- For each platform, import it dynamically (e.g., `async_prepare_setup_platform` in Home Assistant) and call an `async_get_engine` factory to obtain a provider instance.\n- Attach framework-specific context (e.g., `provider.hass = hass` and `provider.name = platform_name`) and store providers in a dictionary keyed by name.\n- Instantiate your HTTP view with this providers dict and register it on the HTTP server.",
            "Step 7: Add tests for behavior and errors.\n- Test GET `/api/stt/{provider}` returns 200 with expected JSON structure and values.\n- Test POST without metadata header returns HTTP 400.\n- Test POST with unsupported sample rate/bitrate/codec/format returns HTTP 415.\n- Test POST with valid metadata returns HTTP 200 and the expected `text` and `result` fields.\n- Test both GET and POST to an unknown provider return HTTP 404.\n- Use these tests as executable documentation and to prevent regressions.",
            "Step 8: Debug common issues when integrating clients.\n- If the client receives HTTP 400, verify that the `X-Speech-Content` header is present, correctly formatted (`key=value` pairs separated by `;`), and that keys match the fields of `SpeechMetadata`.\n- If the client receives HTTP 415, check that the requested `format`, `codec`, `bitrate`, and `samplerate` values are actually listed in the provider's supported capabilities.\n- If receiving HTTP 404, ensure the URL provider path segment exactly matches the provider name registered on the server.",
            "Step 9: Maintain code quality and readability.\n- Use descriptive, correctly spelled function and variable names (e.g., `_metadata_from_header` rather than `_metadat_from_header`).\n- Address review comments that improve naming conventions or style, even if they don’t change functionality. This makes the API easier for future developers and tools to understand.",
            "Step 10: Extend the pattern to other streaming domains.\n- The same architecture—provider abstraction, metadata header, capability discovery endpoint, streaming request body—can be applied to other domains such as video processing, audio effects, or generic binary sensor streams.\n- When designing such APIs, always separate control/metadata from the streaming content and make capabilities discoverable for better client adaptability."
        ]
    }
}