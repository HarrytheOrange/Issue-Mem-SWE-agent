{
    "search_index": {
        "description_for_embedding": "Experimental Optuna PR adding a MultiMetricStudy helper to support objective functions that return multiple metrics (as a dict) instead of a single float. The implementation sequentially optimizes each metric by reusing prior trials, storing all metric values in trial.user_attrs, and reconstructing single-metric studies for each metric. The PR was later closed by the author due to unclear motivation/requirements.",
        "keywords": [
            "optuna",
            "multiple metrics",
            "multi-objective-like optimization",
            "MultiMetricStudy",
            "create_multi_metric_study",
            "trial.user_attrs",
            "study._append_trial",
            "objective returns dict",
            "API design",
            "abandoned feature"
        ]
    },
    "agent_memory": {
        "episodic_memory": "In this PR for Optuna, the author attempted to add basic support for optimizing multiple metrics within a single objective function. The new API allowed objective functions to return a dict of metrics (e.g., {'loss': loss_value, 'accuracy': acc_value}) instead of a single float. To support this, the patch introduced a new callable type ObjectiveFuncType2 and a MultiMetricStudy class, as well as a create_multi_metric_study factory exported from optuna.__init__.\n\nThe MultiMetricStudy.optimize method expected a function that returned a dictionary of metric names to floats, plus two integers: n_first_trials and n_next_trials. Internally it defined a wrapper new_func(trial, index) that invoked the user's function, stored the resulting metrics dict in trial.user_attrs['metrics'], and then selected a single metric value by index to be treated as the objective for that optimization run. The method then:\n\n1. Created an initial Study via create_study and optimized only the first metric (index 0) for n_first_trials.\n2. After these first trials, it discovered all metric names from the metrics stored in trial.user_attrs.\n3. For each remaining metric (index 1..n_metrics-1), it created a new Study, copied all existing trials from the previous study into the new one via the internal study._append_trial API, setting the single objective value to the desired metric (trial.user_attrs['metrics'][new_metric]). Then it continued optimization for that metric with n_next_trials using the same wrapper new_func with the corresponding metric index.\n4. The last created Study instance was held in self.last_study.\n\nThe MultiMetricStudy.get_study(metric) method created yet another new Study and replayed all trials from self.last_study, mapping each trial's single scalar value to the requested metric from trial.user_attrs['metrics'][metric]. This produced a normal Study per metric, from which callers could query best_trial, etc. The provided sample demonstrated using Keras to train an MNIST classifier, returning both 'loss' and 'accuracy' as metrics, then printing best trials for each via mmstudy.get_study('loss') and mmstudy.get_study('accuracy').\n\nHowever, the author ultimately closed the PR themselves, stating they did not fully understand the motivation or description in the linked issue (#745). As a result, the new MultiMetricStudy API and related changes were never merged. This incident illustrates an experimental implementation of multi-metric optimization based on sequential scalar optimization and trial attribute reuse, and the importance of clear problem motivation before finalizing an API.",
        "semantic_memory": "This PR illustrates a pattern for retrofitting multi-metric (or quasi multi-objective) optimization onto a framework originally designed for single scalar objectives. Instead of deeply changing the core optimization engine, the author:\n\n- Wrapped the original objective function to accept a trial object and return a dictionary of metric_name -> float.\n- Stored the full metrics dict in trial.user_attrs, preserving all metrics for each trial without changing the core Trial schema.\n- Defined a helper (MultiMetricStudy) that sequentially optimized each metric by treating one metric at a time as the scalar objective. For each metric, they created a new Study and reconstructed trials from prior studies using the internal _append_trial method, mapping the objective value to the metric of interest while reusing params, distributions, and attrs.\n- Provided a convenience API (get_study(metric)) to obtain a standard single-metric Study view per metric name, enabling reuse of existing tooling and best_trial queries.\n\nGeneralizable ideas:\n\n1. **Multi-metric via user attributes**: When a system only supports a single objective value per trial, you can still evaluate and store multiple metrics per trial in supplementary fields (e.g., user_attrs) while exposing a single chosen metric as the optimization target.\n\n2. **Sequential multi-metric optimization**: If true multi-objective optimization is not available or desired, you can sequentially optimize each metric. For each metric, you can bootstrap the new optimization with existing trials by reusing their parameter sets and measured metrics, then continue exploration focused on that metric.\n\n3. **API design for extended objectives**: Introducing a new objective signature (returning dict instead of float) often requires a thin adapter layer that transforms the richer outputs into what the existing engine expects, thereby avoiding invasive changes.\n\n4. **Use of internal APIs**: The implementation relied on private APIs like study._append_trial to manually construct studies from existing trials. This is powerful but fragile: it couples your feature to internal details that may change, and therefore is risky for public, long-term API design.\n\n5. **Importance of clear motivation**: The PR was closed because the author was unsure about the motivation and requirements. This highlights the need to clearly define use-cases and design goals before implementing non-trivial API extensions; otherwise, the resulting implementation may not align with what users actually need.\n\nThese patterns generalize to any optimization or experiment-management system where you want to support multiple metrics without fully redesigning the underlying engine, but they also warn about the trade-offs of sequential optimization and reliance on internal methods.",
        "procedural_memory": [
            "When you need to support multiple metrics in a single-metric optimization framework, design a wrapper API rather than immediately changing core internals.",
            "Step 1: Clarify requirements and motivation for multi-metric support.\n- Determine whether you need true multi-objective optimization (e.g., Pareto fronts) or simply the ability to evaluate and compare multiple scalar metrics per trial.\n- Identify typical metrics (e.g., loss, accuracy, latency) and how users want to consume them (rankings, separate best trials, joint constraints, etc.).",
            "Step 2: Extend the objective function interface in a backward-compatible way.\n- Define a new objective type that returns a dict of metric_name -> float.\n- Keep the original objective type (returning a single float) working unchanged.\n- Introduce adapter functions that can convert a multi-metric objective into a single-metric objective by selecting one metric at a time.",
            "Step 3: Persist all metrics per trial without breaking existing schemas.\n- Use flexible storage fields (e.g., user_attrs, metadata blobs) attached to each trial to store the full metrics dict.\n- Ensure the trial evaluation code always writes the metrics dict consistently (same keys, numeric values).",
            "Step 4: Implement a multi-metric controller that orchestrates optimization.\n- Design a class (e.g., MultiMetricStudy) that:\n  - Accepts the multi-metric objective function.\n  - For each optimization run, wraps it in a single-metric objective that selects one metric by name or index.\n  - Stores the full metrics dict in the trial so it can be reused later.\n- Provide parameters controlling how many trials to run for the first metric versus subsequent metrics (e.g., n_first_trials and n_next_trials).",
            "Step 5: Reuse and propagate trials across metrics.\n- When optimizing the second and later metrics, bootstrap a new study (or optimization run) with trials from the previous metric's study.\n- For each prior trial, reconstruct it in the new study by copying:\n  - objective value (set it to the selected metric for this run),\n  - params and distributions,\n  - user_attrs (including the metrics dict),\n  - system_attrs,\n  - state and timestamps.\n- Be aware that using private APIs (such as _append_trial) is fragile; prefer public APIs if available, or lobby for a supported way to import trials.",
            "Step 6: Expose per-metric views for downstream use.\n- Implement a method like get_study(metric_name) that builds a standard single-metric Study (or equivalent object) whose objective values are taken from the chosen metric.\n- This allows all existing analysis code (e.g., best_trial, plots, statistics) to work unchanged without multi-metric awareness.",
            "Step 7: Validate and document behavior.\n- Provide examples showing how to define a multi-metric objective and how to access per-metric best trials.\n- Document how metrics are selected, the sequential optimization strategy, and any limitations (e.g., not truly multi-objective, order-dependence, possible bias).",
            "Step 8: Avoid or manage reliance on internal APIs.\n- If you must use internal functions like _append_trial, encapsulate their usage in a small, well-documented layer.\n- Add tests so future internal changes will be caught by failing tests.\n- Prefer requesting or contributing a public API upstream for trial cloning/importing.",
            "Step 9: Reassess if requirements change or remain unclear.\n- If after initial implementation the motivation or design still feels unclear, consider closing or shelving the work instead of merging an unstable API.\n- Use the prototype as a reference for future, better-defined designs."
        ]
    }
}