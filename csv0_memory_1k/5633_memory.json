{
    "search_index": {
        "description_for_embedding": "Home Assistant recorder logbook queries were slow because the events.time_fired column lacked an index. The fix adds an index on time_fired and introduces a simple schema versioning and migration system to apply the index to existing databases without manual intervention.",
        "keywords": [
            "homeassistant",
            "recorder",
            "logbook performance",
            "SQLite",
            "SQLAlchemy",
            "database index",
            "time_fired",
            "schema migration",
            "schema versioning",
            "SchemaChanges",
            "slow query",
            "performance optimization"
        ]
    },
    "agent_memory": {
        "episodic_memory": "In this incident, the Home Assistant recorder component exhibited slow logbook performance. On a 760MiB SQLite database backed by spinning disks, warm-cache queries for a single logbook day took around 1.65 seconds. Profiling and observation suggested the events table was being scanned without an index on the time_fired column, which is heavily used in logbook lookups.\n\nThe fix had two main parts:\n\n1. **Add an index on events.time_fired for new databases**: The Events SQLAlchemy model was updated so that the time_fired column is declared with `index=True`. This ensures future database tables created by `Base.metadata.create_all(engine)` will have the proper index from the start.\n\n2. **Introduce schema versioning and migrate existing databases**:\n   - A new constant `SCHEMA_VERSION = 1` and a corresponding `SchemaChanges` model were introduced. The `schema_changes` table stores `change_id`, `schema_version`, and timestamp `changed`, effectively tracking schema evolution.\n   - The recorder setup (`_setup_connection`) now calls `_migrate_schema()` after creating metadata and the session.\n   - `_migrate_schema()` reads the latest schema version from the `schema_changes` table using a query ordered by `change_id.desc()`. If the stored version equals `SCHEMA_VERSION`, nothing is done.\n   - If no schema version entry exists, `_inspect_schema_version()` is invoked to infer whether the database is new or legacy:\n     * It uses SQLAlchemy's `Inspector` to inspect indexes on the `events` table.\n     * If there is an index with column_names `[\"time_fired\"]`, the database is treated as already having the new schema (a new DB). It writes a `SchemaChanges` row with version `SCHEMA_VERSION` and returns that version.\n     * Otherwise, it concludes the DB predates schema tracking and needs migration, writes a `SchemaChanges` row with version `0`, and returns 0.\n   - For each version from `current_version` up to `SCHEMA_VERSION - 1`, `_migrate_schema()` calls `_apply_update(new_version)` where `new_version = version + 1`.\n   - `_apply_update(1)` implements the migration for schema version 1: it creates an index for `events.time_fired` by constructing an SQLAlchemy `Table` from metadata and an `Index` named `ix_events_time_fired`, then calling `index.create(self.engine)`. If an undefined version is requested, `_apply_update` raises `ValueError` to catch programming errors.\n\n3. **Tests**: New tests in `tests/components/recorder/test_init.py` verify:\n   - `test_schema_no_recheck`: When the schema is already at the latest version, `_migrate_schema()` does not call `_apply_update` or `_inspect_schema_version` again.\n   - `test_invalid_update`: Calling `_apply_update(-1)` raises a `ValueError`, ensuring unsupported migrations fail loudly.\n   - `test_schema_update_calls`: When the `schema_changes` table says version 0, `_migrate_schema()` calls `_apply_update` with each version increment up to `SCHEMA_VERSION` in order.\n\nAfter these changes, the database size increased modestly (~760MiB to ~780MiB), but logbook queries improved significantly, dropping from ~1.65 seconds to ~0.55 seconds for a day of data on the same hardware.",
        "semantic_memory": "This PR illustrates several generalizable practices around database performance and schema evolution:\n\n1. **Index time-based and filter-heavy columns**: Columns frequently used in WHERE clauses (e.g., timestamps used for range queries in log viewers) should usually be indexed. Lack of an index on such a column can cause full table scans and severe performance degradation, especially as data volume grows.\n\n2. **Integrate indexing into the ORM model**: With ORMs like SQLAlchemy, declaring indexes at the model level (e.g., `Column(..., index=True)` or explicit `Index` definitions) ensures new installations and fresh databases automatically get the optimal schema without extra migration steps.\n\n3. **Schema versioning in application code**: When no external migration tool (like Alembic) is used, it's still important to maintain explicit schema versioning:\n   - Store a schema version in a dedicated table (e.g., `schema_changes`) rather than relying on implicit assumptions.\n   - Incrementally apply migrations from the current version to the target version, one version at a time.\n   - Make migrations idempotent and deterministic; log progress and errors.\n\n4. **Backward compatibility and inspection-based migration**: If schema version tracking was not present historically, you can bootstrap it by inspecting the existing database structure:\n   - Use DB introspection (e.g., SQLAlchemy `Inspector`) to detect whether certain columns, indexes, or tables already exist.\n   - Based on this inspection, decide whether the DB is new (already using the latest schema) or legacy (in need of migration).\n   - Record the deduced version in the version table to avoid re-running detection logic.\n\n5. **Separation of migration orchestration and individual updates**: Splitting the logic that orchestrates migration (`_migrate_schema`) from the logic that applies each update (`_apply_update(version)`) is a clean pattern. It allows you to:\n   - Keep migration sequencing logic small and generic.\n   - Implement each schema change in a dedicated, version-specific block.\n   - Fail fast for undefined versions, helping maintainers keep the migration path coherent.\n\n6. **Testing schema migration paths**: Migration logic is easy to break and hard to debug in production, so it needs explicit tests:\n   - Verify no-op behavior when the schema is up to date.\n   - Verify correct ordering and coverage of updates when migrating from older versions.\n   - Test error handling for invalid or unsupported version numbers.\n\n7. **Performance vs storage trade-offs**: Adding indexes increases database size but often produces significant performance gains. This patch shows accepting a modest storage increase (~20MiB) for a significant speedup (3x faster queries) is often a good trade.\n\nOverall, the pattern is: detect performance bottlenecks from frequent queries, add targeted indexes, and wrap schema changes in a robust, versioned migration mechanism with tests.",
        "procedural_memory": [
            "Step-by-step instructions on how to diagnose and fix similar issues.",
            "Step 1: Identify slow queries and their access patterns\n- Enable query logging or use a profiler to find slow database operations (e.g., logbook or timeline endpoints).\n- Inspect the SQL produced by the ORM or the raw queries.\n- Focus on queries that read large tables with filters on a few columns (e.g., time ranges, foreign keys, event_type).",
            "Step 2: Check for missing or suboptimal indexes\n- Use the database’s introspection tools (e.g., `PRAGMA index_list(table)` in SQLite, `SHOW INDEX` in MySQL, or SQLAlchemy’s `Inspector.get_indexes`) to inspect existing indexes.\n- Verify whether frequently used filter columns (such as timestamps, IDs, statuses) are indexed.\n- If a query is filtering by `WHERE time_fired BETWEEN ...`, ensure `time_fired` has an index.",
            "Step 3: Add indexes to ORM models for new installations\n- In your SQLAlchemy model, add `index=True` to heavily queried columns:\n  - Example: `time_fired = Column(DateTime(timezone=True), index=True)`.\n- Alternatively, define explicit `Index` constructs at model definition time for composite or named indexes.\n- Run your tests or create a fresh database to ensure the new schema with indexes is created correctly.",
            "Step 4: Introduce schema version tracking if it does not exist\n- Define a constant, e.g., `SCHEMA_VERSION = N`, representing the current schema version.\n- Create a simple versioning table/model, e.g.:\n  - `class SchemaChanges(Base):`\n  - `__tablename__ = 'schema_changes'`\n  - `change_id = Column(Integer, primary_key=True)`\n  - `schema_version = Column(Integer)`\n  - `changed = Column(DateTime(timezone=True), default=datetime.utcnow)`.\n- Ensure this table is included in your metadata so it is created with `Base.metadata.create_all(engine)`.",
            "Step 5: Implement migration orchestration\n- After establishing the database connection and metadata, implement a function (e.g., `_migrate_schema`) that:\n  - Queries the latest `SchemaChanges` row ordered by `change_id.desc()` to find the current version.\n  - If the current version equals `SCHEMA_VERSION`, returns immediately.\n  - If there is no row, call an inspection function (see Step 6) to determine whether the DB is new or legacy, then insert the inferred version into `schema_changes`.\n  - Iterate from `current_version` to `SCHEMA_VERSION - 1`, and for each step:\n    - Compute `new_version = version + 1`.\n    - Call a version-specific function like `_apply_update(new_version)`.\n    - Insert a new row into `schema_changes` recording `new_version`.\n  - Log before and after applying each update for traceability.",
            "Step 6: Bootstrap versioning for legacy databases via inspection\n- Implement an inspection function (e.g., `_inspect_schema_version`) that examines the current DB structure to infer version:\n  - Use `Inspector.from_engine(engine).get_indexes('events')` (or equivalent) to list indexes.\n  - Check for the presence of new schema elements (e.g., an index on `[\"time_fired\"]`).\n  - If present, assume the DB already uses the latest schema (i.e., treat it as new), insert `SchemaChanges(schema_version=SCHEMA_VERSION)`, and return `SCHEMA_VERSION`.\n  - If not present, treat it as a legacy database needing migration: insert `SchemaChanges(schema_version=0)` and return 0.",
            "Step 7: Implement version-specific migrations\n- Create a function like `_apply_update(version)` that contains a switch/if-else for each known version:\n  - For example, for version 1:\n    - Use `Table(table_name, Base.metadata)` to construct a lightweight table representation bound to metadata.\n    - Construct an `Index` instance: `Index('ix_events_time_fired', table.c.time_fired)`.\n    - Call `index.create(engine)` to create the index.\n    - Log start and completion.\n  - For any unsupported version, raise a `ValueError` to prevent silent failures and catch mistakes early.",
            "Step 8: Wire migration into startup/connection setup\n- After `Base.metadata.create_all(engine)` and before the application starts using the database:\n  - Call your migration orchestrator (`_migrate_schema()`) so that any required schema changes are applied before normal operations.\n  - Only after migration completes, set any flags that indicate DB readiness (e.g., `db_ready.set()`).",
            "Step 9: Write tests for migration behavior\n- Add unit tests to ensure the migration system behaves correctly:\n  - Test no-op behavior when the schema is at the latest version (no calls to `_apply_update` or inspection).\n  - Seed the `schema_changes` table with an older version and verify that `_migrate_schema()` calls `_apply_update` for each intermediate version in order.\n  - Test that invalid version numbers given to `_apply_update` raise exceptions.\n  - Mock or patch the migration functions to assert call counts and order (e.g., using `unittest.mock.patch` and `assert_has_calls`).",
            "Step 10: Validate performance and monitor\n- After deploying the index and migration, re-run the slow queries or relevant feature (e.g., logbook view) to measure performance improvements.\n- Compare timings before and after (for example, verify a significant reduction from >1s to sub-second query times).\n- Monitor logs for any migration errors and confirm that new installations skip unnecessary migration work and start at the latest schema version."
        ]
    }
}